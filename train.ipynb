{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils_ import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "import argparse\n",
    "import numpy as np\n",
    "from Models import Pipeline, MTLoss, Prediction #CrossAttention, Context\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(dim = 768, upscale_dim = 256, codes='128,64,32,8', batch_size=2, learning_rate=0.001, weight_decay=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLoaders2(main_task_path = './Data/SignData/train_data', scaffold_task_path = './Data/2018/train_data', batch_size=8, slice=[-1, -1, -1], test_path='./Data/SignData/test_data'):\n",
    "\tprint('Reading the Main Task Dataset...')\n",
    "\tmain_task_dataset = RevSigData(main_task_path, mode='MAIN', slice_=slice[0], transform=Transform(), sigtx=ScaleSigScores())\n",
    "\t#main_task_dataset = dataset.readData(main_task_path, Transform(), mode='MAIN', n=slice[0])\n",
    "\tprint('Reading the Scaffolds Task Dataset...')\n",
    "\tscaffold_task_dataset = RevSigData(scaffold_task_path, mode='SCAFFOLDS', slice_=slice[1], transform=Transform())\n",
    "\t#scaffold_task_dataset = dataset.readData(scaffold_task_path, Transform(), mode='SCAFFOLDS', n=slice[1])\n",
    "\t\n",
    "\n",
    "\tif test_path:\n",
    "\t\tprint('Reading the test Dataset')\n",
    "\t\ttest_dataset = RevSigData(test_path, mode='TEST', slice_=slice[2], transform=Transform(), sigtx=ScaleSigScores())\n",
    "\t\t#test_dataset = dataset.readData(test_path, Transform(), mode='TEST', n=slice[2])\n",
    "\telse:\n",
    "\t\ttest_dataset = None\n",
    "\n",
    "\n",
    "\t#length of the both task datasets\n",
    "\tmain_task_len = len(main_task_dataset)\n",
    "\tscaffold_task_len = len(scaffold_task_dataset)\n",
    "\ttest_len = len(test_dataset)\n",
    "\n",
    "\t#inflate the smaller dataset to match the size of the larger one\n",
    "\tif main_task_len < scaffold_task_len:\n",
    "\t\tdifference = scaffold_task_len - main_task_len\n",
    "\t\tsample = [random.choice(main_task_dataset) for _ in range(difference)]\n",
    "\t\tmain_task_dataset = main_task_dataset + sample\n",
    "\t\n",
    "\t# print(len(main_task_dataset), len(scaffold_task_dataset))\n",
    "\t#print(main_task_len, scaffold_task_len)\n",
    "\treturn (main_task_dataset, scaffold_task_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-9cc901ffc164>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_task_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaffold_task_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetLoaders2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "main_task_dataset, scaffold_task_dataset, test_dataset = getLoaders2(batch_size=args.batch_size, slice=[-1,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_task_dataloader = DataLoader(main_task_dataset, batch_size = args.batch_size, shuffle = True, num_workers=4)\n",
    "scaffold_task_dataloader = DataLoader(scaffold_task_dataset, batch_size = args.batch_size, shuffle=True, num_workers=4)\n",
    "if test_dataset != None:\n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "else:\n",
    "    test_data_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main_task_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-efd7450eadf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_task_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaffold_task_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'main_task_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate(model, main_task_predictor, scaffold_task_predictor, Criterion, test_loader):\n",
    "\twith torch.no_grad():\n",
    "\t\teval_loss = []\n",
    "\t\tfor i, d in enumerate(test_loader,0):\n",
    "\t\t\tscaffold_task_data = d\n",
    "\t\t\tpapers_sc, reviews_sc, recs_sc, confs_sc, sign_m = scaffold_task_data[0].transpose(1,2).float().to(device),\\\n",
    "\t\t\t\t\t\t\t scaffold_task_data[1].transpose(1,2).float().to(device), \\\n",
    "\t\t\t\t\t\t\t scaffold_task_data[2].float().to(device),\\\n",
    "\t\t\t\t\t\t\t scaffold_task_data[3].float().to(device),\\\n",
    "\t\t\t\t\t\t\t scaffold_task_data[4].float().to(device)\n",
    "\n",
    "\t\t\tex, subj, opine = sign_m[:,0], sign_m[:,1], sign_m[:,2]\n",
    "\t\t\tout, rec_codes, conf_codes = model(papers_sc, reviews_sc)\n",
    "\t\t\trec_preds, conf_preds = scaffold_task_predictor(rec_codes.view(out.shape[0], -1), conf_codes.view(out.shape[0], -1))\n",
    "\n",
    "\t\t\t#out_m, rec_codes_m, conf_codes_m = model(papers_sc, reviews_sc)\n",
    "\t\t\tex_preds, subj_preds, intensity_preds = main_task_predictor(out, rec_codes, conf_codes)\n",
    "\n",
    "\n",
    "\t\t\tloss = Criterion([rec_preds.squeeze(1), conf_preds.squeeze(1), ex_preds.squeeze(1), subj_preds.squeeze(1), intensity_preds.squeeze(1)], [recs_sc, confs_sc, ex, subj, opine])\n",
    "\t\t\t\n",
    "\t\t\teval_loss.append(loss.item())\n",
    "\t\treturn np.average(eval_loss)\n",
    "\n",
    "\n",
    "\n",
    "def train(args, dataloaders):\n",
    "    main_task_loader, scaffold_task_loader, test_loader = dataloaders\n",
    "    model = Pipeline.Pipeline(args).to(device)\n",
    "    main_task_predictor = Prediction.MainPrediction(args.upscale_dim, args.upscale_dim, 16).to(device)\n",
    "    scaffold_task_predictor = Prediction.ScaffoldPrediction(args.upscale_dim, 8).to(device)\n",
    "\n",
    "    print(model)\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.shape)\n",
    "    print(\"No. of Trainable parameters {}\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "    Criterion = MTLoss.MTLoss().to(device2)\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + list(Criterion.parameters()), lr=args.learning_rate, weight_decay=args.weight_decay) #+ list(main_task_predictor.parameters()) + list(scaffold_task_predictor.parameters())\n",
    "    optimizerMain = torch.optim.Adam(main_task_predictor.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    optimizerScaffold = torch.optim.Adam(scaffold_task_predictor.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    epochs = 100\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "        for i, d in enumerate(zip(scaffold_task_loader, main_task_loader),0):\n",
    "            #print(i)\n",
    "            main_task_data = d[1]\n",
    "            scaffold_task_data = d[0]\n",
    "            papers_sc, reviews_sc, recs_sc, confs_sc = scaffold_task_data[0].transpose(1,2).float().to(device),\\\n",
    "                                 scaffold_task_data[1].transpose(1,2).float().to(device), \\\n",
    "                                 scaffold_task_data[2].float().to(device),\\\n",
    "                                 scaffold_task_data[3].float().to(device)\n",
    "\n",
    "            #print(ex.shape, subj.shape, opine.shape, recs_sc.shape, confs_sc.shape)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            optimizerMain.zero_grad()\n",
    "            optimizerScaffold.zero_grad()\n",
    "            out, rec_codes, conf_codes = model(papers_sc, reviews_sc)\n",
    "            rec_preds, conf_preds = scaffold_task_predictor(rec_codes.view(out.shape[0], -1), conf_codes.view(out.shape[0], -1))\n",
    "            \n",
    "            del papers_sc\n",
    "            del reviews_sc\n",
    "            \n",
    "            papers_sc, reviews_sc, sign_m = main_task_data[0].transpose(1,2).float().to(device),\\\n",
    "                                 main_task_data[1].transpose(1,2).float().to(device), \\\n",
    "                                 main_task_data[2].float().to(device)\n",
    "\n",
    "            ex, subj, opine = sign_m[:,0], sign_m[:,1], sign_m[:,2]\n",
    "\n",
    "            #do the for the main task\n",
    "            out_m, rec_codes_m, conf_codes_m = model(papers_sc, reviews_sc)\n",
    "            ex_preds, subj_preds, intensity_preds = main_task_predictor(out_m, rec_codes_m, conf_codes_m)\n",
    "            #print(ex_preds.shape, subj_preds.shape, intensity_preds.shape)\n",
    "\n",
    "\n",
    "            loss = Criterion([rec_preds.squeeze(1), conf_preds.squeeze(1), ex_preds.squeeze(1), subj_preds.squeeze(1), intensity_preds.squeeze(1)], [recs_sc, confs_sc, ex, subj, opine])\n",
    "            epoch_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizerMain.step()\n",
    "            optimizerScaffold.step()\n",
    "            optimizer.step()\n",
    "        #print(\"Epoch {} Loss: {:.3f}\".format(epoch, np.average(epoch_loss)))\n",
    "            del papers_sc\n",
    "            del reviews_sc\n",
    "            gc.collect()\n",
    "        # \tbreak\n",
    "        # break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            eval_loss = evaluate(model, main_task_predictor, scaffold_task_predictor, Criterion, test_loader)\n",
    "\n",
    "            print('Epoch: {} Train Loss: {:.6f}, Test Loss: {:.6f}'.format(epoch, np.average(epoch_loss),\\\n",
    "                            eval_loss))\n",
    "            # print(\"Exhaustive {}\".format(list(zip(ex_preds.data, ex.data))))\n",
    "            # print(\"Subjectivity {}\".format(list(zip(subj_preds.data, subj.data))))\n",
    "            # print(\"Intensity {}\".format(list(zip(intensity_preds.data, opine.data))))\n",
    "            # print(\"Recommendation {}\".format(list(zip(rec_preds.data, recs_sc.data))))\n",
    "            # print(\"Confidence {}\".format(list(zip(conf_preds.data, confs_sc.data))))\n",
    "\n",
    "            #logging.info('Predictions, Actual : {}'.format(str(list(zip(recs_preds_t, recs_sc_t)))))\n",
    "        #break\n",
    "\n",
    "\n",
    "def main(args, dataloaders=(main_task_dataloader, scaffold_task_dataloader, test_data_loader)):\n",
    "    train(args, dataloaders)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(\n",
      "  (cross_attention): CrossAttention(\n",
      "    (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (contextor): Sequential(\n",
      "    (coder0): Context(\n",
      "      (linear): Linear(in_features=768, out_features=256, bias=True)\n",
      "      (codes): Linear(in_features=256, out_features=128, bias=False)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (coder1): Context(\n",
      "      (linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (codes): Linear(in_features=256, out_features=64, bias=False)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (coder2): Context(\n",
      "      (linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (codes): Linear(in_features=256, out_features=32, bias=False)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (coder3): Context(\n",
      "      (linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (codes): Linear(in_features=256, out_features=8, bias=False)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (rec_codes): Context(\n",
      "    (linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (codes): Linear(in_features=256, out_features=8, bias=False)\n",
      "    (act): ReLU()\n",
      "  )\n",
      "  (conf_codes): Context(\n",
      "    (linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (codes): Linear(in_features=256, out_features=8, bias=False)\n",
      "    (act): ReLU()\n",
      "  )\n",
      ")\n",
      "cross_attention.linear1.weight torch.Size([256, 768])\n",
      "cross_attention.linear1.bias torch.Size([256])\n",
      "contextor.coder0.linear.weight torch.Size([256, 768])\n",
      "contextor.coder0.linear.bias torch.Size([256])\n",
      "contextor.coder0.codes.weight torch.Size([128, 256])\n",
      "contextor.coder1.linear.weight torch.Size([256, 256])\n",
      "contextor.coder1.linear.bias torch.Size([256])\n",
      "contextor.coder1.codes.weight torch.Size([64, 256])\n",
      "contextor.coder2.linear.weight torch.Size([256, 256])\n",
      "contextor.coder2.linear.bias torch.Size([256])\n",
      "contextor.coder2.codes.weight torch.Size([32, 256])\n",
      "contextor.coder3.linear.weight torch.Size([256, 256])\n",
      "contextor.coder3.linear.bias torch.Size([256])\n",
      "contextor.coder3.codes.weight torch.Size([8, 256])\n",
      "rec_codes.linear.weight torch.Size([256, 256])\n",
      "rec_codes.linear.bias torch.Size([256])\n",
      "rec_codes.codes.weight torch.Size([8, 256])\n",
      "conf_codes.linear.weight torch.Size([256, 256])\n",
      "conf_codes.linear.bias torch.Size([256])\n",
      "conf_codes.codes.weight torch.Size([8, 256])\n",
      "No. of Trainable parameters 786176\n",
      "Epoch: 0 Train Loss: 20.978231, Test Loss: 24.089643\n",
      "Epoch: 1 Train Loss: 16.094613, Test Loss: 17.710211\n",
      "Epoch: 2 Train Loss: 13.489517, Test Loss: 17.966402\n",
      "Epoch: 3 Train Loss: 12.959306, Test Loss: 16.149176\n",
      "Epoch: 4 Train Loss: 12.410875, Test Loss: 13.896947\n",
      "Epoch: 5 Train Loss: 12.005326, Test Loss: 19.271325\n",
      "Epoch: 6 Train Loss: 11.924823, Test Loss: 12.263668\n",
      "Epoch: 7 Train Loss: 11.755954, Test Loss: 17.616422\n",
      "Epoch: 8 Train Loss: 11.496296, Test Loss: 14.725958\n",
      "Epoch: 9 Train Loss: 11.284573, Test Loss: 11.676680\n",
      "Epoch: 10 Train Loss: 11.387819, Test Loss: 16.202621\n",
      "Epoch: 11 Train Loss: 11.373499, Test Loss: 13.787546\n",
      "Epoch: 12 Train Loss: 11.115358, Test Loss: 13.820032\n",
      "Epoch: 13 Train Loss: 10.957923, Test Loss: 12.990627\n",
      "Epoch: 14 Train Loss: 10.850128, Test Loss: 14.567307\n",
      "Epoch: 15 Train Loss: 10.668553, Test Loss: 15.507806\n",
      "Epoch: 16 Train Loss: 10.526581, Test Loss: 14.679967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home2/tirthankar/miniconda3_1/envs/rajeev3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home2/tirthankar/miniconda3_1/envs/rajeev3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home2/tirthankar/miniconda3_1/envs/rajeev3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home2/tirthankar/miniconda3_1/envs/rajeev3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-aacc94af8ac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_task_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaffold_task_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-aaf180b92cb5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args, dataloaders)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_task_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaffold_task_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-aaf180b92cb5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, dataloaders)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrec_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubj_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintensity_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrecs_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfs_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0moptimizerMain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(args, dataloaders=(main_task_dataloader, scaffold_task_dataloader, test_data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, main_task_predictor, scaffold_task_predictor, test_loader, Criterion=None):\n",
    "    with torch.no_grad():\n",
    "        eval_loss = []\n",
    "        ex_preds_, ex_actual = [], []\n",
    "        subj_preds_, subj_actual = [], []\n",
    "        int_preds_, int_actual = [], []\n",
    "        rec_preds_, rec_actual = [], []\n",
    "        conf_preds_, conf_actual = [], []\n",
    "        for i, d in enumerate(test_loader,0):\n",
    "            scaffold_task_data = d\n",
    "            papers_sc, reviews_sc, recs_sc, confs_sc, sign_m = scaffold_task_data[0].transpose(1,2).float().to(device),\\\n",
    "                             scaffold_task_data[1].transpose(1,2).float().to(device), \\\n",
    "                             scaffold_task_data[2].float().to(device),\\\n",
    "                             scaffold_task_data[3].float().to(device),\\\n",
    "                             scaffold_task_data[4].float().to(device)\n",
    "\n",
    "            ex, subj, opine = sign_m[:,0], sign_m[:,1], sign_m[:,2]\n",
    "            out, rec_codes, conf_codes = model(papers_sc, reviews_sc)\n",
    "            rec_preds, conf_preds = scaffold_task_predictor(rec_codes.view(out.shape[0], -1), conf_codes.view(out.shape[0], -1))\n",
    "\n",
    "            #out_m, rec_codes_m, conf_codes_m = model(papers_sc, reviews_sc)\n",
    "            ex_preds, subj_preds, intensity_preds = main_task_predictor(out, rec_codes, conf_codes)\n",
    "            ex_preds_.append(ex_preds.item())\n",
    "            ex_actual.append(ex.item())\n",
    "            subj_preds_.append(subj_preds.item())\n",
    "            subj_actual.append(subj.item())\n",
    "            int_preds_.append(intensity_preds.item())\n",
    "            int_actual.append(opine.item())\n",
    "\n",
    "\n",
    "\n",
    "            if Criterion != None:\n",
    "                loss = Criterion([rec_preds.squeeze(1), conf_preds.squeeze(1), ex_preds.squeeze(1), subj_preds.squeeze(1), intensity_preds.squeeze(1)], [recs_sc, confs_sc, ex, subj, opine])\n",
    "                eval_loss.append(loss.item())\n",
    "            else:\n",
    "                eval_loss.append(0)\n",
    "\n",
    "        return np.average(eval_loss), (ex_preds_, ex_actual), (subj_preds_, subj_actual), (int_preds_, int_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./MODELS/exp3.pt', map_location=device)\n",
    "args = SimpleNamespace(dim = checkpoint['dim'], upscale_dim = checkpoint['upscale_dim'], codes=checkpoint['codes']) \n",
    "model = Pipeline.Pipeline(args).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "main_task_predictor = Prediction.MainPrediction(args.upscale_dim, args.upscale_dim, 32).to(device)\n",
    "main_task_predictor.load_state_dict(checkpoint['main_state_dict'])\n",
    "scaffold_task_predictor = Prediction.ScaffoldPrediction(args.upscale_dim, 8).to(device)\n",
    "scaffold_task_predictor.load_state_dict(checkpoint['scaffold_state_dict'])\n",
    "Criterion = MTLoss.MTLoss().to(device)\n",
    "Criterion.load_state_dict(checkpoint['criterion_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the Main Task Dataset...\n",
      "Reading the Scaffolds Task Dataset...\n",
      "Reading the test Dataset\n"
     ]
    }
   ],
   "source": [
    "_, _, test_dataset = getLoaders2(batch_size=1, slice=[-1,-1,-1])\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, exhaustive, subjective, opinion = evaluate(model, main_task_predictor, scaffold_task_predictor, test_data_loader, Criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(zip(*opinion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "opine_pred, opine_actual = opinion\n",
    "subj_pred, subj_actual = subjective\n",
    "exhaustive_pred, exhaustive_actual = exhaustive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "def cosine(a, b):\n",
    "    return dot(a,b)/(norm(a)*norm(b))\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(np.mean((np.asarray(predictions)-np.asarray(targets))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(score):\n",
    "    RMSE = rmse(score[0], score[1])\n",
    "    sim = cosine(score[0], score[1])\n",
    "    print(\"RMSE: {}, cosine similarity: {}\".format(RMSE, sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.5017433881250435, cosine similarity: 0.9895367837062644\n",
      "RMSE: 0.8054140323941106, cosine similarity: 0.982373976678173\n",
      "RMSE: 1.45750686097663, cosine similarity: 0.9397649115706209\n"
     ]
    }
   ],
   "source": [
    "for score in [(exhaustive_pred, exhaustive_actual), (subj_pred, subj_actual), (opine_pred, opine_actual)]:\n",
    "    get_metric(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.5518540202548305, cosine similarity: 0.8932938675323975\n",
      "RMSE: 2.1199669478437224, cosine similarity: 0.8705800832137459\n",
      "RMSE: 1.6752461775372902, cosine similarity: 0.9184800659455056\n"
     ]
    }
   ],
   "source": [
    "exhaustive_pred = [np.average(exhaustive_actual)]*len(exhaustive_actual)\n",
    "subj_pred = [np.average(subj_actual)]*len(exhaustive_actual)\n",
    "opine_pred = [np.average(opine_actual)]*len(exhaustive_actual)\n",
    "for score in [(exhaustive_pred, exhaustive_actual), (subj_pred, subj_actual), (opine_pred, opine_actual)]:\n",
    "    get_metric(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "path = './Data/SignData/test/reviews'\n",
    "reviews = os.listdir(path)\n",
    "filenames = os.listdir('./Data/SignData/test_data/')\n",
    "\n",
    "review_text = []\n",
    "indices = []\n",
    "for i, filename in enumerate(filenames):\n",
    "    split = filename.split('_')\n",
    "    fname, count = ''.join(c for c in split[:-1]), split[-1]\n",
    "    if fname in reviews:\n",
    "        rev = json.load(open(os.path.join(path, fname)))['reviews'][int(count)]\n",
    "        review_text.append(rev)\n",
    "    else:\n",
    "        indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dict_ = defaultdict(lambda: {})\n",
    "for i in range(len(opine_pred)):\n",
    "    scores = {}\n",
    "    if i not in indices:\n",
    "        scores['layer1'] = (exhaustive_pred[i], exhaustive_actual[i])\n",
    "        scores['layer2'] = (subj_pred[i], subj_actual[i])\n",
    "        scores['layer3'] = (opine_pred[i], opine_actual[i])\n",
    "        dict_[i] = scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 89, 66, 62, 59, 54, 37)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "91, 89, 66, 62, 59, 54, (37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "l1, l2 = np.array([0.05428571,0.93013972,-0.99966815]), np.array([33.26288336,252.25067107,0.99966679])\n",
    "def normalize(s):\n",
    "    s = np.array(s)\n",
    "    sent = s[2]\n",
    "    s = (s - l1)/(l2 - l1)\n",
    "    s = (s*9) + 1\n",
    "    s[2] = sent\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.8522656   4.01916201 -0.78107315]\n",
      "{'RECOMMENDATION': '4', 'REVIEW TITLE': ' ', 'comments': '\"The authors tackle the problem of estimating risk in a survival analysis setting with competing risks. They propose directly optimizing the time-dependent discrimination index using a siamese survival network. Experiments on several real-world dataset reveal modest gains in comparison with the state of the art.\\\\n\\\\n- The authors should clearly highlight what is their main technical contribution. For example, Eqs. 1-6 appear to be background material since the time-dependent discrimination index is taken from the literature, as the authors point out earlier. However, this is unclear from the writing. \\\\n\\\\n- One of the main motivations of the authors is to propose a model that is specially design to avoid the nonidentifiability issue in an scenario with competing risks. It is unclear why the authors solution is able to solve such an issue, specially given the modest reported gains in comparison with several competitive baselines. In other words, the authors oversell their own work, specially in comparison with the state of the art.\\\\n\\\\n- The authors use off-the-shelf siamese networks for their settting and thus it is questionable there is any novelty there. The application/setting may be novel, but not the architecture of choice.\\\\n\\\\n- From Eq. 4 to Eq. 5, the authors argue that the denominator does not depend on the model parameters and can be ignored. However, afterwards the objective does combine time-dependent discrimination indices of several competing risks, with different denominator values. This could be problematic if the risks are unbalanced.\\\\n\\\\n- The competitive gain of the authors method in comparison with other competing methods is minor.\\\\n\\\\n- The authors introduce F(t, D | x) as cumulative incidence function (CDF) at the beginning of section 2, however, afterwards they use R^m(t, x), which they define as risk of the subject experiencing event m before t. Is the latter a proxy for the former? How are they related?\"', 'VARIANCE': 0, 'CONFIDENCE': '4', 'SCORES': [6.888857143, 85.23873991148139, -0.7810731530189514]}\n"
     ]
    }
   ],
   "source": [
    "for i in [79]:\n",
    "    new_scores = normalize(review_text[i]['SCORES'])\n",
    "    print(new_scores)\n",
    "    print(review_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, review in enumerate(review_text):\n",
    "    new_scores = normalize(review_text[i]['SCORES'])\n",
    "    for k,v in dict_.items():\n",
    "        if np.round(dict_[k]['layer1'][1], decimals=3) == np.round(new_scores[0], decimals=3):\n",
    "            review_text[i]['PREDICTIONS'] = dict_[k]\n",
    "            review_text[i]['NORMALISED'] = new_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, rev in enumerate(review_text):\n",
    "#     print(i)\n",
    "#     print(rev['PREDICTIONS'])\n",
    "#     print(rev['NORMALISED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tst_indices = [77, 63, 51, 48, 43, 29, 28, 8, 3]\n",
    "# with open('analysis', 'w') as f:\n",
    "#     for ind in tst_indices:\n",
    "#         f.w('##############')\n",
    "#         print(review_text[ind])\n",
    "#         print('##############')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Round(num):\n",
    "    return np.round(num, decimals=3)\n",
    "\n",
    "reviews = []\n",
    "import csv\n",
    "for i, review in enumerate(review_text):\n",
    "    new_scores = list(map(lambda x: Round(x), normalize(review_text[i]['SCORES'])))\n",
    "    review['SCORES'] = list(map(lambda x: Round(x), review['SCORES']))\n",
    "    review['Layer 1'], review['Layer 1 N'] = review['SCORES'][0], new_scores[0]\n",
    "    review['Layer 2'], review['Layer 2 N'] = review['SCORES'][1], new_scores[1]\n",
    "    review['Layer 3'], review['Layer 3 N'] = review['SCORES'][2], new_scores[2]\n",
    "    reviews.append(review)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_names = reviews[0].keys()\n",
    "with open('Test_Reviews.csv', 'w') as csvfile: \n",
    "    writer = csv.DictWriter(csvfile, fieldnames = field_names) \n",
    "    writer.writeheader() \n",
    "    writer.writerows(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rajeev3",
   "language": "python",
   "name": "rajeev3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
