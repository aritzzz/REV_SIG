 "The paper proposes combining classification-specific neural networks with auto-encoders.[[INT-NEU,PDI-NEU], [null]]  This is done in a straightforward manner by designating a few nodes in the output layer for classification and few for reconstruction.[[MET-NEU], [null]]  The training objective is then changed to minimize the sum of the classification loss (as measured by cross-entropy for instance) and the reconstruction error (as measured by ell-2 error as is done in training auto-encoders).[[EXP-NEU], [null]]  \n\nThe authors minimize the loss function by greedy layer-wise training as is done in several prior works.[[RWK-NEU,EXP-NEU], [CMP-NEU]]  The authors then perform other experiments on the learned representations in the output layer (those corresponding to classification + those corresponding to reconstruction).[[EXP-NEU], [null]]  For example, the authors plot the nearest-neighbors for classification-features and for reconstruction-features and observe that the two are very different.[[EXP-NEU], [null]]  The authors also observe that interpolating between two reconstruction-feature vectors (by convex combinations) seems to interpolate well between the two corresponding images.[[EXP-NEU,RES-NEU], [null]] \n\nWhile the experimental results are interesting[[EXP-POS,RES-POS], [EMP-POS]]  they are not striking especially when viewed in the context of the tremendous amount of work on auto-encoders.[[EXP-NEG,RES-NEG], [EMP-NEG]]  Training the classification-features along with reconstruction-features does not seem to give any significantly new insights. "[[EXP-NEG], [NOV-POS]]