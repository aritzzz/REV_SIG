 "The paper proposes training an autoencoder such that the middle layer representation consists of the class label of the input and a hidden vector representation called \"style memory\", which would presumably capture non-class information.[[INT-NEU,PDI-NEU], [null]]  The idea of learning representations that decompose into class-specific and class-agnostic parts, and more generally \"style\" and \"content\", is an interesting and long-standing problem.[[PDI-POS], [null]]  The results in the paper are mostly qualitative and only on MNIST.[[RES-NEU,DAT-NEU], [null]]  They do not show convincingly that the network managed to learn interesting class-specific and class-agnostic representations.[[MET-NEG,RES-NEG], [EMP-NEG]]  It's not clear whether the examples shown in figures 7 to 11 are representative of the network's general behavior.[[TNF-NEG], [PNF-NEG]]  The tSNE visualization in figure 6 seems to indicate that the style memory representation does not capture class information as well as the raw pixels, but doesn't indicate whether that representation is sensible.[[TNF-NEG], [PNF-NEG]] \n\nThe use of fully connected networks on images may affect the quality of the learned representations, and it may be necessary to use convolutional networks to get interesting results.[[RES-NEU], [EMP-NEU]]  It may also be interesting to consider class-specific representations that are more general than just the class label.[[EXP-NEU], [EMP-NEU]]  For example, see \"Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure\" by Salakhutdinov and Hinton, 2007, which learns hidden vector representations for both class-specific and class-agnostic parts. (This paper should be cited.)"[[RWK-NEU,BIB-NEU], [CMP-NEU]]