"The paper proposes training an autoencoder such that the middle layer representation consists of the class label of the input and a hidden vector representation called \"style memory\", which would presumably capture non-class information. The idea of learning representations that decompose into class-specific and class-agnostic parts, and more generally \"style\" and \"content\", is an interesting and long-standing problem. The results in the paper are mostly qualitative and only on MNIST. They do not show convincingly that the network managed to learn interesting class-specific and class-agnostic representations. It's not clear whether the examples shown in figures 7 to 11 are representative of the network's general behavior. The tSNE visualization in figure 6 seems to indicate that the style memory representation does not capture class information as well as the raw pixels, but doesn't indicate whether that representation is sensible.\n\nThe use of fully connected networks on images may affect the quality of the learned representations, and it may be necessary to use convolutional networks to get interesting results. It may also be interesting to consider class-specific representations that are more general than just the class label. For example, see \"Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure\" by Salakhutdinov and Hinton, 2007, which learns hidden vector representations for both class-specific and class-agnostic parts. (This paper should be cited.)"