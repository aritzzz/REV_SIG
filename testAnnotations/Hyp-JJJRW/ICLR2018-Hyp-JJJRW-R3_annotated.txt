 "The paper proposes combining classification-specific neural networks with auto-encoders.[[SMY], [GEN]]  This is done in a straightforward manner by designating a few nodes in the output layer for classification and few for reconstruction.[[SMY], [GEN]]  The training objective is then changed to minimize the sum of the classification loss (as measured by cross-entropy for instance) and the reconstruction error (as measured by ell-2 error as is done in training auto-encoders).[[SMY], [GEN]]  \n\nThe authors minimize the loss function by greedy layer-wise training as is done in several prior works.[[SMY], [GEN]]  The authors then perform other experiments on the learned representations in the output layer (those corresponding to classification + those corresponding to reconstruction).[[SMY], [GEN]]  For example, the authors plot the nearest-neighbors for classification-features and for reconstruction-features and observe that the two are very different.[[SMY], [GEN]]  The authors also observe that interpolating between two reconstruction-feature vectors (by convex combinations) seems to interpolate well between the two corresponding images.[[SMY], [GEN]] \n\nWhile the experimental results are interesting[[APC], [MAJ]]  they are not striking especially when viewed in the context of the tremendous amount of work on auto-encoders.[[CRT], [MIN]]  Training the classification-features along with reconstruction-features does not seem to give any significantly new insights. "[[CRT], [MAJ]]