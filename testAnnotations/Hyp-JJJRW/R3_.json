{"rating": "4", "confidence": "3", "review": "\"The paper proposes combining classification-specific neural networks with auto-encoders. This is done in a straightforward manner by designating a few nodes in the output layer for classification and few for reconstruction. The training objective is then changed to minimize the sum of the classification loss (as measured by cross-entropy for instance) and the reconstruction error (as measured by ell-2 error as is done in training auto-encoders). \\n\\nThe authors minimize the loss function by greedy layer-wise training as is done in several prior works. The authors then perform other experiments on the learned representations in the output layer (those corresponding to classification + those corresponding to reconstruction). For example, the authors plot the nearest-neighbors for classification-features and for reconstruction-features and observe that the two are very different. The authors also observe that interpolating between two reconstruction-feature vectors (by convex combinations) seems to interpolate well between the two corresponding images.\\n\\nWhile the experimental results are interesting they are not striking especially when viewed in the context of the tremendous amount of work on auto-encoders. Training the classification-features along with reconstruction-features does not seem to give any significantly new insights. \"", "variance": 0, "title": " ", "Scores": {"layer_1": "3.382857143", "layer_2": "34.259481037924154", "layer_3": -0.2371230646967888}}