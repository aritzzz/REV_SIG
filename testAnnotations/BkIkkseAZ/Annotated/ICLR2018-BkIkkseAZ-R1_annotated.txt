 "The paper studies the theoretical properties of the two-layer neural networks.[[INT-NEU], [NULL]]  \n\nTo summarize the result, let's use the theta to denote the layer closer to the label, and W to denote the layer closer to the data.[[RES-NEU], [NULL]]  \n\nThe paper shows that \na) if W is fixed, then with respect to the randomness of the data, with prob. 1, the Jacobian matrix of the model is full rank[[RES-NEU], [SUB-NEU]] \nb) suppose that we run an algorithm with fresh samples, then with respect to the randomness of the k-th sample, we have that with prob. 1, W_k is full rank, and the Jacobian of the model is full rank. [[RES-NEU], [SUB-NEU]] \n\nIt's know (essentially from the proof of Carmon and Soudry) that if the Jacobian of the model is full rank for any matrix W w.r.t the randomness of the data, then all stationary points are global.[[RWK-NEU], [SUB-NEU]]  But the paper cannot establish such a result. [[RES-NEG], [APR-NEG]] \n\nThe paper is not very clear, and after figuring out what it's doing, I don't feel it really provides many new things beyond C-S and Xie et al.[[ANA-NEG], [APR-NEG]] \n\nThe paper argues that it works for activation beyond relu but result a) is much much weaker than the one with for all quantifier for W.[[ANA-NEG], [APR-NEG]]  result b) is very sensitive to the exactness of the events (such as W is exactly full rank) --- the events that the paper talks just naturally never happen as long as the density of the random variables doesn't degenerate.[[DAT-NEG], [null]]   \n\nAs the author admitted, the results don't provide any formal guarantees for the convergence to a global minimum.[[RES-NEG], [APR-NEG]]  It's also a bit hard for me to find the techniques here provide new ideas that would potentially lead to resolving this question. [[MET-NEU], [NOV-NEU]] \n\n--------------------\n\nadditional review after seeing the author's response: \n\nThe author's response pointed out some of the limitation of Soudry and Carmon, and Xie et al's which I agree.[[RWK-POS], [SUB-POS]]  However, none of this limitation is addressed by this paper (or addressed in a misleading way to some extent.)[[ANA-NEG], [null]]   The key technical limitation is the dependency of the local minima on the weight parameters[[EXP-NEU], [SUB-NEU]] . Soudry and Carmon addresses this in a partial way by using the random dropout, which is a super cool idea.[[RWK-POS], [NOV-POS]]  Xie et al couldn't address this globally but show that the Jacobian is well conditioned for a class of weights.[[RWK-NEU], [NOV-NEU]]  The paper here doesn't have either and only shows that for a single fixed weight matrix, the Jacobian is well-conditioned.[[MET-NEG], [NOV-NEG]]  \n\nI don't also see the value of extension to other activation function[[MET-NEG], [SUB-NEG]] . To some extent this is not consistent with the empirical observation that relu is very important for deep learning.[[ANA-NEG], [SUB-NEG]]  \n\nRegarding the effect of randomness, since the paper only shows the convergence to a first-order optimal solution, I don't see why randomness is necessary.[[MET-NEG], [SUB-NEG]]  Gradient descent can converge to a first order optimal solution. [[MET-NEU], [SUB-NEU]] (Indeed I have a typo in my previous review regarding \"w.r.t. k-th sample\", which should be \"w.r.t. k-th update\". )[[ANA-NEU], [CLA-NEU]]  Moreover, to justify the effect of the randomness, the paper should have empirical experiments.[[EXP-NEU], [null]]  \n\nI think the writing of the paper is also misleading in several places. \n"[[OAL-NEG], [CLA-NEG]]