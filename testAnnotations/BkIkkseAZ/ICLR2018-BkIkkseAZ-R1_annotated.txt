 "The paper studies the theoretical properties of the two-layer neural networks.[[SMY], [GEN]]  \n\nTo summarize the result, let's use the theta to denote the layer closer to the label, and W to denote the layer closer to the data.[[DIS], [GEN]]  \n\nThe paper shows that \na) if W is fixed, then with respect to the randomness of the data, with prob. 1, the Jacobian matrix of the model is full rank[[DIS], [GEN]] \nb) suppose that we run an algorithm with fresh samples, then with respect to the randomness of the k-th sample, we have that with prob. 1, W_k is full rank, and the Jacobian of the model is full rank. [[DIS], [GEN]] \n\nIt's know (essentially from the proof of Carmon and Soudry) that if the Jacobian of the model is full rank for any matrix W w.r.t the randomness of the data, then all stationary points are global.[[DIS], [GEN]]  But the paper cannot establish such a result. [[CRT], [MAJ]] \n\nThe paper is not very clear, and after figuring out what it's doing, I don't feel it really provides many new things beyond C-S and Xie et al.[[CRT], [MAJ]] \n\nThe paper argues that it works for activation beyond relu but result a) is much much weaker than the one with for all quantifier for W.[[CRT], [MAJ]]  result b) is very sensitive to the exactness of the events (such as W is exactly full rank) --- the events that the paper talks just naturally never happen as long as the density of the random variables doesn't degenerate.[[CRT], [MAJ]]   \n\nAs the author admitted, the results don't provide any formal guarantees for the convergence to a global minimum.[[DFT], [MAJ]]  It's also a bit hard for me to find the techniques here provide new ideas that would potentially lead to resolving this question. [[DFT], [MIN]] \n\n--------------------\n\nadditional review after seeing the author's response: \n\nThe author's response pointed out some of the limitation of Soudry and Carmon, and Xie et al's which I agree.[[DIS], [GEN]]  However, none of this limitation is addressed by this paper (or addressed in a misleading way to some extent.)[[CRT], [MAJ]]   The key technical limitation is the dependency of the local minima on the weight parameters[[DFT], [MAJ]] . Soudry and Carmon addresses this in a partial way by using the random dropout, which is a super cool idea.[[DIS], [GEN]]  Xie et al couldn't address this globally but show that the Jacobian is well conditioned for a class of weights.[[DIS], [GEN]]  The paper here doesn't have either and only shows that for a single fixed weight matrix, the Jacobian is well-conditioned.[[CRT], [MAJ]]  \n\nI don't also see the value of extension to other activation function[[DFT], [GEN]] . To some extent this is not consistent with the empirical observation that relu is very important for deep learning.[[DFT], [MAJ]]  \n\nRegarding the effect of randomness, since the paper only shows the convergence to a first-order optimal solution, I don't see why randomness is necessary.[[DFT], [MIN]]  Gradient descent can converge to a first order optimal solution. [[DIS], [GEN]] (Indeed I have a typo in my previous review regarding \"w.r.t. k-th sample\", which should be \"w.r.t. k-th update\". )[[DIS], [GEN]]  Moreover, to justify the effect of the randomness, the paper should have empirical experiments.[[SUG], [MIN]]  \n\nI think the writing of the paper is also misleading in several places. \n"[[CRT], [MAJ]]