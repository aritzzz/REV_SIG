{"rating": "4", "confidence": "5", "review": "\"The paper studies the theoretical properties of the two-layer neural networks. \\n\\nTo summarize the result, let's use the theta to denote the layer closer to the label, and W to denote the layer closer to the data. \\n\\nThe paper shows that \\na) if W is fixed, then with respect to the randomness of the data, with prob. 1, the Jacobian matrix of the model is full rank\\nb) suppose that we run an algorithm with fresh samples, then with respect to the randomness of the k-th sample, we have that with prob. 1, W_k is full rank, and the Jacobian of the model is full rank. \\n\\nIt's know (essentially from the proof of Carmon and Soudry) that if the Jacobian of the model is full rank for any matrix W w.r.t the randomness of the data, then all stationary points are global. But the paper cannot establish such a result. \\n\\nThe paper is not very clear, and after figuring out what it's doing, I don't feel it really provides many new things beyond C-S and Xie et al.\\n\\nThe paper argues that it works for activation beyond relu but result a) is much much weaker than the one with for all quantifier for W. result b) is very sensitive to the exactness of the events (such as W is exactly full rank) --- the events that the paper talks just naturally never happen as long as the density of the random variables doesn't degenerate.  \\n\\nAs the author admitted, the results don't provide any formal guarantees for the convergence to a global minimum. It's also a bit hard for me to find the techniques here provide new ideas that would potentially lead to resolving this question. \\n\\n--------------------\\n\\nadditional review after seeing the author's response: \\n\\nThe author's response pointed out some of the limitation of Soudry and Carmon, and Xie et al's which I agree. However, none of this limitation is addressed by this paper (or addressed in a misleading way to some extent.)  The key technical limitation is the dependency of the local minima on the weight parameters. Soudry and Carmon addresses this in a partial way by using the random dropout, which is a super cool idea. Xie et al couldn't address this globally but show that the Jacobian is well conditioned for a class of weights. The paper here doesn't have either and only shows that for a single fixed weight matrix, the Jacobian is well-conditioned. \\n\\nI don't also see the value of extension to other activation function. To some extent this is not consistent with the empirical observation that relu is very important for deep learning. \\n\\nRegarding the effect of randomness, since the paper only shows the convergence to a first-order optimal solution, I don't see why randomness is necessary. Gradient descent can converge to a first order optimal solution. (Indeed I have a typo in my previous review regarding \\\"w.r.t. k-th sample\\\", which should be \\\"w.r.t. k-th update\\\". ) Moreover, to justify the effect of the randomness, the paper should have empirical experiments. \\n\\nI think the writing of the paper is also misleading in several places. \\n\"", "variance": 0, "title": " ", "Scores": {"layer_1": "9.731428571", "layer_2": "64.64071856287426", "layer_3": -0.5168030095100403}}