{"rating": "7", "confidence": "5", "review": "\"This paper aims to study some of the theoretical properties of the global optima of single-hidden layer neural networks and also the convergence to such a solution. I think there are some interesting arguments made in the paper e.g. Lemmas 4.1, 5.1, 5.2, and 5.3. However, as I started reading beyond intro I increasingly got the sense that this paper is somewhat incomplete e.g. while certain claims are made (abstract/intro) the theoretical justification are rather far from these claims. Of course there is a chance that I might be misunderstanding some things and happy to adjust my score based on the discussions here.\\n\\nDetailed comments:\\n1) My main concern is that the abstract and intro claims things that are never proven (or even stated) in the rest of the paper\\nExample 1 from abstract: \\n\\u201cWe show that for a wide class of differentiable activation functions (this class involved \\u201calmost\\u201d all functions which are not piecewise linear), we have that first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular.\\u201d\\n\\nThis is certainly not proven and in fact not formally stated anywhere in the paper. Closest result to this is Lemma 4.1 however, because the optimal solution is data dependent this lemma can not be used to conclude this. \\n\\nExample 2 from intro when comparing with other results on page 2:\\nThe authors essentially state that they have less restrictive assumptions in the form of the network or assumptions on the data (e.g. do not require Gaussianity). However as explained above the final conclusions are also significantly weaker than this prior literature so it\\u2019s a bit of apples vs oranges comparison.\\n\\n2) Page 2 minor typos\\nWe study training problem -->we study the training problem\\nIn the regime training objective--> in the regime the training objective\\n\\n3) the basic idea argument and derivative calculations in section 3 is identical to section 4 of Soltan...et al\\n\\n4) Lemma 4.1 is nice, well done! That being said it does not seem easy to make it (1) quantifiable (2) apply to all W. It would also be nice to compare with Soudry et. al.\\n\\n5) Argument on top of page 6 is incorrect as the global optima is data dependent and hence lemma 4.1 (which is for a fixed matrix) does not apply\\n\\n6) Section 5 on page 6. Again the stated conclusion here that the iterates do not lead to singular W is much weaker than the claims made early on.\\n \\n7) I haven\\u2019t had time yet to verify correctness of Lemmas 5.1, 5.2, and Lemma 5.3 in detail but if this holds is a neat argument to side step invertibility w.r.t. W, Nicely done!\\n\\n8) What is the difference between Lemma 5.4 and Lemma 6.12 of Soltan...et al \\n\\n9) Theorem 5.9. Given that the arguments in this paper do not show asymptotic convergence to a point where gradient vanishes and W is invertible why is the proposed algorithm better than a simple approach in which gradient descent is applied but a small amount of independent Gaussian noise is injected in every iteration over W. By adjusting the noise variance across time one can ensure a result of the kind in Theorem 5.9 (Of course in the absence of a quantifiable version of Lemma 4.1 which can apply to all W that result will also suffer from the same issues).\\n\"", "variance": 0, "title": " ", "Scores": {"layer_1": "9.14375", "layer_2": "68.83832335329342", "layer_3": -0.4805433042347431}}