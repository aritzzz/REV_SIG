{"rating": "2", "confidence": "4", "review": "\"Here are my main critics of the papers:\\n\\n1. Equation (1), (2), (3) are those expectations w.r.t. the data distribution (otherwise I can't think of any other stochasticity)? If so your phrase \\\"is zero given a sequence of inputs X1, ...,T\\\" is misleading. \\n2. Lack of motivation for IE or UIE. Where is your background material? I do not understand why we would like to assume (1), (2), (3). Why the same intuition of UIE can be applied to RNNs? \\n3. The paper proposed the new architecture RIN, but it is not much different than a simple RNN with identity initialization. Not much novelty.\\n4. The experimental results are not convincing. It's not compared against any previous published results. E.g. the addition tasks and sMNIST tasks are not as good as those reported in [1]. Also it only has been tested on very simple datasets.\\n\\n\\n[1] Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations. Behnam Neyshabur, Yuhuai Wu, Ruslan Salakhutdinov, Nathan Srebro.\"", "variance": 0, "title": " "}