{"rating": "7", "confidence": "4", "review": "\"Summary: \\nThe authors present a simple variation of vanilla recurrent neural networks, which use ReLU hiddens and a fixed identity matrix that is added to the hidden-to-hidden weight matrix. This identity connection acts as a \\u201csurrogate memory\\u201d component, preserving hidden activations over time steps. \\nThe experiments demonstrate that this architecture reliably solves the addition task for up to 400 input frames. It also achieves a very good performance on sequential and permuted MNIST and achieves SOTA performance on bAbI.\\nThe authors observe that the proposed recurrent identity network (RIN) is relatively robust to hyperparameter choices. After Le et al. (2015), the paper presents another convincing case for the application of ReLUs in RNNs.\\n\\nReview: \\nI very much like the paper. The motivation and architecture is presented very clearly and I am happy to also see explorations of simpler recurrent architectures in parallel to research of gated architectures!\\nI have a few comments and questions:\\n1) Clarification: In Section 2.2, do you really mean bit-wise multiplication or element-wise? If bit-wise, can you elaborate why? I might have missed something.\\n2) Why does the learning curve of the IRNN stop around epoch 270 in Figure 2c? Also some curves in the appendix stop abruptly without visible explosions. Were these experiments run until completion? If so, would it be possible to plot the complete curves?\\n3) I think for a fair comparison with LSTMs and IRNNs a limited hyperparameter search should be performed separately on all three architectures at least for the addition task. Optimal hyperparameters are usually model-specific. Admittedly, the authors mention that they do not intend to make claims about superior performance to LSTMs, however the competitive performance of small RINs is mentioned a couple of times in the manuscript.\\nLe et al. (2015) for instance perform a coarse grid search for each model.\\n4) I wouldn't say that ResNets are Gated Neural Networks, as the branches are just summed up. There is no (multiplicative) gating as in Highway Networks.\\n5) I think what enables the training of very deep networks or LSTMs on long sequences is the presence of a (close-to-)identity component in forward/backward propagation, not the gating. The use of ReLU activations in IRNNs (with identity initialization of the hidden-to-hidden weights) and RINs (effectively initialized with identity plus some noise) makes the recurrence more linear than with squashing activation functions.\\n6) Regarding the absence of gating in RINs: What is your intuition on how the model would perform in tasks for which conditional forgetting is useful. Consider for example a task with long sequences, outputs at every time step and hidden activations not necessarily being encouraged to estimate last step hidden activations. Would RINs readily learn to reset parts of the hidden state?\\n7) Henaff et al. (2016) might be related, as they are also looking into the addition task with long sequences.\\n\\nOverall, the presented idea is novel to the best of my knowledge and the manuscript is well-written. I would recommend it for acceptance, but would like to see the above points addressed (especially 1-3 and some comments on 4-6). After a revision I would consider to increase the score.\\n\\nReferences:\\nHenaff, Mikael, Arthur Szlam, and Yann LeCun. \\\"Recurrent orthogonal networks and long-memory tasks.\\\" In International Conference on Machine Learning, pp. 2034-2042. 2016.\\nLe, Quoc V., Navdeep Jaitly, and Geoffrey E. Hinton. \\\"A simple way to initialize recurrent networks of rectified linear units.\\\" arXiv preprint arXiv:1504.00941 (2015).\"", "variance": 0, "title": " ", "Scores": {"layer_1": "16.5871118", "layer_2": "142.2018316308559", "layer_3": 0.044264789138521464}}