 "Here are my main critics of the papers:\n\n1. Equation (1), (2), (3) are those expectations w.r.t. the data distribution (otherwise I can't think of any other stochasticity)?[[MET-NEG], [EMP-NEU]]  If so your phrase \"is zero given a sequence of inputs X1, ...,T\" is misleading.[[MET-NEG], [EMP-NEU]]  \n2. Lack of motivation for IE or UIE.[[MET-NEG], [EMP-NEU]]  Where is your background material?[[CNT], [EMP-NEU]]  I do not understand why we would like to assume (1), (2), (3).[[MET-NEG], [EMP-NEU]]  Why the same intuition of UIE can be applied to RNNs?[[MET-NEG], [EMP-NEG]]  \n3. The paper proposed the new architecture RIN, but it is not much different than a simple RNN with identity initialization.[[MET-NEG], [CMP-NEG]]  Not much novelty.[[OAL-NEG], [NOV-NEG]] \n4. The experimental results are not convincing.[[EXP-NEG,RES-NEG], [EMP-NEG]]  It's not compared against any previous published results.[[RWK-NEG,RES-NEG], [CMP-NEG]]  E.g. the addition tasks and sMNIST tasks are not as good as those reported in [1].[[DAT-NEG,MET-NEG], [EMP-NEG]]  Also it only has been tested on very simple datasets.[[DAT-NEG], [EMP-NEG]] \n\n\n[1] Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations.[[RWK-NEU,BIB-NEU], [SUB-NEU]]  Behnam Neyshabur, Yuhuai Wu, Ruslan Salakhutdinov, Nathan Srebro."[[RWK-NEU,BIB-NEU], [SUB-NEU]]