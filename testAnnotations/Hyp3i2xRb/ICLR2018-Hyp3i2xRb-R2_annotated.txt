 "Here are my main critics of the papers:\n\n1. Equation (1), (2), (3) are those expectations w.r.t. the data distribution (otherwise I can't think of any other stochasticity)?[[CRT], [MAJ]]  If so your phrase \"is zero given a sequence of inputs X1, ...,T\" is misleading.[[CRT], [MAJ]]  \n2. Lack of motivation for IE or UIE.[[CRT], [MAJ]]  Where is your background material?[[CRT], [MAJ]]  I do not understand why we would like to assume (1), (2), (3).[[CRT], [MAJ]]  Why the same intuition of UIE can be applied to RNNs?[[QSN,CRT], [MAJ]]  \n3. The paper proposed the new architecture RIN, but it is not much different than a simple RNN with identity initialization.[[CRT], [MIN]]  Not much novelty.[[CRT], [MAJ]] \n4. The experimental results are not convincing.[[CRT], [MAJ]]  It's not compared against any previous published results.[[CRT], [MAJ]]  E.g. the addition tasks and sMNIST tasks are not as good as those reported in [1].[[CRT], [MAJ]]  Also it only has been tested on very simple datasets.[[CRT], [MAJ]] \n\n\n[1] Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations.[[DIS], [MIN]]  Behnam Neyshabur, Yuhuai Wu, Ruslan Salakhutdinov, Nathan Srebro."[[DIS], [MIN]]