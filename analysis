##############
{'RECOMMENDATION': '6', 'REVIEW TITLE': ' ', 'comments': '"This paper investigates the complexity of neural networks with piecewise linear activations by studying the number of linear regions of the representable functions. It builds on previous works Montufar et al. (2014) and Raghu et al. (2017) and presents improved bounds on the maximum number of linear regions. It also evaluates the number of regions of small networks during training. \\n\\nThe improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning ``Notes on the number of linear regions of deep neural networks\'\' by Montufar. \\n\\nThe improved lower bound given in Theorem 6 is very modest but neat. Theorem 5 follows easily from this. \\n\\nThe improved upper bound for maxout networks follows a similar intuition but appears to be novel. \\n\\nThe paper also discusses the exact computation of the number of linear regions in small trained networks. It presents experiments during training and with varying network sizes. These give an interesting picture, consistent with the theoretical bounds, and showing the behaviour during training. \\n\\nHere it would be interesting to run more experiments to see how the number of regions might relate to the quality of the trained hypotheses. \\n\\n\\n\\n"', 'VARIANCE': 0, 'CONFIDENCE': '5', 'SCORES': [4.022857143, 50.658682634730546, 0.6850545314642099], 'PREDICTIONS': {'layer1': (2.1663990020751953, 2.0755391120910645), 'layer2': (3.0523781776428223, 2.7808210849761963), 'layer3': (5.5529398918151855, 8.582745552062988)}, 'NORMALISED': array([2.07553903, 2.78082102, 0.68505453])}##############

##############
{'RECOMMENDATION': '6', 'REVIEW TITLE': ' ', 'comments': '"The paper proposes to make the inner layers in a neural network be block diagonal, mainly as an alternative to pruning. The implementation of this seems straightforward, and can be done either via initialization or via pruning on the off-diagonals. There are a few ideas the paper discusses:\\n\\n(1) compared to pruning weight matrices and making them sparse, block diagonal matrices are more efficient since they utilize level 3 BLAS rather than sparse operations which have significant overhead and are not \\"worth it\\" until the matrix is extremely sparse. I think this case is well supported via their experiments, and I largely agree.\\n\\n(2) that therefore, block diagonal layers lead to more efficient networks. This point is murkier, because the paper doesn\'t discuss possible increases in *training time* (due to increased number of iterations) in much detail. At if we only care about running the net, then reducing the time from 0.4s to 0.2s doesn\'t seem to be that useful (maybe it is for real-time predictions? Please cite some work in that case)\\n\\n(3) to summarize points (1) and (2), block diagonal architectures are a nice alternative to pruned architectures, with similar accuracy, and more benefit to speed (mainly speed at run-time, or speed of a single iteration, not necessarily speed to train)\\n\\n[as I am not primarly a neural net researcher, I had always thought pruning was done to decrease over-fitting, not to increase computation speed, so this was a surprise to me; also note that the sparse matrix format can increase runtime if implemented as a sparse object, as demonstrated in this paper, but one could always pretend it is sparse, so you never ought to be slower with a sparse matrix]\\n\\n(4) there is some vague connection to random matrices, with some limited experiments that are consistent with this observation but far from establish it, and without any theoretical analysis (Martingale or Markov chain theory)\\n\\nThis is an experimental/methods paper that proposes a new algorithm, explained only in general details, and backs up it up with two reasonable experiments (that do a good job of convincing me of point (1) above). The authors seem to restrict themselves to convolutional networks in the first paragraph (and experiments) but don\'t discuss the implications or reasons of this assumption. The authors seem to understand the literature well, and not being an expert myself, I have the impression they are doing a fair job.\\n\\n\\nThe paper could have gone farther experimentally (or theoretically) in my opinion. For example, with sparse and block diagonal matrices, reducing the size of the matrix to fit into the cache on the GPU must obviously make a difference, but this did not seem to be investigated. I was also wondering about when 2 or more layers are block sparse, do these blocks overlap? i.e., are they randomly permuted between layers so that the blocks mix? And even with a single block, does it matter what permutation you use? (or perhaps does it not matter due to the convolutional structure?)\\n\\nThe section on the variance of the weights is rather unclear mathematically, starting with the abstract and even continuing into the paper. We are talking about sample variance? What does DeltaVar mean in eq (2)? The Marchenko-Pastur theorem seemed to even be imprecise, since if y>1, then a < 0, implying that there is a nonzero chance that the positive semi-definite matrix XX\' has a negative eigenvalue.\\n\\nI agree this relationship with random matrices could be interesting, but it seems too vague right now. Is there some central limit theorem explanation? Are you sure that you\'ve run enough iterations to fully converge? (Fig 4 was still trending up for b1=64). Was it due to the convolutional net structure (you could test this)? Or, perhaps train a network on two datasets, one which is not learnable (iid random labels), and one which is very easily learnable (e.g., linearly separable). Would this affect the distributions?\\n\\nFurthermore, I think I misunderstood parts, because the scaling in MNIST and CIFAR was different and I didn\'t see why (for MNIST, it was proportional to block size, and for CIFAR it was independent of block size almost).\\n\\nMinor comment: last paragraph of 4.1, comparing with Sindhwani et al., was confusing to me. Why was this mentioned? And it doesn\'t seem to be comparable. I have no idea what \\"Toeplitz (3)\\" is."', 'VARIANCE': 0, 'CONFIDENCE': '4', 'SCORES': [19.98857143, 172.10214864388874, -0.8538342693022319], 'PREDICTIONS': {'layer1': (5.595800876617432, 6.402473449707031), 'layer2': (7.068610191345215, 7.1298136711120605), 'layer3': (3.1140756607055664, 1.6577458381652832)}, 'NORMALISED': array([ 6.40247358,  7.12981388, -0.85383427])}##############

##############
{'RECOMMENDATION': '8', 'REVIEW TITLE': ' ', 'comments': '"The authors train an RNN to perform deduced reckoning (ded reckoning) for spatial navigation, and then study the responses of the model neurons in the RNN. They find many properties reminiscent of neurons in the mammalian entorhinal cortex (EC): grid cells, border cells, etc. When regularization of the network is not used during training, the trained RNNs no longer resemble the EC. This suggests that those constraints (lower overall connectivity strengths, and lower metabolic costs) might play a role in the EC\'s navigation function. \\n\\nThe paper is overall quite interesting and the study is pretty thorough: no major cons come to mind. Some suggestions / criticisms are given below.\\n\\n1) The findings seem conceptually similar to the older sparse coding ideas from the visual cortex. That connection might be worth discussing because removing the regularizing (i.e., metabolic cost) constraint from your RNNS makes them learn representations that differ from the ones seen in EC. The sparse coding models see something similar: without sparsity constraints, the image representations do not resemble those seen in V1, but with sparsity, the learned representations match V1 quite well. That the same observation is made in such disparate brain areas (V1, EC) suggests that sparsity / efficiency might be quite universal constraints on the neural code.\\n\\n2) The finding that regularizing the RNN makes it more closely match the neural code is also foreshadowed somewhat by the 2015 Nature Neuro paper by Susillo et al. That could be worthy of some (brief) discussion.\\n\\nSussillo, D., Churchland, M. M., Kaufman, M. T., & Shenoy, K. V. (2015). A neural network that finds a naturalistic solution for the production of muscle activity. Nature neuroscience, 18(7), 1025-1033.\\n\\n3) Why the different initializations for the recurrent weights for the hexagonal vs other environments? I\'m guessing it\'s because the RNNs don\'t \\"work\\" in all environments with the same initialization (i.e., they either don\'t look like EC, or they don\'t obtain small errors in the navigation task). That seems important to explain more thoroughly than is done in the current text.\\n\\n4) What happens with ongoing training? Animals presumably continue to learn throughout their lives. With on-going (continous) training, do the RNN neurons\' spatial tuning remain stable, or do they continue to \\"drift\\" (so that border cells turn into grid cells turn into irregular cells, or some such)? That result could make some predictions for experiment, that would be testable with chronic methods (like Ca2+ imaging) that can record from the same neurons over multiple experimental sessions.\\n\\n5) It would be nice to more quantitatively map out the relation between speed tuning, direction tuning, and spatial tuning (illustrated in Fig. 3). Specifically, I would quantify the cells\' direction tuning using the circular variance methods that people use for studying retinal direction selective neurons. And I would quantify speed tuning via something like the slope of the firing rate vs speed curves. And quantify spatial tuning somehow (a natural method would be to use the sparsity measures sometimes applied to neural data to quantify how selective the spatial profile is to one or a few specific locations). Then make scatter plots of these quantities against each other. Basically, I\'d love to see the trends for how these types of tuning relate to each other over the whole populations: those trends could then be tested against experimental data (possibly in a future study)."', 'VARIANCE': 0, 'CONFIDENCE': '4', 'SCORES': [15.37752101, 75.84131736526946, -0.3662371505861697], 'PREDICTIONS': {'layer1': (4.050241947174072, 5.15281343460083), 'layer2': (5.1738176345825195, 3.6826324462890625), 'layer3': (3.599306583404541, 3.8519327640533447)}, 'NORMALISED': array([ 5.15281365,  3.68263239, -0.36623715])}##############

##############
{'RECOMMENDATION': '7', 'REVIEW TITLE': ' ', 'comments': '"This paper thoroughly analyzes an algorithmic task (determining if two points in a maze are connected, which requires BFS to solve) by constructing an explicit ConvNet solution and analytically deriving properties of the loss surface around this analytical solution. They show that their analytical solution implements a form of BFS algorithm, characterize the probability of introducing \\"bugs\\" in the algorithm as the weights move away from the optimal solution, and how this influences the error surface for different depths. This analysis is conducted by drawing on results from the field of critical percolation in physics.\\n\\nOverall, I think this is a good paper and its core contribution is definitely valuable: it provides a novel analysis of an algorithmic task which sheds light on how and when the network fails to learn the algorithm, and in particular the role which initialization plays. The analysis is very thorough and the methods described may find use in analyzing other tasks. In particular, this could be a first step towards better understanding the optimization landscape of memory-augmented neural networks (Memory Networks, Neural Turing Machines, etc) which try to learn reasoning tasks or algorithms. It is well-known that these are sensitive to initialization and often require running the optimizer with multiple random seeds and picking the best one. This work actually explains the role of initialization for learning BFS and how certain types of initialization lead to poor solutions. I am curious if a similar analysis could be applied to methods evaluated on the bAbI question-answering tasks (which can be represented as graphs, like the maze task) and possibly yield better initialization or optimization schemes that would remove the need for multiple random seeds.  \\n\\nWith that being said, there is some work that needs to be done to make the paper clearer. In particular, many parts are quite technical and may not be accessible to a broader machine learning audience. It would be good if the authors spent more time developing intuition (through visualization for example) and move some of the more technical proofs to the appendix. Specifically:\\n- I think Figure 3 in the appendix should be moved to the main text, to help understand the behavior of the analytical solution. \\n- Top of page 5, when you describe the checkerboard BFS: please include a visualization somewhere, it could be in the Appendix.\\n- Section 6: there is lots of math here, but the main results don\'t obviously stand out. I would suggest highlighting equations 2 and 4 in some way (for example, proposition/lemma + proof), so that the casual reader can quickly see what the main results are. Interested readers can then work through the math if they want to. Also, some plots/visualizations of the loss surface given in Equations 4 and 5 would be very helpful. \\n\\nAlso, although I found their work to be interesting after finishing the paper, I was initially confused by how the authors frame their work and where the paper was heading. They claim their contribution is in the analysis of loss surfaces (true) and neural nets applied to graph-structured inputs. This second part was confusing - although the maze can be viewed as a graph, many other works apply ConvNets to maze environments [1, 2, 3], and their work has little relation to other work on graph CNNs. Here the assumptions of locality and stationarity underlying CNNs are sensible and I don\'t think the first paragraph in Section 3 justifying the use of the CNN on the maze environment is necessary. However, I think it would make much more sense to mention how their work relates to other neural network architectures which learn algorithms (such as the Neural Turing Machine and variants) or reasoning tasks more generally (for example, memory-augmented networks applied to the bAbI tasks). \\n\\nThere are lots of small typos, please fix them. Here are a few:\\n- \\"For L=16, batch size of 20, ...\\": not a complete sentence. \\n- Right before 6.1.1: \\"when the these such\\" -> \\"when such\\"\\n- Top of page 8: \\"it also have a\\" -> \\"it also has a\\", \\"when encountering larger dataset\\" -> \\"...datasets\\"\\n-  First sentence of 6.2: \\"we turn to the discuss a second\\" -> \\"we turn to the discussion of a second\\"\\n- etc. \\n\\nQuality: High\\nClarity: medium-low\\nOriginality: high\\nSignificance: medium-high\\n\\nReferences:\\n[1] https://arxiv.org/pdf/1602.02867.pdf\\n[2] https://arxiv.org/pdf/1612.08810.pdf\\n[3] https://arxiv.org/pdf/1707.03497.pdf"', 'VARIANCE': 0, 'CONFIDENCE': '3', 'SCORES': [19.74278746, 197.90344311377245, -0.47254096031188964], 'PREDICTIONS': {'layer1': (5.415947437286377, 6.335862636566162), 'layer2': (6.850130081176758, 8.053779602050781), 'layer3': (3.3324456214904785, 3.373565673828125)}, 'NORMALISED': array([ 6.33586265,  8.05377997, -0.47254096])}##############

##############
{'RECOMMENDATION': '6', 'REVIEW TITLE': ' ', 'comments': '"Pros: \\nThe paper proposes a \\u201cbi-directional block self-attention network (Bi-BloSAN)\\u201d for sequence encoding, which inherits the advantages of multi-head (Vaswani et al., 2017) and DiSAN (Shen et al., 2017) network but is claimed to be more memory-efficient. The paper is written clearly and is easy to follow. The source code is released for duplicability. The main originality is using block (or hierarchical) structures; i.e., the proposed models split the an entire sequence into blocks, apply an intra-block SAN to each block for modeling local context, and then apply an inter-block SAN to the output for all blocks to capture long-range dependency. The proposed model was tested on nine benchmarks  and achieve good efficiency-memory trade-off. \\n\\nCons:\\n- Methodology of the paper is very incremental compared with previous models.  \\n- Many of the baselines listed in the paper are not competitive; e.g.,  for SNLI, state-of-the-art results are not included in the paper. \\n- The paper argues advantages of the proposed models over CNN by assuming the latter only captures local dependency, which, however, is not supported by discussion on or comparison with hierarchical CNN.\\n- The block splitting (as detailed in appendix) is rather arbitrary in terms of that it potentially divides coherent language segments apart. This is unnatural, e.g., compared  with alternatives such as using linguistic segments as blocks.\\n- The main originality of paper is the block style. However, the paper doesn\\u2019t analyze how and why the block brings improvement. \\n-If we remove intra-block self-attention (but only keep token-level self-attention), whether the performance will be significantly worse?\\n"', 'VARIANCE': 0, 'CONFIDENCE': '4', 'SCORES': [3.728571429, 34.87025948103793, -0.6298991441726685], 'PREDICTIONS': {'layer1': (3.0105276107788086, 1.9957834482192993), 'layer2': (3.9289984703063965, 2.2154242992401123), 'layer3': (4.201791286468506, 2.6654539108276367)}, 'NORMALISED': array([ 1.99578344,  2.21542429, -0.62989914])}##############

##############
{'RECOMMENDATION': '5', 'REVIEW TITLE': ' ', 'comments': '"The paper addresses the problem of tensor decomposition which is relevant and interesting. The paper proposes Tensor Ring (TR) decomposition which improves over and bases on the Tensor Train (TT) decomposition method. TT decomposes a tensor in to a sequences of latent tensors where the first and last tensors are a 2D matrices. \\n\\nThe proposed TR method generalizes TT in that the first and last tensors are also 3rd-order tensors instead of 2nd-order. I think such generalization is interesting but the innovation seems to be very limited. \\n\\nThe paper develops three different kinds of solvers for TR decomposition, i.e., SVD, ALS and SGD. All of these are well known methods. \\n\\nFinally, the paper provides experimental results on synthetic data (3 oscillated functions) and image data (few sampled images). I think the paper could be greatly improved by providing more experiments and ablations to validate the benefits of the proposed methods.\\n\\nPlease refer to below for more comments and questions.\\n\\n-- The rating has been updated.\\n\\nPros:\\n1. The topic is interesting.\\n2. The generalization over TT makes sense.\\n\\nCons:\\n1. The writing of the paper could be improved and more clear: the conclusions on inner product and F-norm can be integrated into \\"Theorem 5\\". And those \\"theorems\\" in section 4 are just some properties from previous definitions; they are not theorems. \\n2. The property of TR decomposition is that the tensors can be shifted (circular invariance). This is an interesting property and it seems to be the major strength of TR over TT. I think the paper could be significantly improved by providing more applications of this property in both theory and experiments.\\n3. As the number of latent tensors increase, the ALS method becomes much worse approximation of the original optimization. Any insights or results on the optimization performance vs. the number of latent tensors?\\n4. Also, the paper mentions Eq. 5 (ALS) is optimized by solving d subproblems alternatively. I think this only contains a single round of optimization. Should ALS be applied repeated (each round solves d problems) until convergence?\\n5. What is the memory consumption for different solvers?\\n6. SGD also needs to update at least d times for all d latent tensors. Why is the complexity O(r^3) independent of the parameter d?\\n7. The ALS is so slow (if looking at the results in section 5.1), which becomes not practical. The experimental part could be improved by providing more results and description about a guidance on how to choose from different solvers.\\n8. What does \\"iteration\\" mean in experimental results such as table 2? Different algorithms have different cost for \\"each iteration\\" so comparing that seems not fair. The results could make more sense by providing total time consumptions and time cost per iteration. also applies to table 4.\\n9. Why is the \\\\epsion in table 3 not consistent? Why not choose \\\\epsion = 9e-4 and \\\\epsilon=2e-15 for tensorization?\\n10. Also, table 3 could be greatly improved by providing more ablations such as results for (n=16, d=8), (n=4, d=4), etc. That could help readers to better understand the effect of TR.\\n11. Section 5.3 could be improved by providing a curve (compression vs. error) instead of just providing a table of sampled operating points.\\n12. The paper mentions the application of image representation but only experiment on 32x32 images. How does the proposed method handle large images? Otherwise, it does not seem to be a practical application.\\n13. Figure 5: Are the RSE measures computed over the whole CIFAR-10 dataset or the displayed images?\\n\\nMinor:\\n- Typo: Page 4 Line 7 \\"Note that this algorithm use the similar strategy\\": use -> uses"', 'VARIANCE': 0, 'CONFIDENCE': '4', 'SCORES': [23.49875, 281.7945359281438, -0.49087591287566396], 'PREDICTIONS': {'layer1': (6.417518615722656, 7.353781700134277), 'layer2': (8.215941429138184, 11.057991027832031), 'layer3': (3.947037696838379, 3.291058301925659)}, 'NORMALISED': array([ 7.35378166, 11.0579907 , -0.49087591])}##############

##############
{'RECOMMENDATION': '7', 'REVIEW TITLE': ' ', 'comments': '"The authors has addressed my concerns, so I raised my rating. \\n\\nThe paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting.\\n\\nThere are no results on large corpora such as 1 billion tokens benchmark corpus, or at least medium level corpus with 50 million tokens. The corpora the authors choose are quite small, the variance of the estimates are high, and similar conclusions might not be valid on a large corpus. \\n\\n[1] provides the results of character level language models on Enwik8 dataset, which shows regularization doesn\'t have much effect and needs less tuning. Results on this data might be more convincing.\\n\\nThe results of MOS is very good, but the computation complexity is much higher than other baselines. In the experiments, the embedding dimension of MOS is slightly smaller, but the number of mixture is 15. This will make it less usable, I think it\'s necessary to provide the training time comparison.\\n\\nFinally experiments on machine translation or speech recognition should be done and to see what improvements the proposed method could bring for BLEU or WER. \\n\\n[1] Melis, G\\u00e1bor, Chris Dyer, and Phil Blunsom. \\"On the state of the art of evaluation in neural language models.\\" arXiv preprint arXiv:1707.05589 (2017).\\n\\n[2] Joris Pelemans, Noam Shazeer, Ciprian Chelba, Sparse Non-negative Matrix Language Modeling,  Transactions of the Association for Computational Linguistics, vol. 4 (2016), pp. 329-342\\n\\n[3] Shazeer et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. ICLR 2017\\n"', 'VARIANCE': 0, 'CONFIDENCE': '5', 'SCORES': [4.911801242, 24.206586826347305, 0.03173989802598953], 'PREDICTIONS': {'layer1': (2.594576597213745, 2.316455364227295), 'layer2': (3.257692337036133, 1.83354914188385), 'layer3': (3.9044337272644043, 5.642829418182373)}, 'NORMALISED': array([2.31645546, 1.83354918, 0.0317399 ])}##############

##############
{'RECOMMENDATION': '4', 'REVIEW TITLE': ' ', 'comments': '"The paper studies the theoretical properties of the two-layer neural networks. \\n\\nTo summarize the result, let\'s use the theta to denote the layer closer to the label, and W to denote the layer closer to the data. \\n\\nThe paper shows that \\na) if W is fixed, then with respect to the randomness of the data, with prob. 1, the Jacobian matrix of the model is full rank\\nb) suppose that we run an algorithm with fresh samples, then with respect to the randomness of the k-th sample, we have that with prob. 1, W_k is full rank, and the Jacobian of the model is full rank. \\n\\nIt\'s know (essentially from the proof of Carmon and Soudry) that if the Jacobian of the model is full rank for any matrix W w.r.t the randomness of the data, then all stationary points are global. But the paper cannot establish such a result. \\n\\nThe paper is not very clear, and after figuring out what it\'s doing, I don\'t feel it really provides many new things beyond C-S and Xie et al.\\n\\nThe paper argues that it works for activation beyond relu but result a) is much much weaker than the one with for all quantifier for W. result b) is very sensitive to the exactness of the events (such as W is exactly full rank) --- the events that the paper talks just naturally never happen as long as the density of the random variables doesn\'t degenerate.  \\n\\nAs the author admitted, the results don\'t provide any formal guarantees for the convergence to a global minimum. It\'s also a bit hard for me to find the techniques here provide new ideas that would potentially lead to resolving this question. \\n\\n--------------------\\n\\nadditional review after seeing the author\'s response: \\n\\nThe author\'s response pointed out some of the limitation of Soudry and Carmon, and Xie et al\'s which I agree. However, none of this limitation is addressed by this paper (or addressed in a misleading way to some extent.)  The key technical limitation is the dependency of the local minima on the weight parameters. Soudry and Carmon addresses this in a partial way by using the random dropout, which is a super cool idea. Xie et al couldn\'t address this globally but show that the Jacobian is well conditioned for a class of weights. The paper here doesn\'t have either and only shows that for a single fixed weight matrix, the Jacobian is well-conditioned. \\n\\nI don\'t also see the value of extension to other activation function. To some extent this is not consistent with the empirical observation that relu is very important for deep learning. \\n\\nRegarding the effect of randomness, since the paper only shows the convergence to a first-order optimal solution, I don\'t see why randomness is necessary. Gradient descent can converge to a first order optimal solution. (Indeed I have a typo in my previous review regarding \\"w.r.t. k-th sample\\", which should be \\"w.r.t. k-th update\\". ) Moreover, to justify the effect of the randomness, the paper should have empirical experiments. \\n\\nI think the writing of the paper is also misleading in several places. \\n"', 'VARIANCE': 0, 'CONFIDENCE': '5', 'SCORES': [9.731428571, 64.64071856287426, -0.5168030095100403], 'PREDICTIONS': {'layer1': (4.140858173370361, 3.622642755508423), 'layer2': (5.280961036682129, 3.281529426574707), 'layer3': (3.574413776397705, 3.174386501312256)}, 'NORMALISED': array([ 3.62264269,  3.28152951, -0.51680301])}##############

##############
{'RECOMMENDATION': '7', 'REVIEW TITLE': ' ', 'comments': '"Summary:\\n The paper presents an unsupervised method for detecting adversarial examples of neural networks. The method includes two independent components: an \\u2018input defender\\u2019 which tried to inspect the input, and a \\u2018latent defender\\u2019 trying to inspect a hidden representation. Both are based on the claim that adversarial examples lie outside a certain sub-space occupied by the natural image examples, and modeling this sub-space hence enables their detection. The input defender is based on sparse coding, and the latent defender on modeling the latent activity as a mixture of Gaussians. Experiments are presented on MInst, Cifar10, and ImageNet.\\n \\n-\\tIntroduction: The motivation for detecting adversarial examples is not stated clearly enough. How can such examples be used by a malicious agent to cause damage to a system? Sketching some such scenarios would help the reader understand why the issue is practically important. I was not convinced it is. \\nPage 4: \\n-\\tStep 3 of the algorithm is not clear:\\no\\tHow exactly does HDDA model the data (formally) and how does it estimate the parameters? In the current version, the paper does not explain the HDDA formalism and learning algorithm, which is a main building block in the proposed system (as it provides the density score used for adversarial examples detection). Hence the paper cannot be read as a standalone document. I went on to read the relevant HDDA paper, but it is also not clear which of the model variants presented there is used in this paper.\\no\\tWhat is the relation between the model learned at stage 2 (the centers c^i) and the model learnt by HDDA? Are they completely different models? Or are the C^I used when learning the HDDA model (and how)? \\nIf these are separate models, how are they used in conjunction to give a final density score? If I understand correctly, only the HDDA model is used to get the final score, and the C^i are only used to make the \\\\phy(x) representation more class-seperable. Is that right?\\n-\\tFigure 4, b and c: it is not clear what the (x,y,z) measurements plotted in these 3D drawings are (what are the axis).\\nPage 5:\\n-\\tSection 2: the risk analysis is done in a standard Bayesian way and leads to a ratio of PDFs in equation 5. However, this form is not appropriate for the case presented at this paper, since the method presented only models one of these PDFs (Specifically p(x | W1)  - there is not generative model of p(x|W2)).  \\n-\\tThe authors claim in the last sentence of the section that p(x|W2) is equivalent to 1-p(x|W1), but this is not true: these are two continuous densities, they do not sum to 1, and a model of p(x|W2) is not available (as far as I understand the method)\\nPage 6:\\n-\\tHow is equation 7) optimized?\\n-\\tWhich patchs are extracted from images, for training and at inference time? Are these patchs a dense coverage of the image? Sparsely sampled? Densely sampled with overlaps?\\n-\\tIts not clear enough what exactly is the \\u2018PSNR\\u2019 value which is used for the adversarial example detection, and what exactly is \\u2018profile the PSNR of legitimate samples within each class\\u2019. A formal definition of PSNR and\\u2019profiling\\u2019 is missing (does profiling simply mean finding a threshold for filtering?)\\nPage 7:\\n-\\tFigure 7 is not very informative. Given the ROC curves in figure 8  and table 1 it is redundant. \\n\\nPage 8:\\n-\\tThe results in general indicate that the method is much better than chance, but it is not clear if it is practical, because the false alarm rates for high detection are quite high. For example on ImageNet, 14.2% of the innocent images are mistakenly rejected as malicious to get 90% detection rate. I do not think this working point is useful for a real application\\n-\\tGiven the high flares alarm rate, it is surprising that experiments with multiple checkpoints are not presented (specifically as this case of multiple checkpoints is discussed explicitly in previous sections of the paper).  Experiments with multiple checkpoints are clear required to complete the picture regarding the empirical performance of this method\\n-\\tThe experiments show that essentially, the latent defenders are stronger than the input defender in most cases. However, an ablation study of the latent defender is missing: Specifcially, it is not clear a) how much does stage 2 (model refinement with clusters)  contribute to the accuracy (how does the model do without it? And 3) how important is the HDDA and the specific variant used (which is not clear) important: is it important to model the Gaussians using a sub-space? Of which dimension?\\n\\nOverall:\\nPros:\\n-\\t A nice idea with some novelty,  based on a non-trivial observation\\n-\\tThe experimental results how the idea holds some promise\\nCons\\n-\\tThe method is not presented clearly enough: the main component modeling the network activity is not explained (the HDDA module used)\\n-\\tThe results presented show that the method is probably not suitable for a practical application yet (high false alarm rate for good detection rate)\\n-\\tExperimental results are partial: results are not presented for multiple defenders, no ablation experiments\\n\\n\\nAfter revision:\\nSome of my comments were addressed, and some were not.\\nSpecifically, results were presented for multiple defenders and some ablation experiments were highlihgted\\nThings not addressed:\\n - The risk analysis is still not relevant. The authors removed a clearly flawed sentence, but the analysis still assumes that two densities (of \'good\' and \'bad\' examples) are modeled, while in the work presented only one of them is. Hence this analysis does not add anything to the paper-  it states a general case which does not fit the current scenario and its relation to the work is not clear. It would have been better to omit it and use the space to describe HDDA and the specific variant used in this work, as this is the main tool doing the distinction.\\n\\nI believe the paper should be accepted.\\n"', 'VARIANCE': 0, 'CONFIDENCE': '3', 'SCORES': [28.76591667, 328.4349700598803, -0.9334554639127519], 'PREDICTIONS': {'layer1': (8.404945373535156, 8.781258583068848), 'layer2': (10.596014976501465, 12.72822380065918), 'layer3': (2.6522088050842285, 1.299450397491455)}, 'NORMALISED': array([ 8.78125838, 12.72822394, -0.93345546])}##############

