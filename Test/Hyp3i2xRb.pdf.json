{
  "name" : "Hyp3i2xRb.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Numerous methods have been proposed for mitigating the vanishing gradient problem including the use of second-order optimization methods (e.g., Hessian-free optimization (Martens & Sutskever, 2011)), specific training schedules (e.g., Greedy Layer-wise training (Schmidhuber, 1992; Hinton et al., 2006; Vincent et al., 2008)), and special weight initialization methods when training on both plain FFNs and RNNs (Glorot & Bengio, 2010; Mishkin & Matas, 2015; Le et al., 2015; Jing et al., 2016; Xie et al., 2017; Jing et al., 2017).\nGated Neural Networks (GNNs) also help to mitigate this problem by introducing “gates” to control information flow through the network over layers or sequences. Notable examples include recurrent networks such as Long-short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Chung et al., 2014; Cho et al., 2014), and feedforward networks such as Highway Networks (HNs) (Srivastava et al., 2015), and Residual Networks (ResNets) (He et al., 2015). One can successfully train very deep models by employing these models, e.g., ResNets can be trained with over 1,000 layers. It has been demonstrated that removing (lesioning) or reordering (re-shuffling) random layers in deep feedforward GNNs does not noticeable affect the performance of the network (Veit et al., 2016) Noticeably, one interpretation for this effect as given by Greff et al. (2016) is that the functional blocks in HNs or ResNets engage in an Unrolled Iterative Estimate (UIE) of representations and that layers in this block of HNs or ResNets iteratively refine a single set of representations.\nIn this paper, we investigate if the view of Iterative Estimation (IE) can also be applied towards recurrent GNNs (Section 2.1). We present a formal analysis for GNNs by examining a dual gate design common in LSTM and GRU (Section 2.2). The analysis suggests that the use of gates in GNNs encourages the network to learn an identity mapping which can be beneficial in training deep architectures (He et al., 2016; Greff et al., 2016).\nWe propose a new formulation of a plain RNN, called a Recurrent Identity Network (RIN), that is encouraged to learn an identity mapping without the use of gates (Section 2). This network uses ReLU as the activation function and contains a set of non-trainable parameters. This simple yet effective method helps the plain recurrent network to overcome the vanishing gradient problem while it is still able to model long-range dependencies. This network is compared against two competing networks, the IRNN (Le et al., 2015) and LSTM, on several long sequence modeling tasks including\nthe adding problem (Section 3.1), Sequential and Permuted MNIST classification tasks (Section 3.2), and bAbI question answering tasks (Section 3.3). RINs show faster convergence than IRNNs and LSTMs in the early stage of the training phase and reach competitive performance in all benchmarks. Note that the use of ReLU in RNNs usually leads to training instability, and therefore the network is sensitive to training hyperparameters. Our proposed RIN network demonstrates that a plain RNN does not suffer from this problem even with the use of ReLUs as shown in Section 3. We discuss further implications of this network and related work in Section 4."
    }, {
      "heading" : "2 METHODS",
      "text" : ""
    }, {
      "heading" : "2.1 ITERATIVE ESTIMATION VIEW IN RNNS",
      "text" : "Representation learning in RNNs requires that the network build a latent state, which reflects the temporal dependencies over a sequence of inputs. In this section, we explore an interpretation of this process using iterative estimation (IE), a view that is similar to the UIE view for feedforward GNNs. Formally, we characterize this viewpoint in Eq. 1, that is, the expectation of the difference between the hidden activation at step t, ht, and the last hidden activation at step T , hT , is zero:\nEx1,...,T [ht − hT ] = 0. (1)\nThis formulation implies that an RNN layer maintains and updates the same set of representations over the input sequence. Given the fact that the hidden activation at every step is an estimation of the final activation, we derive Eq. 3.\nEx1,...,T [ht − hT ]− Ex1,...,T [ht−1 − hT ] = 0 (2) ⇒ Ex1,...,T [ht − ht−1] = 0 (3)\nFig. 1 shows an empirical observation of the IE in the adding problem (experimental details in Section 3.1). Here, we use the Average Estimation Error (AEE) measure (Greff et al., 2016) to quantify the expectation of the difference between two hidden activations. The measured AEEs in Fig. 1 are close to 0 indicating that the LSTM model fulfills the view of IE. The results also suggest that the network learns an identity mapping since the activation levels are similar on average across all recurrent updates. In the next section, we shall show that the use of gates in GNNs encourages the network to learn an identity mapping and whether this analysis can be extended to plain recurrent networks."
    }, {
      "heading" : "2.2 ANALYSIS OF GNNS",
      "text" : "Popular GNNs such as LSTM, GRU; and recent variants such as the Phased-LSTM (Neil et al., 2016), and Intersection RNN (Collins et al., 2017), share the same dual gate design following:\nht = Ht Tt + ht−1 Ct (4)\nwhere t ∈ [1, T ], Ht = σ(xt,ht−1) represents the hidden transformation, Tt = τ(xt,ht−1) is the transform gate, and Ct = φ(xt,ht−1) is the carry gate. σ, τ and φ are recurrent layers that have their trainable parameters and activation functions. represents element-wise product operator. Note that ht may not be the output activation at the recurrent step t. For example in LSTM, ht represents the memory cell state. Typically, the elements of transform gate Tt,k and carry gate Ct,k are between 0 (close) and 1 (open), the value indicates the openness of the gate at the kth neuron. Hence, a plain recurrent network is a subcase of Eq. 4 when Tt = 1 and Ct = 0.\nNote that conventionally, the initial hidden activation h0 is 0 to represent a “void state” at the start of computation. For h0 to fit into Eq. 4’s framework, we define an auxiliary state h−1 as the previous state of h0, and T0 = 1, C0 = 0. We also define another auxiliary state hT+1 = hT , TT+1 = 0, and CT+1 = 1 as the succeeding state of hT .\nBased on the recursive definition in Eq. 4, we can write the final layer output hT as follows:\nhT = h0 T∏\nt=1\nCt + T∑ t=1\n( Ht Tt\nT+1∏ i=t+1 Ci\n) (5)\nwhere we use ∏ to represent element-wise multiplication over a series of terms.\nAccording to Eq. 3, and supposing that Eq. 5 fulfills the Eq. 1, we can use a zero-mean residual t for describing the difference between the outputs of recurrent steps:\nht − ht−1 = t (6) 0 = 0 (7)\nPlugging Eq. 6 into Eq. 5, we get hT = h0 + λ (8) where\nλ = T∑ t=1 λt = T∑ t=1 ( t∑ i=1 i ) T+1∏ j=t+1 Cj − ( t−1∑ i=0 i ) T∏ j=t Cj  (9) The complete deduction of Eqs. 8–9 is presented in Appendix A. Eq. 8 performs an identity mapping when the carry gate Ct is always open. In Eq. 9, the term ∑t i=1 i represents “a level of represen-\ntation that is formed between h1 and ht”. Moreover, the term ∏T\nj=t Cj extract the “useful” part of this representation and contribute to the final representation of the recurrent layer. Here, we interpret “useful” as any quantity that helps in minimizing the cost function. Therefore, the contribution, λt, at each recurrent step, quantifies the representation that is learned in the step t. Furthermore, it is generally believed that a GNN manages and maintains the latent state through the carry gate, such as the forget gate in LSTM. If the carry gate is closed, then it is impossible for the old state to be preserved while undergoing recurrent updates. However, if we set Ct = 0, t ∈ [1, T ] in Eq. 9, we get:\nhT = h0 + T∑ t=1 t (10)\nIf h0 = 0 (void state at the start), we can turn Eq. 10 into:\nhT = 1 + T∑ t=2 t = h1 + T∑ t=2 t (11)\nEq. 11 shows that the state can be preserved without the help of the carry gate. This result indicates that it is possible for a plain recurrent network to learn an identity mapping as well."
    }, {
      "heading" : "2.3 RECURRENT IDENTITY NETWORKS",
      "text" : "Motivated by the previous iterative estimation interpretation of RNNs, we formulate a novel plain recurrent network variant — Recurrent Identity Network (RIN):\nht = ReLU (Wxt +Uht−1 + ht−1 + b) (12) = ReLU (Wxt + (U+ I)ht−1 + b) (13)\nwhere W is the input-to-hidden weight matrix, U is the hidden-to-hidden weight matrix, and I is a non-trainable identity matrix that acts as a “surrogate memory” component. This formulation encourages the network to preserve a copy of the last state by embedding I into the hidden-tohidden weights. This “surrogate memory” component maintains the representation encoded in the past recurrent steps."
    }, {
      "heading" : "3 RESULTS",
      "text" : "In this section, we compare the performances of the RIN, IRNN, and LSTM in a set of tasks that require modeling long-range dependencies."
    }, {
      "heading" : "3.1 THE ADDING PROBLEM",
      "text" : "The adding problem is a standard task for examining the capability of RNNs for modeling longrange dependencies (Hochreiter & Schmidhuber, 1997). In this task, two numbers are randomly selected from a long sequence. The network has to predict the sum of these two numbers. The task becomes challenging as the length of the sequence T increases because the relevant numbers can be far from each other in a long sequence. We report experimental results from three datasets that have sequence lengths of T1 = 200, T2 = 300, and T3 = 400 respectively. Each dataset has 100,000 training samples and 10,000 testing samples. Each sequence of a dataset has Ti numbers that are randomly sampled from a uniform distribution in [0, 1]. Each sequence is accompanied by a mask that indicates the two chosen random positions.\nWe compare the performance between RINs, IRNNs, and LSTMs using the same experimental settings. Each network has one hidden layer with 100 hidden units. Note that a LSTM has four times more parameters than corresponding RIN and IRNN models. The optimizer minimizes the Mean Squared Error (MSE) between the target sum and the predicted sum. We initially used the RMSprop (Tieleman & Hinton, 2012) optimizer. However, some IRNN models failed to converge using this optimizer. Therefore, we chose the Adam optimizer (Kingma & Ba, 2014) so a fair comparison can be made between the different networks. The batch size is 32. Gradient clipping value for all models is 100. The models are trained with maximum 300 epochs until they converged. The initial learning rates are different between the datasets because we found that IRNNs are sensitive to the initial learning rate as the sequence length increases. The learning rates α200 = 10−4, α300 = 10−5 and α400 = 10−6 are applied to T1, T2 and T3 correspondingly. The input-to-hidden weights of RINs and IRNNs and hidden-to-hidden weights of RINs are initialized using a similar method to Le et al. (2015) where the weights are drawn from a Gaussian distribution N (0, 10−3). The LSTM is initialized with the settings where the input-to-hidden weights use Glorot Uniform (Glorot & Bengio, 2010) and hidden-to-hidden weights use an orthogonal matrix as suggested by Saxe et al.\n(2013). Bias values for all networks are initialized to 0. No explicit regularization is employed. We do not perform an exhaustive hyperparameter search in these experiments.\nThe baseline MSE of the task is 0.167. This score is achieved by predicting the sum of two numbers as 1 regardless of the input sequence. Fig. 2 shows MSE plots for different test datasets. RINs and IRNNs reached the same level of performance in all experiments, and LSTMs performed the worst. Notably, LSTM fails to converge in the dataset with T3 = 400. The use of ReLU in RINs and IRNNs causes some degree of instability in the training phase. However, in most cases, RINs converge faster and are more stable than IRNNs (see training loss plots in Fig. 5 of Appendix B). Note that because IRNNs are sensitive to the initial learning rate, applying high learning rates such as α = 10−3 for T2 and T3 could cause the training of the network to fail."
    }, {
      "heading" : "3.2 SEQUENTIAL AND PERMUTED MNIST",
      "text" : "Sequential and Permuted MNIST are introduced by Le et al. (2015) for evaluating RNNs. Sequential MNIST presents each pixel of the MNIST handwritten image (Lecun et al., 1998) to the network sequentially (e.g., from the top left corner of the image to the bottom right corner of the image). After the network has seen all 28× 28 = 784 pixels, the network produces the class of the image. This task requires the network to model a very long sequence that has 784 steps. Permuted MNIST is an even harder task than the Sequential MNIST in that a fixed random index permutation is applied to all images. This random permutation breaks the association between adjacent pixels. The network is expected to find the hidden relations between pixels so that it can correctly classify the image.\nAll networks are trained with the RMSprop optimizer (Tieleman & Hinton, 2012) and a batch size of 128. The networks are trained with maximum 500 epochs until they are converged. The initial learning rate is set to α = 10−6. Weight initialization follows the same setup as Section 3.1. No explicit regularization is added.\nTable 1 summarizes the accuracy performance of the networks on the Sequential and Permuted MNIST datasets. For small network sizes (1–100, 1–200), RINs outperform IRNNs in their accuracy performance. For bigger networks, RINs and IRNNs achieve similar performance; however, RINs converge much faster than IRNNs in the early stage of training (see Fig. 3). LSTMs perform the worst on both tasks in terms of both convergence speed and final accuracy. Appendix C presents the full experimental results.\nTo investigate the limit of RINs, we adopted the concept of Deep Transition (DT) Networks (Pascanu et al., 2013) for increasing the implicit network depth. In this extended RIN model called RINDT, each recurrent step performs two hidden transitions instead of one (the formulation is given in Appendix D). The network modification increases the inherent depth by a factor of two. The results showed that the error signal could survive 784× 2 = 1568 computation steps in RIN-DTs. In Fig. 4, we show the evidence of learning identity mapping empirically by collecting the hidden activation from all recurrent steps and evaluating Eqs. 1 and 3. The network matches the IE when AEE is close to zero. We also compute the variance of the difference between two recurrent steps. Fig. 4(a) suggests that all networks bound the variance across recurrent steps. Fig. 4(b) offers a closer perspective where it measures the AEE between two adjacent steps. The levels of activations for all networks are always kept the same on an average, which is an evidence of learning identity mapping. We also observed that the magnitude of the variance becomes significantly larger at the last 200 steps in IRNN and RIN. Repeated application of ReLU may cause this effect during recurrent update (Jastrzebski et al., 2017). Other experiments in this section exhibit similar behaviors, complete results are shown in Appendix C (Fig. 8–12). Note that this empirical analysis only demonstrates that the tested RNNs have the evidence of learning identity mapping across recurrent updates as RINs and IRNNs largely fulfill the view of IE. We do not over-explain the relationship between this analysis and the performance of the network."
    }, {
      "heading" : "3.3 BABI QUESTION ANSWERING TASKS",
      "text" : "The bAbI dataset provides 20 question answering tasks that measure the understanding of language and the performance of reasoning in neural networks (Weston et al., 2015). Each task consists of 1,000 training samples and 1,000 test samples. A sample consists of three parts: a list of statements, a question and an answer (examples in Table 2). The answer to the question can be inferred from the statements that are logically organized together.\nWe compare the performance of the RIN, IRNN, and LSTM on these tasks. All networks follow a network design where the network firstly embeds each word into a vector of 200 dimensions. The statements are then appended together to a single sequence and encoded by a recurrent layer while another recurrent layer encodes the question sequence. The outputs of these two recurrent layers are concatenated together, and this concatenated sequence is then passed to a different recurrent layer for decoding the answer. Finally, the network predicts the answer via a softmax layer. The recurrent layers in all networks have 100 hidden units. This network design roughly follows the architecture presented in Jing et al. (2017). The initial learning rates are set to α = 10−3 for RINs and LSTMs and α = 10−4 for IRNNs because IRNNs fail to converge with a higher learning rate on many tasks. We chose the Adam optimizer over the RMSprop optimizer because of the same reasons as in the adding problem. The batch size is 32. Each network is trained for maximum 100 epochs until the network converges. The recurrent layers in the network follow the same initialization steps as in Section 3.1.\nThe results in Table 3 show that RINs can reach mean performance similar to the state-of-theart performance reported in Jing et al. (2017). As discussed in Section 3.1, the use of ReLU as the activation function can lead to instability during training of IRNN for tasks that have lengthy statements (e.g.. 3-Three Supporting Facts, 5-Three Arg. Relations)."
    }, {
      "heading" : "4 DISCUSSION",
      "text" : "In this paper, we discussed the iterative representation refinement in RNNs and how this viewpoint could help in learning identity mapping. Under this observation, we demonstrated that the contribution of each recurrent step a GNN can be jointly determined by the representation that is formed up to the current step, and the openness of the carry gate in later recurrent updates. Note in Eq. 9, the element-wise multiplication of Cts selects the encoded representation that could arrive at the output of the layer. Thus, it is possible to embed a special function in Cts so that they are sensitive to certain pattern of interests. For example, in Phased LSTM, the time gate is inherently interested in temporal frequency selection (Neil et al., 2016).\nMotivated by the analysis presented in Section 2, we propose a novel plain recurrent network variant, the Recurrent Identity Network (RIN), that can model long-range dependencies without the use of gates. Compared to the conventional formulation of plain RNNs, the formulation of RINs only adds a set of non-trainable weights to represent a “surrogate memory” component so that the learned representation can be maintained across two recurrent steps.\nExperimental results in Section 3 show that RINs are competitive against other network models such as IRNNs and LSTMs. Particularly, small RINs produce 12%–67% higher accuracy in the Sequential and Permuted MNIST. Furthermore, RINs demonstrated much faster convergence speed in early phase of training, which is a desirable advantage for platforms with limited computing resources. RINs work well without advanced methods of weight initializations and are relatively insensitive to hyperparameters such as learning rate, batch size, and selection of optimizer. This property can be very helpful when the time available for choosing hyperparameters is limited. Note that we do not claim that RINs outperform LSTMs in general because LSTMs may achieve comparable performance with finely-tuned hyperparameters.\nThe use of ReLU in RNNs might be counterintuitive at first sight because the repeated application of this activation is more likely causing gradient explosion than conventional choices of activation function, such as hyperbolic tangent (tanh) function or sigmoid function. Although the proposed IRNN (Le et al., 2015) reduces the problem by the identity initialization, in our experiments, we usually found that IRNN is more sensitive to training parameters and more unstable than RINs and LSTMs. On the contrary, feedforward models that use ReLU usually produce better results and converge faster than FFNs that use the tanh or sigmoid activation function. In this paper, we provide a promising method of using ReLU in RNNs so that the network is less sensitive to the training\nconditions. The experimental results also support the argument that the use of ReLU significantly speeds up the convergence.\nDuring the development of this paper, a recent independent work (Zagoruyko & Komodakis, 2017) presented a similar network formulation with a focus on training of deep plain FFNs without skip connections. DiracNet uses the idea of ResNets where it assumes that the identity initialization can replace the role of the skip-connection in ResNets. DiracNet employed a particular kind of activation function — negative concatenated ReLU (NCReLU), and this activation function allows the layer output to approximate the layer input when the expectation of the weights are close to zero. In this paper, we showed that an RNN can be trained without the use of gates or special activation functions, which complements the findings and provides theoretical basis in Zagoruyko & Komodakis (2017).\nWe hope to see more empirical and theoretical insights that explains the effectiveness of the RIN by simply embedding a non-trainable identity matrix. In future, we will investigate the reasons for the faster convergence speed of the RIN during training. Furthermore, we will investigate why RIN can be trained stably with the repeated application of ReLU and why it is less sensitive to training parameters than the two other models."
    }, {
      "heading" : "A ALGEBRA OF EQS. 8–9",
      "text" : "Popular GNNs such as LSTM, GRU; and recent variants such as the Phased-LSTM (Neil et al., 2016), and Intersection RNN (Collins et al., 2017), share the same dual gate design described as follows:\nht = Ht Tt + ht−1 Ct (14)\nwhere t ∈ [1, T ], Ht = σ(xt,ht−1) represents the hidden transformation, Tt = τ(xt,ht−1) is the transform gate, and Ct = φ(xt,ht−1) is the carry gate. σ, τ and φ are recurrent layers that have their trainable parameters and activation functions. represents element-wise product operator. Note that ht may not be the output activation at the recurrent step t. For example in LSTM, ht represents the memory cell state. Typically, the elements of transform gate Tt,k and carry gate Ct,k are between 0 (close) and 1 (open), the value indicates the openness of the gate at the kth neuron. Hence, a plain recurrent network is a subcase of Eq. 14 when Tt = 1 and Ct = 0.\nNote that conventionally, the initial hidden activation h0 is 0 to represent a “void state” at the start of computation. For h0 to fit into Eq. 4’s framework, we define an auxiliary state h−1 as the previous state of h0, and T0 = 1, C0 = 0. We also define another auxiliary state hT+1 = hT , TT+1 = 0, and CT+1 = 1 as the succeeding state of hT .\nBased on the recursive definition in Eq. 4, we can write the final layer output hT as follows:\nhT = h0 T∏\nt=1\nCt + T∑ t=1\n( Ht Tt\nT+1∏ i=t+1 Ci\n) (15)\nwhere we use ∏ to represent element-wise multiplication over a series of terms.\nAccording to Eq. 3, and supposing that Eq. 5 fulfills the Eq. 1, we can use a zero-mean residual t for describing the difference between the outputs of recurrent steps:\nht − ht−1 = t (16) 0 = 0 (17)\nThen we can rewrite Eq. 16 as:\nHt Tt + ht−1 Ct = ht−1 + t (18)\nSubstituting Eq. 18 into Eq. 15:\nhT =h0 T∏\nt=1\nCt + T∑ t=1 ((1−Ct) ht−1 + t) T+1∏ j=t+1 Cj  (19) =h0\nT∏ t=1 Ct + T∑ t=1 ((1−Ct) (h0 + t−1∑ i=1 i ) + t ) T+1∏ j=t+1 Cj  (20) We can rearrange Eqn. 20 to\nhT =h0\n( T∑\nt=0 (1−Ct) T+1∏ i=t+1 Ci\n) + λ (21)\n=h0 ( CT+1 −\nT+1∏ t=0 Ct\n) + λ (22)\n=h0 + λ (Eq. 8)\nwhere\nλ = T∑ t=1 λt = T∑ t=1 ((1−Ct) t−1∑ i=1 i + t ) T+1∏ j=t+1 Cj  (23)\nThe term λ in Eq. 23 can be reorganized to,\nλ = T∑ t=1 λt = T∑ t=1 ((1−Ct) t−1∑ i=1 i + t ) T+1∏ j=t+1 Cj  (24) =\nT∑ t=1 ( t∑ i=1 i −Ct t−1∑ i=0 i ) T+1∏ j=t+1 Cj  (25) =\nT∑ t=1  t∑ i=1 i T+1∏ j=t+1 Cj − t−1∑ i=0 i T∏ j=t Cj  (Eq. 9)"
    }, {
      "heading" : "B DETAILS IN THE ADDING PROBLEM EXPERIMENTS",
      "text" : "Fig. 5 presents the MSE plots during the training phase. As we discussed in Section 3.1, the choice of ReLU can occur some degree of instability during the training. Compared to RINs, IRNNs are much more unstable in T3 = 400.\n0 50 100 150 200 250 300 epochs\n10 7\n10 5\n10 3\n10 1\n101\n103\n105\nm ea\nn sq\nua re\nd er\nro r\nRIN 1-100 IRNN 1-100 LSTM 1-100 Baseline=0.167\n0 50 100 150 200 250 300 epochs\n10 5\n10 4\n10 3\n10 2\n10 1\n100\n101\nm ea\nn sq\nua re\nd er\nro r\nRIN 1-100 IRNN 1-100 LSTM 1-100 Baseline=0.167\n0 50 100 150 200 250 300 epochs\n10 1\n103 107\n1011 1015\n1019 1023 1027 1031\nm ea\nn sq\nua re\nd er\nro r\nRIN 1-100 IRNN 1-100 LSTM 1-100 Baseline=0.167\n(a) T1 = 200 (b) T2 = 300 (c) T3 = 400\nFigure 5: Mean Squared Error (MSE) plots for the adding problem with different sequence lengths at training phase. The figures are presented in log scale. All models are trained up to 300 epochs."
    }, {
      "heading" : "C DETAILS IN SEQUENTIAL AND PERMUTED MNIST EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "D RINS WITH DEEP TRANSITIONS",
      "text" : "In Section 3.2, we tested an additional model for RINs, which takes the concept of Deep Transition Networks (DTNs) Pascanu et al. (2013). Instead of stacking the recurrent layers, DTNs add multiple nonlinear transitions in a single recurrent step. This modification massively increases the depth of the network. In our RIN-DTs, the number of transition per recurrent step is two. Because the length of the sequence for Sequential and Permuted MNIST tasks is 784, RIN-DTs have the depth of 784× 2 = 1568. The recurrent layer is defined in Eqs. 26–27.\nĥt = f(W1xt + (U1 + I)ht−1 + b1) (26)\nht = f((U2 + I)ĥt + b2) (27)"
    } ],
    "references" : [ {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Çaglar Gülçehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "translation. CoRR,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Çaglar Gülçehre", "KyungHyun Cho", "Yoshua Bengio" ],
      "venue" : "CoRR, abs/1412.3555,",
      "citeRegEx" : "Chung et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Capacity and trainability in recurrent neural networks",
      "author" : [ "Jasmine Collins", "Jascha Sohl-Dickstein", "David Sussillo" ],
      "venue" : "In 5th International Conference on Learning Representations, Palais des Congrès Neptune,",
      "citeRegEx" : "Collins et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Collins et al\\.",
      "year" : 2017
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio" ],
      "venue" : "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Glorot and Bengio.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Highway and residual networks learn unrolled iterative estimation",
      "author" : [ "Klaus Greff", "Rupesh Kumar Srivastava", "Jürgen Schmidhuber" ],
      "venue" : null,
      "citeRegEx" : "Greff et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Greff et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "CoRR, abs/1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Identity Mappings in Deep Residual Networks, pp. 630–645",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Geoffrey E. Hinton", "Simon Osindero", "Yee-Whye Teh" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Tunable efficient unitary neural networks (EUNN) and their application to RNN",
      "author" : [ "Li Jing", "Yichen Shen", "Tena Dubcek", "John Peurifoy", "Scott A. Skirlo", "Max Tegmark", "Marin Soljacic" ],
      "venue" : null,
      "citeRegEx" : "Jing et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jing et al\\.",
      "year" : 2016
    }, {
      "title" : "Gated Orthogonal Recurrent Units: On Learning to Forget",
      "author" : [ "Li Jing", "Cagla Gulcehre", "John Peurifoy", "Yichen Shen", "Max Tegmark", "Marin Soljačić", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Jing et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jing et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "In Proceedings of the 3rd International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "A simple way to initialize recurrent networks of rectified linear units",
      "author" : [ "Quoc V. Le", "Navdeep Jaitly", "Geoffrey E. Hinton" ],
      "venue" : "CoRR, abs/1504.00941,",
      "citeRegEx" : "Le et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2015
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann Lecun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Lecun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Lecun et al\\.",
      "year" : 1998
    }, {
      "title" : "Learning recurrent neural networks with hessian-free optimization",
      "author" : [ "James Martens", "Ilya Sutskever" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning,",
      "citeRegEx" : "Martens and Sutskever.,? \\Q2011\\E",
      "shortCiteRegEx" : "Martens and Sutskever.",
      "year" : 2011
    }, {
      "title" : "All you need is a good init",
      "author" : [ "Dmytro Mishkin", "Jiri Matas" ],
      "venue" : "CoRR, abs/1511.06422,",
      "citeRegEx" : "Mishkin and Matas.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mishkin and Matas.",
      "year" : 2015
    }, {
      "title" : "Phased lstm: Accelerating recurrent network training for long or event-based sequences",
      "author" : [ "Daniel Neil", "Michael Pfeiffer", "Shih-Chii Liu" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Neil et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Neil et al\\.",
      "year" : 2016
    }, {
      "title" : "How to construct deep recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Çaglar Gülçehre", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "CoRR, abs/1312.6026,",
      "citeRegEx" : "Pascanu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2013
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "Andrew M. Saxe", "James L. McClelland", "Surya Ganguli" ],
      "venue" : "CoRR, abs/1312.6120,",
      "citeRegEx" : "Saxe et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Saxe et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning complex, extended sequences using the principle of history compression",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Schmidhuber.,? \\Q1992\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 1992
    }, {
      "title" : "Training very deep networks",
      "author" : [ "Rupesh K Srivastava", "Klaus Greff", "Juergen Schmidhuber" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent magnitude",
      "author" : [ "Tijmen Tieleman", "Geoffrey Hinton" ],
      "venue" : "COURSERA: Neural Networks for Machine Learning,",
      "citeRegEx" : "Tieleman and Hinton.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tieleman and Hinton.",
      "year" : 2012
    }, {
      "title" : "Residual networks are exponential ensembles of relatively shallow networks",
      "author" : [ "Andreas Veit", "Michael J. Wilber", "Serge J. Belongie" ],
      "venue" : null,
      "citeRegEx" : "Veit et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Veit et al\\.",
      "year" : 2016
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol" ],
      "venue" : "In Proceedings of the 25th International Conference on Machine Learning,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2008
    }, {
      "title" : "Towards ai-complete question answering: A set of prerequisite toy",
      "author" : [ "Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov" ],
      "venue" : "tasks. CoRR,",
      "citeRegEx" : "Weston et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2015
    }, {
      "title" : "All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation",
      "author" : [ "Di Xie", "Jiang Xiong", "Shiliang Pu" ],
      "venue" : null,
      "citeRegEx" : "Xie et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2017
    }, {
      "title" : "DiracNets: Training Very Deep Neural Networks",
      "author" : [ "Sergey Zagoruyko", "Nikos Komodakis" ],
      "venue" : "Without Skip-Connections. CoRR,",
      "citeRegEx" : "Zagoruyko and Komodakis.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zagoruyko and Komodakis.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : ", Greedy Layer-wise training (Schmidhuber, 1992; Hinton et al., 2006; Vincent et al., 2008)), and special weight initialization methods when training on both plain FFNs and RNNs (Glorot & Bengio, 2010; Mishkin & Matas, 2015; Le et al.",
      "startOffset" : 29,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : ", Greedy Layer-wise training (Schmidhuber, 1992; Hinton et al., 2006; Vincent et al., 2008)), and special weight initialization methods when training on both plain FFNs and RNNs (Glorot & Bengio, 2010; Mishkin & Matas, 2015; Le et al.",
      "startOffset" : 29,
      "endOffset" : 91
    }, {
      "referenceID" : 23,
      "context" : ", Greedy Layer-wise training (Schmidhuber, 1992; Hinton et al., 2006; Vincent et al., 2008)), and special weight initialization methods when training on both plain FFNs and RNNs (Glorot & Bengio, 2010; Mishkin & Matas, 2015; Le et al.",
      "startOffset" : 29,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : ", 2008)), and special weight initialization methods when training on both plain FFNs and RNNs (Glorot & Bengio, 2010; Mishkin & Matas, 2015; Le et al., 2015; Jing et al., 2016; Xie et al., 2017; Jing et al., 2017).",
      "startOffset" : 94,
      "endOffset" : 213
    }, {
      "referenceID" : 9,
      "context" : ", 2008)), and special weight initialization methods when training on both plain FFNs and RNNs (Glorot & Bengio, 2010; Mishkin & Matas, 2015; Le et al., 2015; Jing et al., 2016; Xie et al., 2017; Jing et al., 2017).",
      "startOffset" : 94,
      "endOffset" : 213
    }, {
      "referenceID" : 25,
      "context" : ", 2008)), and special weight initialization methods when training on both plain FFNs and RNNs (Glorot & Bengio, 2010; Mishkin & Matas, 2015; Le et al., 2015; Jing et al., 2016; Xie et al., 2017; Jing et al., 2017).",
      "startOffset" : 94,
      "endOffset" : 213
    }, {
      "referenceID" : 10,
      "context" : ", 2008)), and special weight initialization methods when training on both plain FFNs and RNNs (Glorot & Bengio, 2010; Mishkin & Matas, 2015; Le et al., 2015; Jing et al., 2016; Xie et al., 2017; Jing et al., 2017).",
      "startOffset" : 94,
      "endOffset" : 213
    }, {
      "referenceID" : 1,
      "context" : "Notable examples include recurrent networks such as Long-short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Chung et al., 2014; Cho et al., 2014), and feedforward networks such as Highway Networks (HNs) (Srivastava et al.",
      "startOffset" : 143,
      "endOffset" : 181
    }, {
      "referenceID" : 0,
      "context" : "Notable examples include recurrent networks such as Long-short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Chung et al., 2014; Cho et al., 2014), and feedforward networks such as Highway Networks (HNs) (Srivastava et al.",
      "startOffset" : 143,
      "endOffset" : 181
    }, {
      "referenceID" : 20,
      "context" : ", 2014), and feedforward networks such as Highway Networks (HNs) (Srivastava et al., 2015), and Residual Networks (ResNets) (He et al.",
      "startOffset" : 65,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : ", 2015), and Residual Networks (ResNets) (He et al., 2015).",
      "startOffset" : 41,
      "endOffset" : 58
    }, {
      "referenceID" : 22,
      "context" : "It has been demonstrated that removing (lesioning) or reordering (re-shuffling) random layers in deep feedforward GNNs does not noticeable affect the performance of the network (Veit et al., 2016) Noticeably, one interpretation for this effect as given by Greff et al.",
      "startOffset" : 177,
      "endOffset" : 196
    }, {
      "referenceID" : 6,
      "context" : "The analysis suggests that the use of gates in GNNs encourages the network to learn an identity mapping which can be beneficial in training deep architectures (He et al., 2016; Greff et al., 2016).",
      "startOffset" : 159,
      "endOffset" : 196
    }, {
      "referenceID" : 4,
      "context" : "The analysis suggests that the use of gates in GNNs encourages the network to learn an identity mapping which can be beneficial in training deep architectures (He et al., 2016; Greff et al., 2016).",
      "startOffset" : 159,
      "endOffset" : 196
    }, {
      "referenceID" : 12,
      "context" : "This network is compared against two competing networks, the IRNN (Le et al., 2015) and LSTM, on several long sequence modeling tasks including 1",
      "startOffset" : 66,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Cho et al., 2014), and feedforward networks such as Highway Networks (HNs) (Srivastava et al., 2015), and Residual Networks (ResNets) (He et al., 2015). One can successfully train very deep models by employing these models, e.g., ResNets can be trained with over 1,000 layers. It has been demonstrated that removing (lesioning) or reordering (re-shuffling) random layers in deep feedforward GNNs does not noticeable affect the performance of the network (Veit et al., 2016) Noticeably, one interpretation for this effect as given by Greff et al. (2016) is that the functional blocks in HNs or ResNets engage in an Unrolled Iterative Estimate (UIE) of representations and that layers in this block of HNs or ResNets iteratively refine a single set of representations.",
      "startOffset" : 8,
      "endOffset" : 561
    }, {
      "referenceID" : 4,
      "context" : "Here, we use the Average Estimation Error (AEE) measure (Greff et al., 2016) to quantify the expectation of the difference between two hidden activations.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : "2 ANALYSIS OF GNNS Popular GNNs such as LSTM, GRU; and recent variants such as the Phased-LSTM (Neil et al., 2016), and Intersection RNN (Collins et al.",
      "startOffset" : 95,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : ", 2016), and Intersection RNN (Collins et al., 2017), share the same dual gate design following: ht = Ht Tt + ht−1 Ct (4) 2",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "The input-to-hidden weights of RINs and IRNNs and hidden-to-hidden weights of RINs are initialized using a similar method to Le et al. (2015) where the weights are drawn from a Gaussian distribution N (0, 10−3).",
      "startOffset" : 125,
      "endOffset" : 142
    }, {
      "referenceID" : 13,
      "context" : "Sequential MNIST presents each pixel of the MNIST handwritten image (Lecun et al., 1998) to the network sequentially (e.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "To investigate the limit of RINs, we adopted the concept of Deep Transition (DT) Networks (Pascanu et al., 2013) for increasing the implicit network depth.",
      "startOffset" : 90,
      "endOffset" : 112
    }, {
      "referenceID" : 12,
      "context" : "2 SEQUENTIAL AND PERMUTED MNIST Sequential and Permuted MNIST are introduced by Le et al. (2015) for evaluating RNNs.",
      "startOffset" : 80,
      "endOffset" : 97
    }, {
      "referenceID" : 12,
      "context" : "The LSTM results matches with Le et al. (2015) Network Type Sequential MNIST Permuted MNIST RIN IRNN LSTM RIN IRNN LSTM 1–100 91.",
      "startOffset" : 30,
      "endOffset" : 47
    }, {
      "referenceID" : 24,
      "context" : "3 BABI QUESTION ANSWERING TASKS The bAbI dataset provides 20 question answering tasks that measure the understanding of language and the performance of reasoning in neural networks (Weston et al., 2015).",
      "startOffset" : 181,
      "endOffset" : 202
    }, {
      "referenceID" : 9,
      "context" : "This network design roughly follows the architecture presented in Jing et al. (2017). The initial learning rates are set to α = 10−3 for RINs and LSTMs and α = 10−4 for IRNNs because IRNNs fail to converge with a higher learning rate on many tasks.",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "This network design roughly follows the architecture presented in Jing et al. (2017). The initial learning rates are set to α = 10−3 for RINs and LSTMs and α = 10−4 for IRNNs because IRNNs fail to converge with a higher learning rate on many tasks. We chose the Adam optimizer over the RMSprop optimizer because of the same reasons as in the adding problem. The batch size is 32. Each network is trained for maximum 100 epochs until the network converges. The recurrent layers in the network follow the same initialization steps as in Section 3.1. The results in Table 3 show that RINs can reach mean performance similar to the state-of-theart performance reported in Jing et al. (2017). As discussed in Section 3.",
      "startOffset" : 66,
      "endOffset" : 687
    }, {
      "referenceID" : 9,
      "context" : "Task RIN IRNN LSTM Jing et al. (2017) Weston et al.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "Task RIN IRNN LSTM Jing et al. (2017) Weston et al. (2015) 1: Single Supporting Fact 51.",
      "startOffset" : 19,
      "endOffset" : 59
    }, {
      "referenceID" : 16,
      "context" : "For example, in Phased LSTM, the time gate is inherently interested in temporal frequency selection (Neil et al., 2016).",
      "startOffset" : 100,
      "endOffset" : 119
    }, {
      "referenceID" : 12,
      "context" : "Although the proposed IRNN (Le et al., 2015) reduces the problem by the identity initialization, in our experiments, we usually found that IRNN is more sensitive to training parameters and more unstable than RINs and LSTMs.",
      "startOffset" : 27,
      "endOffset" : 44
    } ],
    "year" : 2017,
    "abstractText" : "Plain recurrent networks greatly suffer from the vanishing gradient problem while Gated Neural Networks (GNNs) such as Long-short Term Memory (LSTM) and Gated Recurrent Unit (GRU) deliver promising results in many sequence learning tasks through sophisticated network designs. This paper shows how we can address this problem in a plain recurrent network by analyzing the gating mechanisms in GNNs. We propose a novel network called the Recurrent Identity Network (RIN) which allows a plain recurrent network to overcome the vanishing gradient problem while training very deep models without the use of gates. We compare this model with IRNNs and LSTMs on multiple sequence modeling benchmarks. The RINs demonstrate competitive performance and converge faster in all tasks. Notably, small RIN models produce 12%–67% higher accuracy on the Sequential and Permuted MNIST datasets and reach state-of-the-art performance on the bAbI question answering dataset.",
    "creator" : "LaTeX with hyperref package"
  }
}