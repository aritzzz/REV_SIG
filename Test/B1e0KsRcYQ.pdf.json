{
  "name" : "B1e0KsRcYQ.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Learning rich and compact representations is an open topic in many fields such as word embedding (Mikolov et al. (2013)), visual question-answering (Yang et al. (2016)), object recognition (Szegedy et al. (2015)) or image retrieval (Opitz et al. (2017)). The standard approach extracts features from the input data (text, image, etc.) and builds a representation that will be next processed for a given task (classification, retrieval, etc.). These features are usually extracted with deep neural networks and the representation is trained in an end-to-end manner. Recently, representations that compute first order statistics over input data have been outperformed by improved models that compute higher order statistics such as bilinear models. This embedding strategy generates richer representations and has been applied in a wide range of tasks : word embedding (Clinchant & Perronnin (2013)), VQA (Kim et al. (2017)), fine grained classification (Wei et al. (2018)), etc. and gets state-of-the-art results. For instance, Bilinear models perform the best for fine grained visual classification tasks by producing efficient representations that model more details within an image than classical first order statistics (Lin et al. (2015)).\nHowever, even if the increase in performances is unquestionable, second order models suffer from a collection of drawbacks: Their intermediate dimension increases quadratically with respect to input features dimension, they require a projection to lower dimension that is costly both in number of parameters and in computation, they are harder to train than first order models due to the increased dimension, they lack a proper adapted pooling scheme which leads to sub-optimal representations.\nThe two main downsides, namely the high dimensional output representations and the sub-efficient pooling scheme, have been widely studied over the last decade. On one hand, the dimensionality issue has been studied through factorization scheme, either representation oriented such as Compact Bilinear Pooling (Gao et al. (2016)) and Hadamard Product for Low Rank Bilinear Pooling (Kim et al. (2017)), or task oriented as Low-rank Bilinear Pooling (Kong & Fowlkes (2017)). While these factorization schemes are efficient in term of computation cost and number of parameters, the\nintermediate representation is still too large (typically 10k dimension) to ease the training process and using lower dimension greatly deteriorate performances.\nOn the other hand, it is well-known that global average pooling schemes aggregate unrelated features. This problem has been tackled by the use of codebooks such as VLAD (Arandjelovic & Zisserman (2013)) or, in the case of second-order information, Fisher Vectors (Perronnin et al. (2010)). These strategies have been enhanced to be trainable in an end-to-end manner (Arandjelovic et al. (2016); Tang et al. (2016)). However, using a codebook on end-to-end trainable second order features leads to an unreasonably large model, since the already large second order model has to be duplicated for each entry of the codebook. This is for example the case in MFAFVNet (Li et al. (2017b)) for which the second order layer alone (i.e., without the CNN part) already costs over 25M parameters and 40 GFLOP, or about as much as an entire ResNet50.\nIn this paper, we tackle both of these shortcomings (intermediate representation cost and lack of proper pooling) by exploring joint factorization and codebook strategies. Our main results are the following:\n- We first show that state-of-the-art factorization schemes can already be improved by the use of a codebook pooling, albeit at a prohibitive cost.\n- We then propose our main contribution, a joint codebook and factorization scheme that achieves similar results at a much reduced cost.\nSince our approach focuses on representation learning and is task agnostic, we validate it in a retrieval context on several image datasets to show the relevance of the learned representations. We show our model achieves competitive results on these datasets at a very reasonable cost.\nThe remaining of this paper is organized as follows: in the next section, we present the related work on second order pooling, factorization schemes and codebook strategies. In section 3, we present our factorization with the codebook strategy and how we improve its integration. In section 4, we show an ablation study on the Stanford Online Products dataset (Oh Song et al. (2016)). Finally, we compare our approach to the state-of-the-art methods on three image retrieval datasets (Stanford Online Products, CUB-200-2001, Cars-196)."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "In this section, we focus on methods that use representations based on second-order information and we provide a comparison in terms of computational efficiency and number of parameters for the second-order layer. These second-order methods exploit either bilinear pooling (section 2.1) and factorization schemes (section 2.2) or codebook strategies (section 2.3)."
    }, {
      "heading" : "2.1 SECOND-ORDER POOLING",
      "text" : "In this section, we briefly review end-to-end trainable Bilinear pooling (Lin et al. (2015)). This method extracts representations from the same image with two CNNs and computes the crosscovariance as representation. This representation outperforms its first-order version and other second-order representations such as Fisher Vectors (Perronnin et al. (2010)) once the global architecture is fine-tuned. However, bilinear pooling leads to a small improvement compared to secondorder pooling (i.e., the covariance of the CNN features) at the cost of a higher computation. Most of recent works on bilinear pooling only focus on computing covariance of the extracted features, that is :\ny = ∑ i∈S xix T i = XX T ∈ Rd×d (1)\nwhere X = {xi ∈ Rd|i ∈ S} ∈ Rd×hw is the matrix of concatenated CNN features, h and w are the height and the width of the extracted feature map. Another formulation is the vectorized version of y obtained by computing the Kronecker product (⊗) of xi with itself:\ny = ∑ i∈S xi ⊗ xi = vec(XXT ) ∈ Rd 2\n(2)\nDue to the very high dimension of this representation, most recent works on bilinear pooling tend to improve this representation by providing new factorization scheme to reduce the computation."
    }, {
      "heading" : "2.2 FACTORIZATION SCHEMES",
      "text" : "Recent works on bilinear pooling proposed factorization schemes with two objectives: avoiding the direct computation of second order features and reducing the high dimensionality output representation. Gao et al. (2016) proposed Compact Bilinear Pooling that tackles the high dimensionality of second-order features by analyzing two low-rank approximations of the equivalent polynomial kernel (equation (2) from Gao et al. (2016)):\n〈B(X ) ; B(Y)〉 = ∑\nxs∈B(X ) ∑ yu∈B(Y) 〈xs ; yu〉2 ≈ 〈 ∑ xs∈B(X ) φ(xs) ; ∑ yu∈B(Y) φ(yu) 〉 (3)\nwith φ the mapping function that approximates the kernel. This compact formulation allows to keep less than 4% of the components with nearly no loss in performances compared to the uncompressed model. Kim et al. (2017) improved Compact Bilinear Pooling using Hadamard Product and generalized it for visual question-answering tasks.\nKong & Fowlkes (2017) introduced Low Rank Bilinear Pooling (LR-BP) that takes advantage of SVM formulation by jointly training the network and the classifier. The authors propose an efficient factorization as they never compute directly the covariance features and have slightly better results compared to the uncompressed bilinear pooling or Compact Bilinear Pooling. However, as it is, their method is limited to classification with the SVM formulation and cannot be used for other tasks.\nLi et al. (2017a) introduced Factorized Bilinear Network (FBN), an improved version of Compact Bilinear Pooling. FBN never directly computes the covariance matrix. Instead, it projects the features using a hyperplan and computes a quadratic form. The matrix of this quadratic form is supposed rank deficient to reduce the number of parameters and the computation cost with negligible loss in performances. Thus, the output representation (or directly the number of classes for classification tasks) is generated by concatenating these scalars for all projections.\nWei et al. (2018) presented Grassmann Bilinear Pooling as a new factorization scheme. The objective is to take advantage of the rank deficient covariance matrix using Singular Value Decomposition (SVD) and then compute the classifier over these Grassmann manifolds. This factorization is efficient in the sense that it never directly computes the second-order representation and contrary to LR-BP, this formulation allows the construction of a representation, by replacing the number of classes by the representation dimension. In practice, however, they need to greatly reduce the input feature dimension due to the SVD complexity which is cubic in the feature dimension.\nIn this work, we start from a similar factorization as Kim et al. (2017) detailed in section 3.1. However, this factorization is improved by the introduction of a codebook strategy that allows smaller representation dimension and improves performances."
    }, {
      "heading" : "2.3 CODEBOOK STRATEGIES",
      "text" : "An acknowledged drawback of pooling methods is that they pool unrelated features that may decrease performances. To cope with this observation, codebook strategies have been proposed and greatly improved performances by pooling only features that belong to the same codeword.\nThe first representations that take advantage of codebook strategies are Bag of Words (BoW) and in the case of second order information Fisher Vectors (Perronnin et al. (2010)). Fisher Vectors (FVs) extend the BoW framework by replacing the hard assignment of BoW by a Gaussian Mixture Model (GMM) and then compute the representation as an extension of the Fisher Kernel. In practice, covariance matrices are supposed to be diagonal which leads to representations of size N(2d + 1) where d is the dimension of the features and N is the codebook size. Tang et al. (2016) proposed FisherNet, an architecture that integrates FVs as differentiable layer. The proposed layer outperforms non-trainable FVs approach but nonetheless has the high output dimension of the original FV.\nLi et al. (2017b) introduced MFA-FV network, a deep architecture which extends the MFA-FV of Dixit & Vasconcelos (2016) by producing a second order information embedding trainable in an endto-end manner. The proposed formulation takes advantage of both worlds: MFA-FV generates an efficient representation of non-linear manifolds with a small latent space and it can be trained in an end-to-end manner. The main drawbacks of their method is the direct computation of second-order\nfeatures for each codeword (computation cost), the raw projection of this covariance matrix into the latent space for each codeword (computation cost and number of parameters), and finally the representation dimension. In the original paper, the proposed representation reaches 500k dimension, twice the already high dimension of Bilinear Pooling.\nFor a more compact review, computation cost, number of parameters, use of codebook and/or factorization are sumed-up in table 1. This table shows that, to our knowledge, no efficient factorization combined with codebook strategy has been proposed to exploit the richer representation due to codebook but at a small increase in terms of number of parameters or computation cost. As is shown in this table, our proposition combine the best of both worlds by providing a joint codebook and factorization optimization scheme with a similar number of parameters and computation cost to that of methods without codebook strategies."
    }, {
      "heading" : "3 METHOD OVERVIEW",
      "text" : "In section 3.1, we detail the initial factorization scheme and the properties of the Kronecker Product and the dot product that are used in the two next sections. In section 3.2, we extend this factorization to a codebook strategy and show the limitations of this architecture in terms of computation cost, low-rank approximation, number of parameters, etc. In section 3.3 we enhance this representation by sharing projectors to all codewords into the codebook, leading to a joint codebook and factorization optimization."
    }, {
      "heading" : "3.1 INITIAL FACTORIZATION SCHEME",
      "text" : "In this section, we present the factorization used and highlight the advantages and limitations of this scheme. For a given input feature x ∈ Rd, we compute its second-order representation x⊗x ∈ Rd2 and project it into a smaller subspace with W ∈ Rd2×D to build the output feature z(x) ∈ RD. These output features are then pooled to build the output representation z:\nz = ∑ x z(x) = ∑ x W T (x⊗ x) (4)\nIn the rest of the paper, we use the notation zi that refers to the i-th dimension of the output representation z and zi(x) the i-th dimension of the output feature z(x), that is:\nzi = ∑ x zi(x) = ∑ x wTi (x⊗ x) = ∑ x 〈wi ; x⊗ x〉 (5)\nwith wi ∈ Rd 2\na column of W and 〈· ; ·〉 the dot product. Then we enforce a factorization of wi to take advantage of the properties of dot product and Kronecker product, that is ∀(a, b, c,d) ∈ (Rd)4 , 〈a⊗ c ; b⊗ d〉 = 〈a ; b〉 〈c ; d〉. Thus, we use the following rank one decomposition of wi = ui ⊗ vi where (u,v) ∈ (Rd)2. zi(x) from equation (5) becomes:\nzi(x) = 〈ui ; x〉 〈vi ; x〉 (6)\nThis factorization is efficient in term of parameters as it needs only 2dD parameters instead of d2D for the full projection matrix. However, even if this rank one decomposition allows interesting dimension reduction (Gao et al. (2016); Kim et al. (2017)) it is not enough to keep rich representation with smaller dimension. Consequently, we extend the second-order feature to a codebook strategy."
    }, {
      "heading" : "3.2 CODEBOOK STRATEGY",
      "text" : "To extend second-order pooling, we want to pool only similar features, that is which belong to the same codeword. This codebook pooling is interesting because each projection to a sub-space should have only similar features, and they should be encoded with fewer dimension. For a codebook size of N , we compute an assignment function h(·) ∈ RN . This function could be a hard assignment function (e.g., argmin over distance to each cluster) or a soft assignment (e.g., the softmax function). Thus, our output feature zi(x) becomes:\nzi(x) = 〈wi ; h(x)⊗ x⊗ h(x)⊗ x〉 (7)\nRemark that now W ∈ RN2d2×D and wi ∈ RN 2d2 . Here, we duplicate h(x) for generalization purpose: In the case of the original bilinear pooling this formulation becomes h1(x1)⊗x1⊗h2(x2)⊗ x2 and we can use two different codebooks, one for each network. Moreover, this formulation allows more degrees of freedom for the next factorization. As in equation 6, we enforce the rank one decomposition of wi = pi ⊗ qi where (pi, qi) ∈ (RNd)2. This first factorization leads to the following output feature zi(x):\nzi(x) = 〈pi ; h(x)⊗ x〉 〈qi ; h(x)⊗ x〉 (8)\nThis intermediate representation is too large to be computed directly, e.g. using N = 100 and the same parameters as in Table 1, we have intermediate features with 50k dimensions and the two intermediate feature maps consume about 320MB of memory which becomes rapidly intractable if the dimension of z is above 10. Then, we enforce two other factorizations of pi = ∑ j e\n(j) ⊗ ui,j and qi = ∑ j e\n(j) ⊗ vi,j where e(j) ∈ RN is the j-th vector from the natural basis of RN and (ui,j ,vi,j) ∈ (Rd)2. Then equation (8) becomes:\nzi(x) =  N∑ j=1 〈 h(x) ; e(j) 〉 〈x ; ui,j〉  N∑ j=1 〈 h(x) ; e(j) 〉 〈x ; vi,j〉  (9) The decompositions of pi and qi play similar roles as intra-projection in VLAD representation (Delhumeau et al. (2013)). Indeed, if we consider h(·) as a hard assignment function, the projection that will be computed is the only one assigned to the corresponding codewords. Thus, this model learns a projection matrix for each codebook entry.\nFurthermore, equation (9) can be factorized using hj(x), the j-th component of h(x): zi(x) = (∑N j=1 hj(x) 〈x ; ui,j〉 )(∑N j=1 hj(x) 〈x ; vi,j〉 )\n= ( h(x)TUTi x ) ( h(x)TV Ti x ) (10) where Ui ∈ Rd×N is the matrix concatenating the projections of all entries of the codebook for the i-th output dimension. We call this approach C-CBP as it corresponds to the extension of CBP (Gao et al. (2016)) to a Codebook strategy.\nThis representation has multiple advantages: First, it computes second order features that leads to better performances compared to its first order counterpart. Second, the first factorization provides an efficient alternative in terms of number of parameters and computation despite the decreasing performances when it reaches small representation dimension. This downside is addressed by the third advantage which is the codebook strategy. It allows the pooling of only related features while their projections to a sub-space is more compressible. However, even if this codebook strategy improves the performances, the number of parameters is inO(dDN) As such, using large codebook may become intractable. In the next section, we extend this scheme by sharing a set of projectors and enhance the decompositions of pi and qi."
    }, {
      "heading" : "3.3 SHARING PROJECTORS",
      "text" : "In the previous model, for a given codebook entry, there is one dedicated projector that is learned to map to a smaller vector space all features that belong to this codebook entry. The proposed idea is, instead of using a a one-to-one correspondence, we learn a set of projectors that is shared across the codebook. The reasoning behind is that projectors from different codebook entries are unlikely to be all orthogonal. By doing such hypothesis, that is, the vector space spaned by the projection matrices has a lower dimension than the codebook itself, we can have smaller models with nearly no loss in performances. To check this hypothesis, we extend the proposed factorization from section 3.2. We want to generate Ui from {Ũi}i∈{1,...,R} and Vi from {Ṽi}i∈{1,...,R} where R is the number of projections in the set. Then the two new enforced factorization of pi and qi are:\npi(x) = ∑ r fp,r ( h(x) ) e(r) ⊗ ũi,r and qi(x) = ∑ r fq,r ( h(x) ) e(r) ⊗ ṽi,r (11)\nwhere fp and fq are two functions from RN to RR that transform the codebook assignment into a set of coefficient which generate their respective projection matrices. Then, using these factorizations lead to the following equation:\nzi(x) = (∑R r=1 fp,r ( h(x) ) 〈x ; ũi,j〉 )(∑R r=1 fq,r ( h(x) ) 〈x ; ṽi,j〉 ) = ( fp ( h(x) )T ŨTi x )( fq ( h(x) )T Ṽ Ti x\n) (12) In this paper, we only study the case of a linear projection to the sub-space RR, that is fp : a 7→ fp(a) = A\nTa and fq : a 7→ fq(a) = BTa with (A,B) ∈ (RN×R)2. Finally, the fully factorized z transform is computed using the following equation:\nzi(x) = ( h(x)TAŨTi x )( h(x)TBṼ Ti x ) (13)\nEquation (13) is more efficient in terms of parameters than equation (10) as it requires only 2(RdD + NR) parameters instead of 2NdD. We call this approach JCF for Joint Codebook and Factorization. This shared projection is both efficient in terms of number of parameters and in computation by a factor R/N . In the next section, we provide an ablation study of the proposed method, comparing equation (10) and equation (13), demonstrating that learning recombination is both efficient and performing."
    }, {
      "heading" : "3.4 IMPLEMENTATION DETAILS",
      "text" : "In this section, we give some details about our implementation. All our experiments are performed on image retrieval datasets to assess the quality of the representations independantly of any classification scheme. We build our model over pre-trained network such as VGG16 (Simonyan & Zisserman (2014)) or ResNet50 (He et al. (2016)). In both case we reduce the features dimension to 256 dimensions and we l2-normalize them. For the assignment function h, we use the softmax over cosine similarity between the features and the codebook. Once the second-order representations are computed we pull them using global average pooling and we l2-normalize the output representation. Similarities between images are computed using the cosine similarity. In metric, we use Recall@K which takes the value 1 if there is at least one element from the same instance in the top-K results else 0 and averages these scores over the test set. The network is trained in 3 steps using a standard triplet loss function. In the first step, we freeze the ResNet50 and only train our added layers with 100 images per batch, we sample the negative within the batch and we use a learning rate of 10−4 for 40 epochs. In the second one, we unfreeze ResNet50 and fine-tune the whole architecture for 40 epochs more with a learning rate of 10−5 and a batch of 64 images with the negative sampled within the batch. In the last one, we fine-tune the network with a batch size of 64 images sampled by hard mining the training set with a learning rate of 10−5. The margin of the triplet loss is set to 0.1. Images are resized to 224x224 pixels for both train and test sets."
    }, {
      "heading" : "4 ABLATION STUDIES",
      "text" : ""
    }, {
      "heading" : "4.1 BILINEAR POOLING AND CODEBOOK STRATEGY",
      "text" : "In this section, we demonstrate both the relevance of second-order information for retrieval tasks and the influence of the codebook on our method. We report recall@1 on Stanford Online Products in Table 2 for the different configuration detailed below with the training procedure from Section 3.4 without the hard mining step.\nFirst, as a reference, we train a ResNet50 with a global average pooling and a fully connected layer to project the representation dimension from 2048 to 512. We denote it Baseline. Then we reimplement Bilinear Pooling (BP) and Compact Bilinear Pooling (CBP) and extend them naively to a codebook strategy (C-BP and C-CBP). The objective is to demonstrate that such strategy performs well, but a an intractable cost. Results are reported in Table 2. Note that, for each bilinear pooling method, we first add a 1 × 1 convolution to project the ResNet50 features from 2048 to 256 dimensions. This experiment confirm the interest of bilinear pooling in image retrieval with a improvement of 2% over the baseline, while using a 512 dimension representation. Furthermore, even using a codebook strategy with few codewords enhance bilinear pooling by 1% more, however, the number of parameters become intractable for codebook of size greater than 4: this naive strategy requires 270M parameters to extend this model to a codebook with a size of 8.\nUsing the factorization from equation (10) greatly reduces the required number of parameters and allows the exploration of larger codebook. However, this factorization without codebook leads to lower scores than the non factorized bilinear pooling, but adding a codebook strategy increases performances by more than 4% over bilinear pooling without codebook, with nearly 4 times less parameters."
    }, {
      "heading" : "4.2 SHARING PROJECTIONS",
      "text" : "In this part, we study the impact of the sharing projection. We use the same training procedure as in the previous section. For each codebook size, we train architecture with a different number of projections, allowing to compare architectures without the sharing process to architectures with greater codebook size but with the same number of parameters by sharing projectors. Results are reported in Table 3. Sharing projectors leads to smaller models with few loss in performances, and using richer codebooks allows more compression with superior results."
    }, {
      "heading" : "5 COMPARISON WITH BILINEAR FACTORIZATION",
      "text" : "In this section, we report performances of our factorization on 3 fine-grained visual classification (FGVC) datasets: CUB (Wah et al. (2011)), CARS (Krause et al. (2013)) and AIRCRAFT (Maji et al. (2013)). We use VGG16 as backbone network. Furthermore, to demonstrate the effectiveness of our codebook based factorization scheme to produce compact but effective second-order representations we compare JCF to closely-related formulations on FGVC tasks, that are: HPBP (zi(x) = pTi [σ(UTx) σ(V Tx)]). Non-linear multi-rank with shared U ,V .\nMR (zi(x) = xTUiV Ti x). Multi-rank extension of Eq.(6) which allows to compare the benefit of codebook against direct rank increase. This approach is related to FBP which uses a higher rank decomposition (R = 20) than in our tests (R = 8). MR+NL (zi(x) = 1TR[σ(UTi x) σ(V Ti x)]) Multi-rank with the same non-linearity as HPBP. MR+NL+C (zi(x) = pT [σ(UTi x) σ(V Ti x)]) which adds weights to the multi-rank combination. For a fair comparison, we fix the number of parameters for all of these methods to the same number as JCF (N = 32, R = 8). Thus, all methods use d = 256 and D = 512 and R = 8 except HPBP which uses R = 2048 to compensate for its shared matrices U ,V .\nWe report classification accuracy on the three aforementioned datasets in Table 4. As we can see, our method consistently outperforms the multi-rank variants. This confirms our intuition about the importance of grouping features by similarity before projection and aggregation. Indeed, multirank variants do not have a selection mechanism preceding the projection into the subspace that would allow to selectively choose the projectors based on the input features. Instead, all features are projected using the same projectors and then aggregated. We argue that non-linear multi-rank variants bring only marginal improvements, since the non-linearity happens after the projection is made. Although it is still possible to learn a projection coupled with the non-linearity that would lead to a similarity driven aggregation, it is not enforced by design. Since JCF does the similarity driven aggregation by design, it is easier to train, which we believe explains the results."
    }, {
      "heading" : "6 COMPARISON TO THE STATE-OF-THE-ART",
      "text" : "In this section, we compare our method to the state-of-the-art on 3 retrieval datasets: Stanford Online Products (Oh Song et al. (2016)), CUB-200-2011 (Wah et al. (2011)) and Cars-196 (Krause et al. (2013)). For Stanford Online Products and CUB-200-2011, we use the same train/test split as Oh Song et al. (2016). For Cars-196, we use the same as Opitz et al. (2017). We report the standard recall@K with K ∈ {1, 10, 100, 1000} for Stanford Online Products and with K ∈ {1, 2, 4, 8, 16, 32} for the other two. On CUB-200-2011 and Cars-196 (see Table 6) we re-implement Bilinear Pooling (BP) and Compact Bilinear Pooling (CBP) on a VGG16. Even if the results are interesting, the constraint over intermediate representation is too strong to achieve relevant results. We then implement the codebook factorization from equation 10 a codebook size of 32 (denoted C-CBP). This formulation outperforms both classical second-order models by a large margin with few parameters. Moreover, our model that shares projections over the codebook (JCF , computed following equation 13) with R = 8 has 4 times less parameters for a 2% loss on CUB-200-2011 dataset. On Cars-196, the sharing induces a higher loss, but may be improved with more projections to share.\nOn Stanford Online Products, we report the Baseline, implementations from equations 10 (C-CBP) and 13 (JCF ) with R = 8. We achieve state-of-the-art results using both methods and more than 10% improvement over the Baseline. Remark that JCF costs 4 times less than C-CBP at a 1% loss."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "In this paper, we propose a new pooling scheme based which is both efficient in performances (rich representation) and in representation dimension (compact representation). This is thanks to the second-order information that allows richer representation than first-order statistics and thanks to a codebook strategy which pools only related features. To control the computational cost, we extend this pooling scheme with a factorization that shares sets of projections between each entry of the codebook, trading fewer parameters and fewer computation for a small loss in performance. We achieve state-of-the-art results on Stanford Online Products and Cars-196, two image retrieval datasets. Even if our tests are performed on image retrieval datasets, we believe our method can readily be used in place of global average pooling for any task."
    } ],
    "references" : [ {
      "title" : "All about vlad",
      "author" : [ "Relja Arandjelovic", "Andrew Zisserman" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Arandjelovic and Zisserman.,? \\Q2013\\E",
      "shortCiteRegEx" : "Arandjelovic and Zisserman.",
      "year" : 2013
    }, {
      "title" : "Netvlad: Cnn architecture for weakly supervised place recognition",
      "author" : [ "Relja Arandjelovic", "Petr Gronat", "Akihiko Torii", "Tomas Pajdla", "Josef Sivic" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Arandjelovic et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Arandjelovic et al\\.",
      "year" : 2016
    }, {
      "title" : "Aggregating continuous word embeddings for information retrieval",
      "author" : [ "Stephane Clinchant", "Florent Perronnin" ],
      "venue" : "In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,",
      "citeRegEx" : "Clinchant and Perronnin.,? \\Q2013\\E",
      "shortCiteRegEx" : "Clinchant and Perronnin.",
      "year" : 2013
    }, {
      "title" : "Revisiting the VLAD image representation",
      "author" : [ "Jonathan Delhumeau", "Philippe-Henri Gosselin", "Hervé Jégou", "Patrick Perez" ],
      "venue" : "In ACM Multimedia,",
      "citeRegEx" : "Delhumeau et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Delhumeau et al\\.",
      "year" : 2013
    }, {
      "title" : "Object based scene representations using fisher scores of local subspace projections",
      "author" : [ "Mandar D Dixit", "Nuno Vasconcelos" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Dixit and Vasconcelos.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dixit and Vasconcelos.",
      "year" : 2016
    }, {
      "title" : "Compact bilinear pooling",
      "author" : [ "Yang Gao", "Oscar Beijbom", "Ning Zhang", "Trevor Darrell" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Gao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep metric learning with hierarchical triplet loss",
      "author" : [ "Weifeng", "Ge" ],
      "venue" : "In The European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "Weifeng and Ge.,? \\Q2018\\E",
      "shortCiteRegEx" : "Weifeng and Ge.",
      "year" : 2018
    }, {
      "title" : "Monet: Moments embedding network",
      "author" : [ "Mengran Gou", "Fei Xiong", "Octavia Camps", "Mario Sznaier" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Gou et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Gou et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Hadamard Product for Low-rank Bilinear Pooling",
      "author" : [ "Jin-Hwa Kim", "Kyoung Woon On", "Woosang Lim", "Jeonghee Kim", "Jung-Woo Ha", "Byoung-Tak Zhang" ],
      "venue" : "In The 5th International Conference on Learning Representations,",
      "citeRegEx" : "Kim et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2017
    }, {
      "title" : "Low-rank bilinear pooling for fine-grained classification",
      "author" : [ "Shu Kong", "Charless Fowlkes" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Kong and Fowlkes.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kong and Fowlkes.",
      "year" : 2017
    }, {
      "title" : "3d object representations for fine-grained categorization",
      "author" : [ "Jonathan Krause", "Michael Stark", "Jia Deng", "Li Fei-Fei" ],
      "venue" : "In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia,",
      "citeRegEx" : "Krause et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2013
    }, {
      "title" : "Factorized bilinear models for image recognition",
      "author" : [ "Yanghao Li", "Naiyan Wang", "Jiaying Liu", "Xiaodi Hou" ],
      "venue" : "In The IEEE International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Li et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep scene image classification with the mfafvnet",
      "author" : [ "Yunsheng Li", "Mandar Dixit", "Nuno Vasconcelos" ],
      "venue" : "In The IEEE International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Li et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Bilinear cnn models for fine-grained visual recognition",
      "author" : [ "Tsung-Yu Lin", "Aruni RoyChowdhury", "Subhransu Maji" ],
      "venue" : "In The IEEE International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Lin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2015
    }, {
      "title" : "Fine-grained visual classification of aircraft",
      "author" : [ "S. Maji", "J. Kannala", "E. Rahtu", "M. Blaschko", "A. Vedaldi" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Maji et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Maji et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "In Neural and Information Processing System (NIPS),",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "No fuss distance metric learning using proxies",
      "author" : [ "Yair Movshovitz-Attias", "Alexander Toshev", "Thomas K. Leung", "Sergey Ioffe", "Saurabh Singh" ],
      "venue" : "In The IEEE International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Movshovitz.Attias et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Movshovitz.Attias et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep metric learning via lifted structured feature embedding",
      "author" : [ "Hyun Oh Song", "Yu Xiang", "Stefanie Jegelka", "Silvio Savarese" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Song et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2016
    }, {
      "title" : "Bier - boosting independent embeddings robustly",
      "author" : [ "Michael Opitz", "Georg Waltner", "Horst Possegger", "Horst Bischof" ],
      "venue" : "In The IEEE International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Opitz et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Opitz et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving the fisher kernel for large-scale image classification",
      "author" : [ "Florent Perronnin", "Jorge Sánchez", "Thomas Mensink" ],
      "venue" : "Computer Vision – ECCV",
      "citeRegEx" : "Perronnin et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Perronnin et al\\.",
      "year" : 2010
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "CoRR, abs/1409.1556,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Improved deep metric learning with multi-class n-pair loss objective",
      "author" : [ "Kihyuk Sohn" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Sohn.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sohn.",
      "year" : 2016
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy", "Wei Liu", "Yangqing Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep fishernet for object classification",
      "author" : [ "Peng Tang", "Xinggang Wang", "Baoguang Shi", "Xiang Bai", "Wenyu Liu", "Zhuowen Tu" ],
      "venue" : null,
      "citeRegEx" : "Tang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning deep embeddings with histogram loss",
      "author" : [ "Evgeniya Ustinova", "Victor Lempitsky" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Ustinova and Lempitsky.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ustinova and Lempitsky.",
      "year" : 2016
    }, {
      "title" : "The Caltech-UCSD Birds-200-2011 Dataset",
      "author" : [ "C. Wah", "S. Branson", "P. Welinder", "P. Perona", "S. Belongie" ],
      "venue" : "Technical Report CNS-TR-2011-001, California Institute of Technology,",
      "citeRegEx" : "Wah et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wah et al\\.",
      "year" : 2011
    }, {
      "title" : "Grassmann pooling as compact homogeneous bilinear pooling for fine-grained visual classification",
      "author" : [ "Xing Wei", "Yue Zhang", "Yihong Gong", "Jiawei Zhang", "Nanning Zheng" ],
      "venue" : "In The European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "Wei et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2018
    }, {
      "title" : "Sampling matters in deep embedding learning",
      "author" : [ "Chao-Yuan Wu", "R Manmatha", "Alexander J Smola", "Philipp Krähenbühl" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Wu et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2017
    }, {
      "title" : "Stacked attention networks for image question answering",
      "author" : [ "Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Yang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Statistically-motivated second-order pooling",
      "author" : [ "Kaicheng Yu", "Mathieu Salzmann" ],
      "venue" : "In The European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "Yu and Salzmann.,? \\Q2018\\E",
      "shortCiteRegEx" : "Yu and Salzmann.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Learning rich and compact representations is an open topic in many fields such as word embedding (Mikolov et al. (2013)), visual question-answering (Yang et al.",
      "startOffset" : 98,
      "endOffset" : 120
    }, {
      "referenceID" : 13,
      "context" : "Learning rich and compact representations is an open topic in many fields such as word embedding (Mikolov et al. (2013)), visual question-answering (Yang et al. (2016)), object recognition (Szegedy et al.",
      "startOffset" : 98,
      "endOffset" : 168
    }, {
      "referenceID" : 13,
      "context" : "Learning rich and compact representations is an open topic in many fields such as word embedding (Mikolov et al. (2013)), visual question-answering (Yang et al. (2016)), object recognition (Szegedy et al. (2015)) or image retrieval (Opitz et al.",
      "startOffset" : 98,
      "endOffset" : 212
    }, {
      "referenceID" : 13,
      "context" : "Learning rich and compact representations is an open topic in many fields such as word embedding (Mikolov et al. (2013)), visual question-answering (Yang et al. (2016)), object recognition (Szegedy et al. (2015)) or image retrieval (Opitz et al. (2017)).",
      "startOffset" : 98,
      "endOffset" : 253
    }, {
      "referenceID" : 13,
      "context" : "Learning rich and compact representations is an open topic in many fields such as word embedding (Mikolov et al. (2013)), visual question-answering (Yang et al. (2016)), object recognition (Szegedy et al. (2015)) or image retrieval (Opitz et al. (2017)). The standard approach extracts features from the input data (text, image, etc.) and builds a representation that will be next processed for a given task (classification, retrieval, etc.). These features are usually extracted with deep neural networks and the representation is trained in an end-to-end manner. Recently, representations that compute first order statistics over input data have been outperformed by improved models that compute higher order statistics such as bilinear models. This embedding strategy generates richer representations and has been applied in a wide range of tasks : word embedding (Clinchant & Perronnin (2013)), VQA (Kim et al.",
      "startOffset" : 98,
      "endOffset" : 897
    }, {
      "referenceID" : 8,
      "context" : "This embedding strategy generates richer representations and has been applied in a wide range of tasks : word embedding (Clinchant & Perronnin (2013)), VQA (Kim et al. (2017)), fine grained classification (Wei et al.",
      "startOffset" : 157,
      "endOffset" : 175
    }, {
      "referenceID" : 8,
      "context" : "This embedding strategy generates richer representations and has been applied in a wide range of tasks : word embedding (Clinchant & Perronnin (2013)), VQA (Kim et al. (2017)), fine grained classification (Wei et al. (2018)), etc.",
      "startOffset" : 157,
      "endOffset" : 224
    }, {
      "referenceID" : 8,
      "context" : "This embedding strategy generates richer representations and has been applied in a wide range of tasks : word embedding (Clinchant & Perronnin (2013)), VQA (Kim et al. (2017)), fine grained classification (Wei et al. (2018)), etc. and gets state-of-the-art results. For instance, Bilinear models perform the best for fine grained visual classification tasks by producing efficient representations that model more details within an image than classical first order statistics (Lin et al. (2015)).",
      "startOffset" : 157,
      "endOffset" : 494
    }, {
      "referenceID" : 5,
      "context" : "On one hand, the dimensionality issue has been studied through factorization scheme, either representation oriented such as Compact Bilinear Pooling (Gao et al. (2016)) and Hadamard Product for Low Rank Bilinear Pooling (Kim et al.",
      "startOffset" : 150,
      "endOffset" : 168
    }, {
      "referenceID" : 5,
      "context" : "On one hand, the dimensionality issue has been studied through factorization scheme, either representation oriented such as Compact Bilinear Pooling (Gao et al. (2016)) and Hadamard Product for Low Rank Bilinear Pooling (Kim et al. (2017)), or task oriented as Low-rank Bilinear Pooling (Kong & Fowlkes (2017)).",
      "startOffset" : 150,
      "endOffset" : 239
    }, {
      "referenceID" : 5,
      "context" : "On one hand, the dimensionality issue has been studied through factorization scheme, either representation oriented such as Compact Bilinear Pooling (Gao et al. (2016)) and Hadamard Product for Low Rank Bilinear Pooling (Kim et al. (2017)), or task oriented as Low-rank Bilinear Pooling (Kong & Fowlkes (2017)).",
      "startOffset" : 150,
      "endOffset" : 310
    }, {
      "referenceID" : 17,
      "context" : "This problem has been tackled by the use of codebooks such as VLAD (Arandjelovic & Zisserman (2013)) or, in the case of second-order information, Fisher Vectors (Perronnin et al. (2010)).",
      "startOffset" : 162,
      "endOffset" : 186
    }, {
      "referenceID" : 1,
      "context" : "These strategies have been enhanced to be trainable in an end-to-end manner (Arandjelovic et al. (2016); Tang et al.",
      "startOffset" : 77,
      "endOffset" : 104
    }, {
      "referenceID" : 1,
      "context" : "These strategies have been enhanced to be trainable in an end-to-end manner (Arandjelovic et al. (2016); Tang et al. (2016)).",
      "startOffset" : 77,
      "endOffset" : 124
    }, {
      "referenceID" : 1,
      "context" : "These strategies have been enhanced to be trainable in an end-to-end manner (Arandjelovic et al. (2016); Tang et al. (2016)). However, using a codebook on end-to-end trainable second order features leads to an unreasonably large model, since the already large second order model has to be duplicated for each entry of the codebook. This is for example the case in MFAFVNet (Li et al. (2017b)) for which the second order layer alone (i.",
      "startOffset" : 77,
      "endOffset" : 392
    }, {
      "referenceID" : 18,
      "context" : "In section 4, we show an ablation study on the Stanford Online Products dataset (Oh Song et al. (2016)).",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 14,
      "context" : "In this section, we briefly review end-to-end trainable Bilinear pooling (Lin et al. (2015)).",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : "In this section, we briefly review end-to-end trainable Bilinear pooling (Lin et al. (2015)). This method extracts representations from the same image with two CNNs and computes the crosscovariance as representation. This representation outperforms its first-order version and other second-order representations such as Fisher Vectors (Perronnin et al. (2010)) once the global architecture is fine-tuned.",
      "startOffset" : 74,
      "endOffset" : 360
    }, {
      "referenceID" : 5,
      "context" : "Gao et al. (2016) proposed Compact Bilinear Pooling that tackles the high dimensionality of second-order features by analyzing two low-rank approximations of the equivalent polynomial kernel (equation (2) from Gao et al.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "Gao et al. (2016) proposed Compact Bilinear Pooling that tackles the high dimensionality of second-order features by analyzing two low-rank approximations of the equivalent polynomial kernel (equation (2) from Gao et al. (2016)):",
      "startOffset" : 0,
      "endOffset" : 228
    }, {
      "referenceID" : 9,
      "context" : "Kim et al. (2017) improved Compact Bilinear Pooling using Hadamard Product and generalized it for visual question-answering tasks.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 9,
      "context" : "Kim et al. (2017) improved Compact Bilinear Pooling using Hadamard Product and generalized it for visual question-answering tasks. Kong & Fowlkes (2017) introduced Low Rank Bilinear Pooling (LR-BP) that takes advantage of SVM formulation by jointly training the network and the classifier.",
      "startOffset" : 0,
      "endOffset" : 153
    }, {
      "referenceID" : 9,
      "context" : "Kim et al. (2017) improved Compact Bilinear Pooling using Hadamard Product and generalized it for visual question-answering tasks. Kong & Fowlkes (2017) introduced Low Rank Bilinear Pooling (LR-BP) that takes advantage of SVM formulation by jointly training the network and the classifier. The authors propose an efficient factorization as they never compute directly the covariance features and have slightly better results compared to the uncompressed bilinear pooling or Compact Bilinear Pooling. However, as it is, their method is limited to classification with the SVM formulation and cannot be used for other tasks. Li et al. (2017a) introduced Factorized Bilinear Network (FBN), an improved version of Compact Bilinear Pooling.",
      "startOffset" : 0,
      "endOffset" : 640
    }, {
      "referenceID" : 9,
      "context" : "Kim et al. (2017) improved Compact Bilinear Pooling using Hadamard Product and generalized it for visual question-answering tasks. Kong & Fowlkes (2017) introduced Low Rank Bilinear Pooling (LR-BP) that takes advantage of SVM formulation by jointly training the network and the classifier. The authors propose an efficient factorization as they never compute directly the covariance features and have slightly better results compared to the uncompressed bilinear pooling or Compact Bilinear Pooling. However, as it is, their method is limited to classification with the SVM formulation and cannot be used for other tasks. Li et al. (2017a) introduced Factorized Bilinear Network (FBN), an improved version of Compact Bilinear Pooling. FBN never directly computes the covariance matrix. Instead, it projects the features using a hyperplan and computes a quadratic form. The matrix of this quadratic form is supposed rank deficient to reduce the number of parameters and the computation cost with negligible loss in performances. Thus, the output representation (or directly the number of classes for classification tasks) is generated by concatenating these scalars for all projections. Wei et al. (2018) presented Grassmann Bilinear Pooling as a new factorization scheme.",
      "startOffset" : 0,
      "endOffset" : 1204
    }, {
      "referenceID" : 9,
      "context" : "Kim et al. (2017) improved Compact Bilinear Pooling using Hadamard Product and generalized it for visual question-answering tasks. Kong & Fowlkes (2017) introduced Low Rank Bilinear Pooling (LR-BP) that takes advantage of SVM formulation by jointly training the network and the classifier. The authors propose an efficient factorization as they never compute directly the covariance features and have slightly better results compared to the uncompressed bilinear pooling or Compact Bilinear Pooling. However, as it is, their method is limited to classification with the SVM formulation and cannot be used for other tasks. Li et al. (2017a) introduced Factorized Bilinear Network (FBN), an improved version of Compact Bilinear Pooling. FBN never directly computes the covariance matrix. Instead, it projects the features using a hyperplan and computes a quadratic form. The matrix of this quadratic form is supposed rank deficient to reduce the number of parameters and the computation cost with negligible loss in performances. Thus, the output representation (or directly the number of classes for classification tasks) is generated by concatenating these scalars for all projections. Wei et al. (2018) presented Grassmann Bilinear Pooling as a new factorization scheme. The objective is to take advantage of the rank deficient covariance matrix using Singular Value Decomposition (SVD) and then compute the classifier over these Grassmann manifolds. This factorization is efficient in the sense that it never directly computes the second-order representation and contrary to LR-BP, this formulation allows the construction of a representation, by replacing the number of classes by the representation dimension. In practice, however, they need to greatly reduce the input feature dimension due to the SVD complexity which is cubic in the feature dimension. In this work, we start from a similar factorization as Kim et al. (2017) detailed in section 3.",
      "startOffset" : 0,
      "endOffset" : 1932
    }, {
      "referenceID" : 18,
      "context" : "The first representations that take advantage of codebook strategies are Bag of Words (BoW) and in the case of second order information Fisher Vectors (Perronnin et al. (2010)).",
      "startOffset" : 152,
      "endOffset" : 176
    }, {
      "referenceID" : 18,
      "context" : "The first representations that take advantage of codebook strategies are Bag of Words (BoW) and in the case of second order information Fisher Vectors (Perronnin et al. (2010)). Fisher Vectors (FVs) extend the BoW framework by replacing the hard assignment of BoW by a Gaussian Mixture Model (GMM) and then compute the representation as an extension of the Fisher Kernel. In practice, covariance matrices are supposed to be diagonal which leads to representations of size N(2d + 1) where d is the dimension of the features and N is the codebook size. Tang et al. (2016) proposed FisherNet, an architecture that integrates FVs as differentiable layer.",
      "startOffset" : 152,
      "endOffset" : 570
    }, {
      "referenceID" : 12,
      "context" : "Li et al. (2017b) introduced MFA-FV network, a deep architecture which extends the MFA-FV of Dixit & Vasconcelos (2016) by producing a second order information embedding trainable in an endto-end manner.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 12,
      "context" : "Li et al. (2017b) introduced MFA-FV network, a deep architecture which extends the MFA-FV of Dixit & Vasconcelos (2016) by producing a second order information embedding trainable in an endto-end manner.",
      "startOffset" : 0,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "However, even if this rank one decomposition allows interesting dimension reduction (Gao et al. (2016); Kim et al.",
      "startOffset" : 85,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "However, even if this rank one decomposition allows interesting dimension reduction (Gao et al. (2016); Kim et al. (2017)) it is not enough to keep rich representation with smaller dimension.",
      "startOffset" : 85,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "The decompositions of pi and qi play similar roles as intra-projection in VLAD representation (Delhumeau et al. (2013)).",
      "startOffset" : 95,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "We call this approach C-CBP as it corresponds to the extension of CBP (Gao et al. (2016)) to a Codebook strategy.",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "We build our model over pre-trained network such as VGG16 (Simonyan & Zisserman (2014)) or ResNet50 (He et al. (2016)).",
      "startOffset" : 101,
      "endOffset" : 118
    }, {
      "referenceID" : 24,
      "context" : "In this section, we report performances of our factorization on 3 fine-grained visual classification (FGVC) datasets: CUB (Wah et al. (2011)), CARS (Krause et al.",
      "startOffset" : 123,
      "endOffset" : 141
    }, {
      "referenceID" : 11,
      "context" : "(2011)), CARS (Krause et al. (2013)) and AIRCRAFT (Maji et al.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 11,
      "context" : "(2011)), CARS (Krause et al. (2013)) and AIRCRAFT (Maji et al. (2013)).",
      "startOffset" : 15,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "Parameters Full BP - Lin et al. (2015) 84.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 5,
      "context" : "9 256k 200MB CBP-RM - Gao et al. (2016) 83.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "9 256k 200MB CBP-RM - Gao et al. (2016) 83.9 90.5 84.3 8192 38MB CBP-TS - Gao et al. (2016) 84.",
      "startOffset" : 22,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "9 256k 200MB CBP-RM - Gao et al. (2016) 83.9 90.5 84.3 8192 38MB CBP-TS - Gao et al. (2016) 84.0 91.2 84.1 8192 6.3MB MoNet - Gou et al. (2018) 85.",
      "startOffset" : 22,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "9 256k 200MB CBP-RM - Gao et al. (2016) 83.9 90.5 84.3 8192 38MB CBP-TS - Gao et al. (2016) 84.0 91.2 84.1 8192 6.3MB MoNet - Gou et al. (2018) 85.7 90.8 88.1 10k 4KB LRBP - Kong & Fowlkes (2017) 84.",
      "startOffset" : 22,
      "endOffset" : 196
    }, {
      "referenceID" : 5,
      "context" : "9 256k 200MB CBP-RM - Gao et al. (2016) 83.9 90.5 84.3 8192 38MB CBP-TS - Gao et al. (2016) 84.0 91.2 84.1 8192 6.3MB MoNet - Gou et al. (2018) 85.7 90.8 88.1 10k 4KB LRBP - Kong & Fowlkes (2017) 84.2 90.9 87.3 10k 0.8MB FBP - Li et al. (2017a) 82.",
      "startOffset" : 22,
      "endOffset" : 245
    }, {
      "referenceID" : 5,
      "context" : "9 256k 200MB CBP-RM - Gao et al. (2016) 83.9 90.5 84.3 8192 38MB CBP-TS - Gao et al. (2016) 84.0 91.2 84.1 8192 6.3MB MoNet - Gou et al. (2018) 85.7 90.8 88.1 10k 4KB LRBP - Kong & Fowlkes (2017) 84.2 90.9 87.3 10k 0.8MB FBP - Li et al. (2017a) 82.9 SMSO - Yu & Salzmann (2018) 85.",
      "startOffset" : 22,
      "endOffset" : 278
    }, {
      "referenceID" : 17,
      "context" : "In this section, we compare our method to the state-of-the-art on 3 retrieval datasets: Stanford Online Products (Oh Song et al. (2016)), CUB-200-2011 (Wah et al.",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 17,
      "context" : "In this section, we compare our method to the state-of-the-art on 3 retrieval datasets: Stanford Online Products (Oh Song et al. (2016)), CUB-200-2011 (Wah et al. (2011)) and Cars-196 (Krause et al.",
      "startOffset" : 117,
      "endOffset" : 170
    }, {
      "referenceID" : 11,
      "context" : "(2011)) and Cars-196 (Krause et al. (2013)).",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : "(2011)) and Cars-196 (Krause et al. (2013)). For Stanford Online Products and CUB-200-2011, we use the same train/test split as Oh Song et al. (2016). For Cars-196, we use the same as Opitz et al.",
      "startOffset" : 22,
      "endOffset" : 150
    }, {
      "referenceID" : 11,
      "context" : "(2011)) and Cars-196 (Krause et al. (2013)). For Stanford Online Products and CUB-200-2011, we use the same train/test split as Oh Song et al. (2016). For Cars-196, we use the same as Opitz et al. (2017). We report the standard recall@K with K ∈ {1, 10, 100, 1000} for Stanford Online Products and with K ∈ {1, 2, 4, 8, 16, 32} for the other two.",
      "startOffset" : 22,
      "endOffset" : 204
    }, {
      "referenceID" : 17,
      "context" : "r@ 1 10 100 1000 LiftedStruct (Oh Song et al. (2016)) 62.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : "r@ 1 10 100 1000 LiftedStruct (Oh Song et al. (2016)) 62.1 79.8 91.3 97.4 Binomial Deviance (Ustinova & Lempitsky (2016)) 65.",
      "startOffset" : 34,
      "endOffset" : 121
    }, {
      "referenceID" : 17,
      "context" : "r@ 1 10 100 1000 LiftedStruct (Oh Song et al. (2016)) 62.1 79.8 91.3 97.4 Binomial Deviance (Ustinova & Lempitsky (2016)) 65.5 82.3 92.3 97.6 N-Pair-Loss (Sohn (2016)) 67.",
      "startOffset" : 34,
      "endOffset" : 167
    }, {
      "referenceID" : 17,
      "context" : "r@ 1 10 100 1000 LiftedStruct (Oh Song et al. (2016)) 62.1 79.8 91.3 97.4 Binomial Deviance (Ustinova & Lempitsky (2016)) 65.5 82.3 92.3 97.6 N-Pair-Loss (Sohn (2016)) 67.7 83.8 93.0 97.8 HDC (Yuan et al. (2016)) 69.",
      "startOffset" : 34,
      "endOffset" : 212
    }, {
      "referenceID" : 17,
      "context" : "r@ 1 10 100 1000 LiftedStruct (Oh Song et al. (2016)) 62.1 79.8 91.3 97.4 Binomial Deviance (Ustinova & Lempitsky (2016)) 65.5 82.3 92.3 97.6 N-Pair-Loss (Sohn (2016)) 67.7 83.8 93.0 97.8 HDC (Yuan et al. (2016)) 69.5 84.4 92.8 97.7 Margin (Wu et al. (2017)) 72.",
      "startOffset" : 34,
      "endOffset" : 258
    }, {
      "referenceID" : 17,
      "context" : "r@ 1 10 100 1000 LiftedStruct (Oh Song et al. (2016)) 62.1 79.8 91.3 97.4 Binomial Deviance (Ustinova & Lempitsky (2016)) 65.5 82.3 92.3 97.6 N-Pair-Loss (Sohn (2016)) 67.7 83.8 93.0 97.8 HDC (Yuan et al. (2016)) 69.5 84.4 92.8 97.7 Margin (Wu et al. (2017)) 72.7 86.2 93.8 98.0 BIER (Opitz et al. (2017)) 72.",
      "startOffset" : 34,
      "endOffset" : 305
    }, {
      "referenceID" : 17,
      "context" : "0 Proxy NCA (Movshovitz-Attias et al. (2017)) 73.",
      "startOffset" : 13,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "0 Proxy NCA (Movshovitz-Attias et al. (2017)) 73.7 - - HTL (Ge (2018)) 74.",
      "startOffset" : 13,
      "endOffset" : 70
    } ],
    "year" : 2018,
    "abstractText" : "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.",
    "creator" : "LaTeX with hyperref package"
  }
}