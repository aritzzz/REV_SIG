Summary : The authors introduce a variant of NARX RNNs , which has an additional attention mechanism and a reset mechanism . The attention is only applied on subsets of hidden states , referred as delays . The delays are aggregated into a vector using the attention coefficients as weights , and then this vector is multiplied by the reset gates . The model sounds a bit incremental , however , the performance improvements over pMNIST , copy and MobiAct tasks are interesting . A similar kind of architecture has been already proposed : [ 1 ] Soltani et al . “ Higher Order Recurrent Neural Networks ” , arXiv 1605.00064
This paper presents a methodology to allow us to be able to measure uncertainty of the deep neural network predictions , and then apply explore-exploit algorithms such as UCB to obtain better performance in online content recommendation systems . The method presented in this paper seems to be novel but lacks clarity unfortunately . My main doubt comes from Section 4.2.1 , as I am not sure how exactly the two subnets fed into MDN to produce both mean and variance , through another gaussian mixture model . More specifically , I am not able to see how the output of the two subnets get used in the Gaussian mixture model , and also how the variance of the prediction is determined here . Some rewriting is needed there to make this paper better understandable in my opinion . My other concerns of this paper include : 1 . It looks like the training data uses empirical CTR of ( t , c ) as ground truth . This does n't look realistic at all , as most of the time ( t , c ) pair either has no data or very little data in the real world . Otherwise it is a very simple problem to solve , as you can just simply assume it 's a independent binomial model for each ( t , c ) . 2 . In Section 4.2.1 , CTR is modeled as a Gaussian mixture , which does n't look quite right , as CTR is between ( 0,1 ) . 3 . A detailed explanation of the difference between MDN and DDN is needed . 4 . What is OOV in Section 5.3 ?
The authors have undertaken a large scale empirical evaluation on sensitivity and generalization for DNNs within the scope of image classification . They are investigating the suitability of the F-norm of the input-output Jacobian in large scale DNNs and they evaluate sensitivity and generalization metrics across the input space , both on and of the data manifold . They convincingly present strong empirical evidence for the F-norm of the Jacobian to be predictive and informative of generalization of the DNN within the image classification domain . The paper is well written . The problem is clearly presented and motivated . Most potential questions of a reader as well as interesting details are supplied by footnotes and the appendix . The contributions are to my knowledge both novel and significant . The paper seem to be technically correct . The methodology and conclusions are reasonable . I believe that this is important work and applaud the authors for undertaking it . I hope that the interesting leads will be further investigated and that similar studies will be conducted beyond the scope of image classification . ' The research and further investigations would be strengthened if they would include a survey on the networks presented in the literature in a similar manner as the authors did with the generated networks within the presented study . For example compare networks from benchmark competitions in terms of sensitivity and generalization using the metrics presented here . Please define `` generalization gap '' and show how you calculate/estimate it . The term us used differently in much of the machine learning literature ( ? ) . Given this and that the usually sought after generalization error is unobtainable due to the unknown joint distribution over data and label , it is necessary to clarify the precise meaning of `` generalization gap '' and how you calculated it . I intuitively understand but I am not sure that the metric I have in mind is the same as the one you use . Such clarification will also improve the accessibility for a wider audience . Figure 4 : I find Figure 4 : Center a bit confusion . Is it there to show where on the x-axis of Figure 4 : Top , the three points are located ? Does this mean that the points are not located at pi/3 , pi , 5pi/3 as indicated in the figure and the vertical lines of the figure grid ? If it is not , then is it maybe possible to make the different sub-figure in Figure 4 more distinctive , as to not visually float into each other ? Figure 5 : The figure makes me curious about what the regions look like close to the training points , which is currently hidden by the content of the inset squares . Maybe the square content can be made fully transparent so that only the border is kept ? The three inset squares could be shown right below each sub-figure , aligned at the x-axis with the respective position of
Well written and appropriately structured . Well within the remit of the conference . Not much technical novelty to be found , but the original contributions are adequately identified and they are interesting on their own . My main concern ( and complaint ) is not technical , but application-based . This study is ( unfortunately ) typical in that it focuses on and provides detail of the technical modeling issues , but ignores the medical applicability of the model and results . This is exemplified by the fact that the data set is hardly described at all and the 14 abnormalities/pathologies , the rationale behind their choice and the possible interrelations and dependencies are never described from a medical viewpoint . If I were a medical expert , I would not have a clue about how these results and models could be applied in practice , or about what medical insight I could achieve . The bottom line seems to be : `` my model and approach works better than the other guys ' model and approach '' , but one is left with the impression that these experiments could have been made with other data , other problems , other fields of application and they would not have not changed much
This paper proposes an anytime neural network , which can predict anytime while training . To achieve that , the model includes auxiliary predictions which can make early predictions . Specifically , the paper presents a loss weighting scheme that considers high correlation among nearby predictions , an oscillating loss weighting scheme for further improvement , and an ensemble of anytime neural networks . In the experiments , test error of the proposed model was shown to be comparable to the optimal one at each time budget . It is an interesting idea to add auxiliary predictions to enable early predictions and the experimental results look promising as they are close to optimal at each time budget . 1 . In Section 3.2 , there are some discussions on the parallel computations of EANN . The parallel training is not clear to me and it would be great to have more explanation on this with examples . 2 . It seems that EANN is not scalable because the depth is increasing exponentially . For example , given 10 machines , the model with the largest depth would have 2^10 layers , which is difficult to train . It would be great to discuss this issue . 3 . In the experiments , it would be great to add a few alternatives to be compared for anytime predictions .
This is a nice paper proposing a simple but effective heuristic for generating adversarial examples from class labels with no gradient information or class probabilities . Highly relevant prior work was overlooked and there is no theoretical analysis , but I think this paper still makes a valuable contribution worth sharing with a broader audience . What this paper does well : - Suggests a type of attack that has n't been applied to image classifiers - Proposes a simple heuristic method for performing this attack - Evaluates the attack on both benchmark neural networks and a commercial system Problems and limitations : 1 . No theoretical analysis . Under what conditions does the boundary attack succeed or fail ? What geometry of the classification boundaries is necessary ? How likely are those conditions to hold ? Can we measure how well they hold on particular networks ? Since there is no theoretically analysis , the evidence for effectiveness is entirely empirical . That weakens the paper and suggests an important area of future work , but I think the empirical evidence is sufficient to show that there 's something interesting going . Not a fatal flaw . 2 . Poor framing . The paper frames the problem in terms of `` machine learning models '' in general ( beginning with the first line of the abstract ) , but it only investigates image classification . There 's no particular reason to believe that all machine learning algorithms will behave like convolutional neural network image classifiers . Thus , there 's an implicit claim of generality that is not supported . This is a presentation issue that is easily fixed . I suggest changing the title to reflect this , or at least revising the abstract and introduction to make the scope clearer . A minor presentation quibble/suggestion : `` adversarial '' is used in this paper to refer to any class that differs from the true class of the instance to be disguised . But an image of a dalmation that 's labeled as a dalmation is n't adversarial -- it 's just a different image that 's labeled correctly . The adversarial process is about constructing something that will be mislabeled , exploiting some kind of weakness that does n't show up on a natural distribution of inputs . I suggest rewording some of the mentions of adversarial . 3 . Ignorance of prior work . Finding deceptive inputs using only the classifier output has been done by Lowd and Meek ( KDD 2005 ) for linear classifiers and Nelson et al . ( AISTATS 2010 , JMLR 2012 ) for convex-inducing classifiers . Both works include theoretical bounds on the number of queries required for near-optimal adversarial examples . Biggio et al . ( ECML 2013 ) further propose training a surrogate classifier on similar training data , using the predictions of the target classifier to relabel the training data . In this way , decision information from the target model is used to help train a more similar
Summary : this paper proposes algorithmic extensions to two existing RL algorithms to improve exploration in meta-reinforcement learning . The new approach is compared to the baselines on which they are built on a new domain , and a grid-world . This paper needs substantial revision . The first and primary issue is that authors claim their exists not prior work on `` exploration in Meta-RL '' . This appears to be the case because the authors did not use the usual names for this : life-long learning , learning-to-learn , continual learning , multi-task learning , etc . If you use these terms you see that much of the work in these settings is about how to utilize and adapt exploration . Either given a `` free learning phases '' , exploration based in internal drives ( curiosity , intrinsic motivation ) . These are subfields with too much literature to list here . The paper under-review must survey such literature and discuss why these new approaches are a unique contribution . The empirical results do not currently support the claimed contributions of the paper . The first batch of results in on a new task introduced by this paper . Why was a new domain introduced ? How are existing domains not suitable . This is problematic because domains can easily exhibit designer bias , which is difficult to detect . Designing domains are very difficult and why benchmark domains that have been well vetted by the community are such an important standard . In the experiment , the parameters were randomly sampled -- -is a very non-conventional choice . Usually one performance a search for the best setting and then compares the results . This would introduce substantial variance in the results , requiring many more runs to make statistically significant conclusions . The results on the first task are not clear . In fig4 one could argue that e-maml is perhaps performing the best , but the variance of the individual lines makes it difficult to conclude much . In fig5 rl2 gets the best final performance -- -do you have a hypothesis as to why ? Much more analysis of the results is needed . There are well-known measures used in transfer learning to access performance , such as jump-start . Why did you define new ones here ? Figure 6 is difficult to read . Why not define the Gap and then plot the gap . These are very unclear plots especially bottom right . It 's your job to sub-select and highlight results to clearly support the contribution of the paper -- -that is not the case here . Same thing with figure 7 . I am not sure what to conclude from this graph . The paper , overall is very informal and unpolished . The text is littered with colloquial language , which though fun , is not as precise as required for technical documents . Meta-RL is never formally and precisely defined . There are many strong statements e.g. , : ``
In this paper , the authors present an analysis of SGD within an SDE framework . The ideas and the presented results are interesting and are clearly of interest to the deep learning community . The paper is well-written overall . However , the paper has important problems . 1 ) The analysis is widely based on the recent paper by Mandt et al . While being an interesting work on its own , the assumptions made in that paper are very strict and not very realistic . For instance , the assumption that the stochastic gradient noise being Gaussian is very restrictive and trying to justify it just by the usual CLT is not convincing especially when the parameter space is extremely large , the setting that is considered in the paper . 2 ) There is a mistake in the proof Theorem 1 . Even with the assumption that the gradient of sigma is bounded , eq 20 can not be justified and the equality can only be `` approximately equal to '' . The result will only hold if sigma does not depend on theta . However , letting sigma depend on theta is the only difference from Mandt et al . On the other hand , with constant sigma the result is very trivial and can be found in any text book on SDEs ( showing the Gibbs distribution ) . Therefore , presenting it as a new result is misleading . 3 ) Even if the sigma is taken constant and theorem 1 is corrected , I do n't think theorem 2 is conclusive . Theorem 2 basically assumes that the distribution is locally a proper Gaussian ( it is stated as locally convex , however it is taken as quadratic ) and the result just boils down to computing some probability under a Gaussian distribution , which is still quite trivial . Apart from this assumption not being very realistic , the result does not justify the claims on `` the probability of ending in a certain minimum '' -- which is on the other hand a vague statement . First of all `` ending in '' a certain area depends on many different factors , such as the structure of the distribution , the initial point , the distance between the modes etc . Also it is not very surprising that the inverse image of a wider Gaussian density is larger than of a pointy one . This again does not justify the claims . For instance consider a GMM with two components , where the means of the individual components are close to each other , but one component having a very large variance and a smaller weight , and the other one having a lower variance and higher weight . With authors ' claim , the algorithm should spend more time on the wider one , however it is evident that this will not be the case . 4 ) There is a conceptual mistake that the authors assume that SGD will attain
Summary : The paper considers a Bayesian approach in order to infer the distribution over a discrete weight space , from which they derive hardware-friendly low precision NNs . This is an alternative to a standard quantization step , often performed in cases such as emplying NNs on embedded devices . The NN setting considered here contains sign activation functions . The experiments conducted show that the proposed model achieves nice performance on several real world data Comments Due to an error in the openreview platform , I did n't have the chance to bid on time . This is not within my areas of expertise . Sorry for any inconvenience .
The proposed method fraternal dropout is a stochastic alternative of the expectation-linear dropout method , where part of the objective is for the dropout mask to have low variance . The first order way to achieve lower variance is to have smaller weights . The second order is by having more evenly spread weights , so there is more concentration around the mean . As a result , it seems that at least part of the effect of explicitly reducing the variance is just stronger weight penalty . The effect of dropout in the first place is the opposite , where variance is introduced deliberately . So I would like to see some comparisons between this method and various dropout rates , and regular weight penalty combinations . This work is very closely related to expectation linear dropout , except that you are now actually minimizing the variance : 1/2E [ ||f ( s ) - f ( s ' ) || ] is used instead of E [ ||f ( s ) - f_bar|| ] . Eq 5 is very close to this , except the f_bar is not quite the mean , but the value with the mean dropout mask . So all the results should be compared with ELD . I do not think the method is theoretically well-motivated as presented , but the empirical results seem solid . It is somewhat alarming how the analysis has little to do with the neural networks and how dropout works , let along RNNs , while the strength of the empirical results are all on RNNs . I feel the ideas interesting and valuable especially in light of strong empirical results , but the authors should do more to clarify what is actually happening . Minor : why use s_i and s_j , when there is never any reference to i and j ? As far as I can tell , i and j serve as constants , more like s_1 and s_2 .
Error-correcting codes constitute a well-researched area of study within communication engineering . In communication , messages that are to be transmitted are encoded into binary vector called codewords that contained some redundancy . The codewords are then transmitted over a channel that has some random noise . At the receiving end the noisy codewords are then decoded to recover the messages . Many well known families of codes exist , notably convolutional codes and Turbo codes , two code families that are central to this paper , that achieve the near optimal possible performance with efficient algorithms . For Turbo and convolutional codes the efficient MAP decodings are known as Viterbi decoder and the BCJR decoder . For drawing baselines , it is assumed that the random noise in channel is additive Gaussian ( AWGN ) . This paper makes two contributions . First , recurrent neural networks ( RNN ) are proposed to replace the Viterbi and BCJR algorithms for decoding of convolutional and Turbo decoders . These decoders are robust to changes in noise model and blocklength - and shows near optimal performance . It is unclear to me what is the advantage of using RNNs instead of Viterbi or BCJR , both of which are optimal , iterative and runs in linear time . Moreover the authors point out that RNNs are shown to emulate BCJR and Viterbi decodings in prior works - in light of that , why their good performance surprising ? The second contribution of the paper constitutes the design and decoding of codes based on RNNs for a Gaussian channel with noisy feedback . For this channel the optimal codes are unknown . The authors propose an architecture to design codes for this channel . This is a nice step . However , in the performance plot ( figure 8 ) , the RNN based code-decoder does not seem to be outperforming the existing codes except for two points . For both in high and low SNR the performance is suboptimal to Turbo codes and a code by Schalkwijk & Kailath . The section is also super-concise to follow . I think it was necessary to introduce an LSTM encoder - it was hard to understand the overall encoder . This is an issue with the paper - the authors previously mentioned ( 8,16 ) polar code without mentioning what the numbers mean . However , I overall liked the idea of using neural nets to design codes for some non-standard channels . While at the decoding end it does not bring in anything new ( modern coding theory already relies on iterative decoders , that are super fast ) , at the designing-end the Gaussian feedback channel part can be a new direction . This paper lacks theoretical aspect , as to no indication is given why RNN based design/decoders can be good . I am mostly satisfied with the experiments , barring Fig 8 , which does not show the results that the authors claim .
There is no scientific consensus on whether quantum annealers such as the D-Wave 2000Q that use the transverse-field Ising models yield any gains over classical methods ( c.f . https : //arxiv.org/abs/1703.00622 ) . However , it is an exciting research area and this paper is an interesting demonstration of the feasibility of using quantum annealers for reinforcement learning . This paper builds on Crawford et al . ( 2016 ) , an unpublished preprint , who develop a quantum Boltzmann machine reinforcement learning algorithm ( QBM-RL ) . A QBM consists of adding a transverse field term to the RBM Hamiltonian ( negative log likelihood ) , but the benefits of this for unsupervised tasks are unclear ( c.f . https : //arxiv.org/abs/1601.02036 , another unpublished preprint ) . QBM-RL consists of using a QBM to model the state-action variables : it is an undirected graphical model whose visible nodes are clamped to observed state-action pairs . The hidden nodes model dependencies between states and actions , and the weights of the model are updated to maximize the free energy or Q function ( value of the state-action pair ) . The authors extend QBM-RL to work with quantum annealers such as the D-Wave 2000Q , which has a specific bipartite graph structure and requires special consideration because it can only yield samples of hidden variables in a fixed basis . To overcome this , the authors develop a Suzuki-Trotter expansion and call it 'replica stacking ' , where a classical Hamiltonian in one dimension higher is used to approximate the quantum Hamiltonian . This enables the use of quantum annealers . The authors compare their method to standard baselines in a grid world environment . Overall , I do not want to criticize the work . It is an interesting proof of concept . But given the high price of quantum annealers , limited applicability of the technique , and unclear benefits of the authors ' method , I do not think it is relevant to this specific conference . It may be better suited to a workshop specific to quantum machine learning methods . ======================================= + please add an algorithm box for your method . It deviates significantly from QBM-RL . For example , something like : ( 1 ) init weights of boltzmann machine randomly ( 2 ) sample c_eff ~ C from the pool of configurations sampled from the transverse-field Ising model using a quantum annealer with chimera graph ( 3 ) using the samples , calculate effective classical hamiltonian used to approximate the quantum system ( 4 ) use the weight update rules derived from Bellman equations ( spell out the rules ) . + moving the details of sampling into the appendix would help ; they are not important for understanding the main ingredients of your method There are so many moving parts in your system , and someone without a physics background will struggle to understand it . Clarifying the algorithm in terms familiar to machine learning researchers will go a long way toward
The authors approach the task of labeling histology images with just a single global label , with promising results on two different data sets . This is of high relevance given the difficulty in obtaining expert annotated data . At the same time the key elements of the presented approach remain identical to those in a previous study , the main novelty is to replace the final step of the previous architecture ( that averages across a vector ) with a multiplayer perceptron . As such I feel that this would be interesting to present if there is interest in the overall application ( and results of the 2016 CVPR paper ) , but not necessarily as a novel contribution to MIL and histology image classification . Comments to the authors : * The intro starts from a very high clinical level . A introduction that points out specifics of the technical aspects of this application , the remaining technical challenges , and the contribution of this work might be appreciated by some of your readers . * There is preprocessing that includes feature extraction , and part of the algorithm that includes the same feature extraction . This is somewhat confusing to me and maybe you want to review the structure of the sections . You are telling us you are using the first layer ( P=1 ) of the ResNet50 in the method description , and you mention that you are using the pre-final layer in the preprocessing section . I assume you are using the latter , or is P=1 identical to the prefinal layer in your notation ? Tell us . Moreover , not having read Durand 2016 , I would appreciate a few more technical details or formal description here and there . Can you detail about the ranking method in Durand 2016 , for example ? * Would it make sense to discuss Durand 2016 in the base line methods section ? * To some degree this paper evaluates WELDON ( Durand 2016 ) on new data , and compares it against and an extended WELDON algorithm called CHOWDER that features the final MLP step . Results in table 1 suggest that this leads to some 2-5 % performance increase which is a nice result . I would assume that experimental conditions ( training data , preprocessing , optimization , size of ensemble ) are kept constant in between those two comparisons ? Or is there anything of relevance that also changed ( like size of the ensemble , size of training data ) because the WELDON results are essentially previously generated results ? Please comment in case there are differences .
The paper addresses the problem of a mismatch between training classification loss and a loss at test time . This is motivated by use cases in which multiclass classification problems are learned during training , but where binary or reduced multi-class classifications is performed at test time . The question for me is the following : if at test time , we have to solve `` some '' binary classification task , possibly drawn at random from a set of binary problems ( this is not made precise in the paper ) , then why not optimize the same classification error or a surrogate loss at training time ? Instead , the authors start with a multiclass problem , which may introduce a computational burden . when the number of classes is large as one needs to compute a properly normalized softmax . The authors now seem to ask , what if one were to use a multi-classification loss at training time , but then decides at test time that a binary classification of one-vs-all is asked for . If one buys into the relevance of the setting , then of course , one is faced with the problem that the multiclass logits ( aka raw scores ) may not be calibrated to be used for binary classification by applying a fixed threshold . The authors call this sententiously `` Principle of logit separation '' . Not too surprisingly , the standard multiclass losses do not have the desired property , however approaches that reduce multi-class to binary classification at training time do , namely unnormalized models with penalized log Z ( self-normalization ) , the NCE approach , as well as ( the natural in the proposed setting ) binary classification loss . I find this almost a bit circular in the line of argumentation , but ok . It remains odd that while usually one has tried to reduce multiclass to binary , the authors go the opposite direction . The main technical contribution of the paper is the batch-nornalization that makes sure that multiclass logits across mini-batches of data are better calibrated . One can almost think of that as an additional regularization . This seems interesting and does not create much overhead , if one applies mini-batched SGD optimization anyway . However , I feel this technique would need to be investigated with regard to general improvements in a multiclass setting and as such also benchmarked relative to other methods that could be applied .
This paper proposes a method for learning parsers for context-free languages . They demonstrate that this achieves perfect accuracy on training and held-out examples of input/output pairs for two synthetic grammars . In comparison , existing approaches appear to achieve little to no generalization , especially when tested on longer examples than seen during training . The approach is presented very thoroughly . Details about the grammars , the architecture , the learning algorithm , and the hyperparameters are clearly discussed , which is much appreciated . Despite the thoroughness of the task and model descriptions , the proposed method is not well motivated . The description of the relatively complex two-phase reinforcement learning algorithm is largely procedural , and it is not obvious how necessary the individual pieces of the algorithm are . This is particularly problematic because the only empirical result reported is that it achieves 100 % accuracy . Quite a few natural questions left unanswered , limiting what readers can learn from this paper , e.g . - How quickly does the model learn ? Is there a smooth progression that leads to perfect generalization ? - Presumably the policy learned in Phase 1 is a decent model by itself , since it can reliably find candidate traces . How accurate is it ? What are the drawbacks of using that instead of the model from the second phase ? Are there systematic problems , such as overfitting , that necessitate a second phase ? - How robust is the method to hyperparameters and multiple initializations ? Why choose F = 10 and K = 3 ? Presumably , there exists some hyperparameters where the model does not achieve 100 % test accuracy , in which case , what are the failure modes ? Other misc . points : - The paper mentions that `` the training curriculum is very important to regularize the reinforcement learning process . '' Unless I am misunderstanding the experimental setup , this is not supported by the result , correct ? The proposed method achieves perfect accuracy in every condition . - The reimplementations of the methods from Grefenstette et al . 2015 have surprisingly low training accuracy ( in some cases 0 % for Stack LSTM and 2.23 % for DeQueue LSTM ) . Have you evaluated these reimplementations on their reported tasks to tease apart differences due to varying tasks and differences due to varying implementations ?
This paper introduces a graph neural net approach to few-shot learning . Input examples form the nodes of the graph and edge weights are computed as a nonlinear function of the absolute difference between node features . In addition to standard supervised few-shot classification , both semi-supervised and active learning task variants are introduced . The proposed approach captures several popular few-shot learning approaches as special cases . Experiments are conducted on both Omniglot and miniImagenet datasets . Strengths - Use of graph neural nets for few-shot learning is novel . - Introduces novel semi-supervised and active learning variants of few-shot classification . Weaknesses - Improvement in accuracy is small relative to previous work . - Writing seems to be rushed . The originality of applying graph neural networks to the problem of few-shot learning and proposing semi-supervised and active learning variants of the task are the primary strengths of this paper . Graph neural nets seem to be a more natural way of representing sets of items , as opposed to previous approaches that rely on a random ordering of the labeled set , such as the FCE variant of Matching Networks or TCML . Others will likely leverage graph neural net ideas to further tackle few-shot learning problems in the future , and this paper represents a first step in that direction . Regarding the graph , I am wondering if the authors can comment on what scenarios is the graph structure expected to help ? In the case of 1-shot , the graph can only propagate information about other classes , which seems to not be very useful . Though novel , the motivation behind the semi-supervised and active learning setup could use some elaboration . By including unlabeled examples in an episode , it is already known that they belong to one of the K classes . How realistic is this set-up and in what application is it expected that this will show up ? For active learning , the proposed method seems to be specific to the case of obtaining a single label . How can the proposed method be scaled to handle multiple requested labels ? Overall the paper is well-structured and related work covers the relevant papers , but the details of the paper seem hastily written . In the problem set-up section , it is not immediately clear what the distinction between s , r , and t is . Stating more explicitly that s is for the labeled data , etc . would make this section easier to follow . In addition , I would suggest stating the reason why t=1 is a necessary assumption for the proposed model in the few-shot and semi-supervised cases . Regarding the Omniglot dataset , Vinyals et al . ( 2016 ) augmented the classes so that 4,800 classes were used for training and 1,692 for test . Was the same procedure done for the experiments in the paper ? If yes , please update 6.1.1 to make this distinction more clear . If not , please
This paper proposes to apply a group ordered weighted l1 ( GrOWL ) regularization term to promote sparsity and parameter sharing in training deep neural networks and hence compress the model to a light version . The GrOWL regularizer ( Oswal et al. , 2016 ) penalizes the sorted l2 norms of the rows in a parameter matrix with corresponding ordered regularization strength and the effect is similar to the OSCAR ( Bondell & Reich , 2008 ) method that encouraging similar ( rows of ) features to be grouped together . A two-step method is used that the regularizer is applied to a deep neural network at the initial training phase , and after obtaining the parameters , a clustering method is then adopted to force similar parameters to share the same values and then the compacted neural network is retrained . The major concern is that a much more complicated neural network ( with regularizations ) has already been trained and stored to obtain the uncompressed parameters . What ’ s the benefit of the compression and retraining the trained neural network ? In the experiments , the performance of the uncompressed neural network should be evaluated to see how much accuracy loss the regularized methods have . Moreover , since the compressed network loses accuracy , will a smaller neural network can actually achieve similar performance compared to the compressed network from a larger network ? If so , one can directly train a smaller network ( with similar number of parameters as the compressed network ) instead of using a complex two-step method , because the two-step method has to train the original larger network at the first step .
The paper studies the problem of estimating cross-lingual text similarity by mining news corpora . The motivation of the problem and applications are presented well , especially for news recommender systems . However , there are no novel scientific contributions . The idea of fusing standard bi-LSTM layers coupled with a dense fully-connected layer alone is not a substantial technical contribution . Did they try other deep architectures for the task ? The authors cite some previous works to explain their choice of approach for this task . A detailed analysis of different architectures ( recurrent and others ) on the specifc task would have been more convincing . Comparison against other relevant baselines ( including other cross-lingual retrieval approaches ) is missing . There are several existing works on learning cross-lingual word embeddings ( e.g. , Mikolov et al. , 2013 ) . Some of these also make available pre-trained embeddings in multiple languages . You could combine them to learn cross-lingual semantic similarities for the retrieval task . How does your approach compare to these other approaches besides the Siamese LSTM baseline ? Overall , it is unclear what the contributions are — there has been a lot of work in the NLP/IR literature on the same task , yet there is no detailed comparison against any of these relevant baselines . The technical contributions are also not novel or strong to make the paper convincing .
The authors suggest that ideas from statistical mechanics will help to understand the `` peculiar and counterintuitive generalization properties of deep neural networks . '' The paper 's key claim ( from the abstract ) is that their approach `` provides a strong qualitative description of recently-observed empirical results regarding the inability of deep neural networks not to overfit training data , discontinuous learning and sharp transitions in the generalization properties of learning algorithms , etc . '' This claim is restated on p. 2 , third full paragraph . I am sympathetic to the idea that ideas from statistical mechanics are relevant to modern learning theory . However , I do not find this paper at all convincing . I find the paper incoherent : I am unable to understand the argument for the central claims . On the one hand , the paper seems to be written as a `` response '' to Zhang et al . 's `` Understanding Deep Learning Requires Rethinking Generalization '' , ( henceforth Z ) : the introduction mentions Z multiple times , and the title of this work refers to Z . On the other hand , none of the issues raised by Z are ( as far as I can tell ) addressed in any substantial way by this paper . In somewhat more detail , this work discusses two major observations : 1 . Neural nets can easily overtrain , even to random data . 2 . Popular ways to regularize may or may not help . Z certainly observes 1 and arguably observes 2 . ( I 'd argue against , see below , but it 's at least arguable . ) I do not see how this paper addresses either observation . Instead , what the statistical mechanics ( SM ) approach seems to do is explain ( or predict ) the existence of phase transitions , where we suddenly go from a regime of poor generalization to good generalization or vice versa . However , neither Z nor , as far as I can tell , any other reference given here , suggests that these phase transitions are frequently observed in modern deep learning . The most relevant bit from Z is Figure 1c , which suggests that as the noise level is increased ( corresponding to alpha decreasing in this paper ) , the generalization error increases smoothly . This seems to be in direct contradiction to the predictions made by the theories presented here . If the authors wish to hold to the claim that their work `` can provide a qualitative explanation of recently-observed empirical properties that are not easily-understandable from within PAC/VC theory of generalization , as it is commonly-used in ML '' ( p. 2 ) , it is absolutely critical that they be more specific about which specific observations from which papers they think they are explaining . As written , I simply do not see which actual observations they think they explain . In observation 2 , the authors suggest that
Authors propose a reinforcement learning based approach for finding a non-linearity by searching through combinations from a set of unary and binary operators . The best one found is termed Swish unit ; x * sigmoid ( b*x ) . The properties of Swish like allowing information flow on the negative side and linear nature on the positive have been proven to be important for better optimization in the past by other functions like LReLU , PLReLU etc . As pointed out by the authors themselves for b=1 Swish is equivalent to SiL proposed in Elfwing et . al . ( 2017 ) . In terms of experimental validation , in most cases the increase is performance when using Swish as compared to other models are very small fractions . Again , the authors do state that `` our results may not be directly comparable to the results in the corresponding works due to differences in our training steps . '' Based on the Figure 6 authors claim that the non-monotonic bump of Swish on the negative side is very important aspect . More explanation is required on why is it important and how does it help optimization . Distribution of learned b in Swish for different layers of a network can interesting to observe .
This paper presents a reading comprehension model using convolutions and attention . This model does not use any recurrent operation but it is not per se simpler than a recurrent model . Furthermore , the authors proposed an interesting idea to augment additional training data by paraphrasing based on off-the-shelf neural machine translation . On SQuAD dataset , their results show some small improvements using the proposed augmentation technique . Their best results , however , do not outperform the best results reported on the leader board . Overall , this is an interesting study on SQuAD dataset . I would like to see results on more datasets and more discussion on the data augmentation technique . At the moment , the description in section 3 is fuzzy in my opinion . Interesting information could be : - how is the performance of the NMT system ? - how many new data points are finally added into the training data set ? - what do ‘ data aug ’ x 2 or x 3 exactly mean ?
The paper proposes graph-based neural network in which weights from neighboring nodes are adaptively determined . The paper shows importance of propagation layer while showing the non-linear layer does not have significant effect . Further the proposed method also provides class relation based on the edge-wise relevance . The paper is easy to follow and the idea would be reasonable . Importance of the propagation layer than the non-linear layer is interesting , and I think it is worth showing . Variance of results of AGNN is comparable or even smaller than GLN . This is a bit surprising because AGNN would be more complicated computation than GLN . Is there any good explanation of this low variance of AGNN ? Interpretation of Figure 2 is not clear . All colored nodes except for the thick circle are labeled node ? I could n't judge those predictions are appropriate or not .
The paper presents an alternative way to implement weight decay in Adam . Empirical results are shown to support this idea . The idea presented in the paper is interesting , but I have some concerns about it . First , the authors argue that the weight decay should be implemented in a way different from the minimization of a L2 regularization . This seems a very weird statement to me . In fact , it easy to see that what the authors propose is to minimize two different objective functions in SGDW and AdamW ! I am not even sure how I should interpret what they propose . The fact is that SGD and Adam are optimization algorithms , so we can not just change the update rule in the same way in both algorithms and expect them to behave in the same way just because the added terms have the same shape ! Second , the equation ( 5 ) that re-normalize the weight decay parameter as been obtained on one dataset , as the author admit , and tested only on another one . I am not sure this is enough to be considered as a scientific proof . Also , the empirical experiments seem to use the cosine annealing of the learning rate . This means that the only thing the authors proved is that their proposed change yields better results when used with a particular setting of the cosine annealing . What happens in the other cases ? To summarize , I think the idea is interesting but the paper might not be ready to be presented in a scientific conference .
I find this paper not suitable for ICLR . All the results are more or less direct applications of existing optimization techniques , and not provide fundamental new understandings of the learning REPRESENTATION .
Summary : The paper proposes a learnable skimming mechanism for RNN . The model decides whether to send the word to a larger heavy-weight RNN or a light-weight RNN . The heavy-weight and the light-weight RNN each controls a portion of the hidden state . The paper finds that with the proposed skimming method , they achieve a significant reduction in terms of FLOPS . Although it doesn ’ t contribute to much speedup on modern GPU hardware , there is a good speedup on CPU , and it is more power efficient . Contribution : - The paper proposes to use a small RNN to read unimportant text . Unlike ( Yu et al. , 2017 ) , which skips the text , here the model decides between small and large RNN . Pros : - Models that dynamically decide the amount of computation make intuitive sense and are of general interests . - The paper presents solid experimentation on various text classification and question answering datasets . - The proposed method has shown reasonable reduction in FLOPS and CPU speedup with no significant accuracy degradation ( increase in accuracy in some tasks ) . - The paper is well written , and the presentation is good . Cons : - Each model component is not novel . The authors propose to use Gumbel softmax , but does compare other gradient estimators . It would be good to use REINFORCE to do a fair comparison with ( Yu et al. , 2017 ) to see the benefit of using small RNN . - The authors report that training from scratch results in unstable skim rate , while Half pretrain seems to always work better than fully pretrained ones . This makes the success of training a bit adhoc , as one need to actively tune the number of pretraining steps . - Although there is difference from ( Yu et al. , 2017 ) , the contribution of this paper is still incremental . Questions : - Although it is out of the scope for this paper to achieve GPU level speedup , I am curious to know some numbers on GPU speedup . - One recommended task would probably be text summarization , in which the attended text can contribute to the output of the summary . Conclusion : - Based on the comments above , I recommend Accept
This paper presents a convolutional auto-encoder architecture for text encoding and generation . It works on the character level and contains a recursive structure which scales with the length of the input text . Building on the recent state-of-the-art in terms of architectural components , the paper shows the feasibility of this architecture and compares it to LSTM , showing the cnn superiority for auto-encoding . The authors have decided to encode the text into a length of 1024 - Why ? Would different lengths result in a better performance ? You write `` Minimal pre-processing is applied to them since our model can be applied to all languages in the same fashion . '' Please be more specific . Which pre-processing do you apply for each dataset ? I wonder if the comparison to a simple LSTM network is fair . It would be better to use a 2- or 3-layer network . Also , BLSTM are used nowadays . A strong part of this paper is the large amount of investigation and extra experiments . Minor issues : Please correct minor linguistic mistakes as well as spelling mistakes . In Fig . 3 , for example , the t of Different is missing . An issue making it hard to read the paper is that most of the figures appear on another page than where they are mentioned in the text . the authors have chosen to cite a work from 1994 for the vanishing gradient problem . Note , that many ( also earlier ) works have reported this problem in different ways . A good analysis of all researches is performed in Hochreiter , S. , Bengio , Y. , Frasconi , P. , and Schmidhuber , J . ( 2001 ) `` Gradient flow in recurrent nets : the difficulty of learning long-term dependencies '' .
The authors identify a new security threat for deep learning : Decision-based adversarial attacks . This new class of attacks on deep learning systems requires from an attacker only the knowledge of class labels ( previous attacks required more information , e.g. , access to a gradient oracle ) . Unsurprisingly , since the attacker has so few information , such kind of attacks involves quite a lot trial and error . The authors propose one specific attack instance out of this class of attacks . It works as follows . First , an initial point outside of the benign region is guessed . Then multiple steps towards the decision boundary is taken , finally reaching the boundary ( I am not sure about the precise implementation , but it seems not crucial ; the author may please check whether their description of the algorithm is really reproducable ) . Then , in a nutshell , a random walk on a sphere centered around the original , benign point is performed , where after each step , the radius of the sphere is slightly reduced ( drawing the point closer to the original point ) , if and only if the resulting point still is outside of the benign region . The algorithm is evaluated on the following datasets : MNIST , CIFAR , VGG19 , ResNet50 , and InceptionV3 . The paper is rather well written and structured . The text was easy to follow . I suggest that a self-contained description of the problem setting ( assumptions on attacker and defender ; aim ? ) shall be added to the camera-ready version ( being not familiar with the area , I had to read a couple of papers to get a feeling for the setting , before reviewing this paper ) . As in many DL papers these days , there really is n't any math in it worth a mention ; so no reason here to say anything about mathematical soundness . The authors employ a reasonable evaluation criterion in their experiments : the median squared Euclidean distance between the original and adversarially modified data point . The results show consistent improvement for most data sets . In summary , this is an innovative paper , proposing a new class of attacks that totally makes sense in my opinion . Apart from some minor weaknesses in the presentation that can be easily fixed for the camera ready , this is a nice , fresh paper , that might spur more attacks ( and of course new defenses ) from the new class of decision-based attacks . It is worth to note that the authors show that distillation is not a useful defense against such attacks , so we may expect follow-up proposing useful defenses against the new attack ( which BTW is shown to be about a factor of 10 in terms of iterations more costly than the SOTA ) .
The paper proposes to study the influence of ordering in the Curriculum and Self paced learning . The paper is mainly based on empirical justification and observation . The results on 36 data sets show that to some extent the ordering of the training instances in the Curriculum and Self paced learning is not important . The paper involves some interesting ideas and experimental results . I still have some comments . 1 . The empirical results show that different orderings still have different impact for data sets . How to adaptively select an appropriate ordering for given data set ? 2 . The empirical results show that some ordering has negative impact . How to avoid the negative impact ? This question is not answered in the paper . 3 . The ROGS is still clearly inferior to SPLI . It seems that such an observation does not strongly support the claim that ‘ random is good enough ’ .
This work attempts to improve the global consistency of samples generated by generative adversarial networks by replacing the discriminator with an autoregressive model in an encoded feature space . The log likelihood of the classification model is then replaced with the log likelihood of the feature space autoregressive model . It 's not clear what can be said with respect to the convergence properties of this class of models , and this is not discussed . The method is quite similar in spirit to Denoising Feature Matching of Warde-Farley & Bengio ( 2017 ) , as both estimate a density model in feature space -- this method via a constrained autoregressive model and DFM via an estimator of the score function , although DFM was used in conjunction with the standard criterion whereas this method replaces it . This is certainly worth mentioning and discussing . In particular the section in Warde-Farley & Bengio regarding the feature space transformation of the data density seems quite relevant in this work . Unfortunately the only quantitative measurements reporter are Inception scores , which is known to be a poor measure ( and the scores presented are not particularly high , either ) ; Frechet Inception distance or log likelihood estimates via AIS on some dataset would be more convincing . On the plus side , the authors report an average over Inception scores for multiple runs . On the other hand , it sounds as though the stopping criterion was still qualitative .
This paper proposes using long term memory to solve combinatorial optimization problems with binary variables . The authors do not exhibit much knowledge of combinatorial optimization literature ( as has been pointed out by other readers ) and ignore a lot of previous work by the combinatorial optimization community . In particular , evaluating on random instances is not a good measure of performance , as has already been pointed out . The other issue is with the baseline solver , which also seems to be broken since their solution quality seems extremely poor . In light of these issues , I recommend reject .
