Total sentences to be embedded: 295
Total sentences to be embedded: 299
Total sentences to be embedded: 275
Total sentences to be embedded: 257
(257, 768) (1, 257, 768)
torch.Size([790, 768]) torch.Size([790, 768])
torch.Size([1, 790, 768]) torch.Size([1, 790, 768])
{'Recommendation': 5.254, 'Confidence': 4.025, 'Exhaustive': 4.48, 'Aspectual Score': 5.677, 'Intensity': 3.401, 'Actual REC': '9', 'Actual CONF': '4', 'comments': "The paper provides a number of novel interesting theoretical results on `` vanilla '' Gaussian Variational Auto-Encoders ( VAEs ) ( sections 1 , 2 , and 3 ) , which are then used to build a new algorithm called `` 2 stage VAEs '' ( Section 4 ) . The resulting algorithm is as stable as VAEs to train ( it is free of any sort of adversarial training , it comes with a little overhead in terms of extra parameters ) , while achieving a quality of samples which is *very impressive* for an Auto-Encoder ( AE ) based generative modeling techniques ( Section 5 ) . In particular , the method achieves FID score 24 on the CelebA dataset which is on par with the best GAN-based models as reported in [ 1 ] , thus sufficiently reducing the gap between the generative quality of the GAN-based and AE-based models reported in the literature . Main theoretical contributions : 1 . In some cases the variational bound of Gaussian VAEs can get tight ( Theorem 1 ) . In the context of vanilla Gaussian VAEs ( Gaussian prior , encoders , and decoders ) the authors show that if ( a ) the intrinsic data dimensionality r is equal to the data space dimensionality d and ( b ) the latent space dimensionality k is not smaller than r then there is a sequence of encoder-decoder pairs achieving the global minimum of the VAE objective and simultaneously ( a ) zeroing the variational gap and ( b ) precisely matching the true data distribution . In other words , in this setting the variational bound and the Gaussian model does not prevent the true data distribution from being recovered . 2 . In other cases Gaussian VAEs may not recover the actual distribution , but they will recover the real manifold ( Theorems 2 , 3 , 4 and discussions on page 5 ) . In case when r < d , that is when the data distribution is supported on a low dimensional smooth manifold in the input space , things are quite different . The authors show that there are still sequences of encoder-decoder pairs which achieves the global minimum of the VAE objective . However , this time only *some* of these sequences converge to the model which is in a way indistinguishable from the true data distribution ( and thus again Gaussian VAEs do not fundamentally prevent the true distribution from being recovered ) . Nevertheless , all sequences mentioned above recover the true data manifold in that ( a ) the optimal encoder learns to use r dimensional linear subspace in the latent space to encode the inputs in a lossless and noise-free way , while filling the remaining k - r dimensions with a white Gaussian noise and ( b ) the decoder learns to ignore the k - r noisy dimensions and use the r `` informative '' dimensions to produce the outputs perfectly landing on the true data manifold . Main algorithmic contributions : ( 0 ) A simple 2 stage algorithm , where first a vanilla Gaussian VAE is trained on the input dataset and second a separate vanilla Gaussian VAE is trained to match the aggregate posterior obtained after the first stage . The authors support this algorithm with a reasonable theoretical argument based on theoretical insights listed above ( see end of page 6 - beginning of page 7 ) . The algorithm achieves state-of-art FID scores across several data sets among AE based models existing in the literature . Review summary : I would like to say that this paper was a breath of fresh air to me . I really liked how the authors make a strong point that *it is not the Gaussian assumptions that harm the performance of VAEs* in contrast to what is usually believed in the field nowadays . Also , I think *the reported FID scores alone may be considered as a significant enough contribution* , because to my knowledge this is the first paper significantly closing the gap between generative quality of GAN-based models and non-adversarial AE-based methods . *************** *** Couple of comments and typos : *************** ( 0 ) Is the code / checkpoints going to be available anytime soon ? ( 1 ) I would mention [ 2 ] which in a way used a very similar approach , where the aggregate posterior of the implicit generative model was modeled with a separate implicit generative model . Of course , two approaches are very different ( [ 2 ] used an adversarial training to match the aggregate posterior ) , however I believe the paper is worth mentioning . ( 2 ) In light of the discussion on page 6 as well as some of the conclusions regarding commonly reported blurriness of the VAE models , results of Section 4.1 of [ 3 ] look quite relevant . ( 3 ) It would be nice to specify the dimensionality of the Sz matrix in definition 1 . ( 4 ) Line ater Eq . 3 : I think it should be $ \\int p_gt ( x ) \\log p_\\theta ( x ) dx $ ? ( 5 ) Eq 4 : p_\\theta ( x|x ) ( 6 ) Page 4 : `` ... mass to most all measurable ... '' . ( 7 ) Eq 34 . Is it sqrt ( \\gamma_t ) or just \\gamma_t ? ( 8 ) Line after Eq 40 . Why exactly D ( u^* ) is finite ? I only checked proofs of Theorems 1 and 2 in details and those looked correct . [ 1 ] Lucic et al. , 2018 . [ 2 ] Zhao et al. , Adversarially regularized autoencoders , 2017 , http : //proceedings.mlr.press/v80/zhao18b.html [ 3 ] Bousquet et al. , From optimal transport to generative modeling : the VEGAN cookbook . 2017 , https : //arxiv.org/abs/1705.07642"}
(257, 768) (1, 257, 768)
torch.Size([790, 768]) torch.Size([790, 768])
torch.Size([1, 790, 768]) torch.Size([1, 790, 768])
{'Recommendation': 5.121, 'Confidence': 4.132, 'Exhaustive': 5.853, 'Aspectual Score': 7.387, 'Intensity': 3.289, 'Actual REC': '7', 'Actual CONF': '4', 'comments': "Overview : I thank the authors for their interesting and detailed work in this paper . I believe it has the potential to provide strong value to the community interested in using VAEs with an explicit and simple parameterization of the approximate posterior and likelihood as Gaussian . Gaussianity can be appropriate in many cases where no sequential or discrete structure needs to be induced in the model . I find the mathematical arguments interesting and enlightening . However , the authors somewhat mischaracterize the scope of applicability of VAE models in contemporary machine learning , and do n't show familiarity with the broad literature around VAEs outside of this case ( that is , where a Gaussian model of the output would be manifestly inappropriate ) . Since the core of the paper is valuable and salvageable from a clarity standpoint , my comments below are geared towards what changes the authors may make to move this paper into the `` pass '' category . Pros : - Mathematical insights are well reasoned and interesting . Based on the insight from the analysis in the supplementary materials , the authors propose a two-stage VAE which separate learning the a parsimonious representation of the low-dimensional ( lower than the ambient dimension of the input space ) , and the training a second VAE to learn the unknown approximate posterior . The two-stage training procedure is both theoretically motivated and appears to enhance the output quality of VAEs w.r.t . FID score , making them rival GAN architectures on this metric . Cons : - The title and general tone of the paper is too broad : it is only VAE models with Gaussian approximate posteriors and likelihoods . This is hardly the norm for most applications , contrary to the claims of the authors . VAEs are commonly used for discrete random variables , for example . Many cases where VAEs are applied can not use a Gaussian assumption for the likelihood , which is the key requirement for the proofs in the supplement to be valid ( then , the true posterior is also Gaussian , and the KL divergence between that and the approximate posterior can be driven to zero during optimization -- clearly a Gaussian approximate posterior will never have zero KL divergence with a non-Gaussian true posterior ) . - None of the proofs consider the approximation error garnered by only having access to empirical samples through a sample of the ground truth population . ( The ground-truth distribution must be defined with respect to the population rather just the dataset in hand , otherwise we lose all generalizability from a model . ) Moreover , the proofs hold asymptotically . Generalization bounds and error from finite time approximations are very pertinent issues and these are ignored by the presented analyses . Such concerns have motivated many of the recent developments in approximate posterior distributions . Overall , the paper contains little evidence of familiarity with the recent advances in approximate Bayesian inference that have occurred over the past two years . - A central claim of the paper is that the two-stage VAE obviates the need for highly adaptive approximate posteriors . However , no comparison against those models is done in the paper . How does a two-stage VAE compare against one with , e.g. , a normalizing flow approximate posterior ? I acknowledge that the purpose of the paper was to argue for the Gaussianity assumption as less stringent than previously believed , but all of the mathematical arguments take place in an imagined world with infinite time and unbounded access to the population distribution . This is not really the domain of interest in modern computational statistics / machine learning , where issues of generalization and computational efficiency are paramount . - While the mathematical insights are well developed , the specifics of the algorithm used to implement the two-stage VAE are a little opaque . Ancestral sampling now takes place using latent samples from a second VAE . An algorithm box is badly needed for reproducibility . Recommendations / Typos I noted a few typos and omissions that need correction . - Generally , the mathematical proofs in section 7 of the supplement are clear . At the top of page 11 , though , the paragraph correctly begins by stating that the composition of invertible functions is invertible , but fails to establish that G is also invertible . Clearly it is so by construction , but the explicit reasons should be stated ( as a prior sentence promises ) , and so I assume this is an accidental omission . - The title of Section 8.1 has a typo : clearly is it is the negative log of p_ { theta_t } ( x ) which approaches its infimum rather than p_ { theta_t } ( x ) approaching negative infinity . - Equation ( 4 ) : the true posterior has an x as its argument instead of the latent z . - Missing parenthesis under Case 2 and wrong indentation . This analysis also seems to be cut off . Is the case r > d relevant here ? * EDIT : I have read the authors ' detailed response . It has clarified a few key issues , and convinced me of the value to the community for publication in its present ( slightly edited according to the reviwers ' feedback ) form . I would like to see this published and discussed at ICLR and have revised my score accordingly . *"}
(257, 768) (1, 257, 768)
torch.Size([790, 768]) torch.Size([790, 768])
torch.Size([1, 790, 768]) torch.Size([1, 790, 768])
{'Recommendation': 5.303, 'Confidence': 3.854, 'Exhaustive': 2.397, 'Aspectual Score': 2.971, 'Intensity': 3.915, 'Actual REC': '6', 'Actual CONF': '3', 'comments': 'This paper proposed a two-stage VAE method to generate high-quality samples and avoid blurriness . It is accomplished by utilizing a VAE structure on the observation and latent variable separately . The paper exploited a collection of interesting properties of VAE and point out the problem existed in the generative process of VAE . I have several concerns about the paper : 1 . It is necessary to explain why the second-stage VAE can have its latent variable more closely resemble N ( u|0 , I ) . Even if the latent variable closely resemble N ( u|0 , I ) , How does it make sure the generated images are realistic ? I admit that the VAE model can reconstruct realistic data based on its inferred latent variable , however , when given a random sample from N ( u|0 , I ) , the generated images are not good , which is true when the dimension of the latent space is high . I still can â€™ t understand why a second-stage VAE can relief this problem . 2 . The adversarial auto-encoder is also proposed to solve the latent space problem , by comparison , what is the advantage of this paper ? 3 . Why do you set the model as two separate stages ? Will it enhance the performance if we train theses two-stages all together ? 4 . The proofs for the theory 2 and 3 are under the assumption that the manifold dimension of the observation is r , while in reality it is difficult to obtain this r , do these theories applicable if we choose a value for the dimension of the latent space that is smaller than the real manifold dimension of the observation ? How will it affect the performance of the proposed method ? 5 . The value of r and k in each experiment should be specified .'}
(257, 768) (1, 257, 768)
torch.Size([790, 768]) torch.Size([790, 768])
torch.Size([1, 790, 768]) torch.Size([1, 790, 768])
{'Recommendation': 5.376, 'Confidence': 3.709, 'Exhaustive': 1.314, 'Aspectual Score': 1.446, 'Intensity': 4.529, 'Actual REC': 0, 'Actual CONF': 0, 'comments': ''}
