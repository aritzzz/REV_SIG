paper entitled siamese survival analysis reports application deep learning three cases competing risk survival analysis author follow reasoning ideas explored context survival analysis thereby disregarding significant published literature based concordance index ci nbesides deficit paper present proper statistical setup censoring assumed random numerical results referring standard implementations thereby neglecting state art solution said particular use deep learning context might novel
paper introduces new memory mechanism specifically tailored agent navigation 2d environments memory consists 2d array includes trainable read write mechanisms rl agent policy function context read read next step write vectors functions observation effectiveness proposed architecture evaluated via reinforcement learning mazes solved evaluation included 1000 test mazes sets good precedent evaluation subfield nmy main concern lack experiments test whether agent really learned localize plan routes using memory architecture downsampling experiment section seems indicate contrary downsampling memory lead position aliasing seems indicate agent using memory store map location concerned whether proposed agent actually employing navigation strategy seems suggested simply good agent architecture task optimization reasons short experiment appendix seems try answer question results anecdotal best nif good rl performance navigation tasks ultimate goal one imagine agent directly copies raw map observation world centric memory use something like value iteration network shortest path planning plan routes point classical algorithms solve navigation even partially observable 2d grid worlds bother deep rl
paper introduces bi directional block self attention model bi biosan general purpose encoder sequence modeling tasks nlp experiments include tasks like natural language inference reading comprehension squad semantic relatedness sentence classifications new model shows decent performance comparing bi lstm cnn baselines running reasonably fast speed nthe advantage model use little memory rnns enjoy parallelizable computation sans achieve similar better performance nwhile appreciate solid experiment section think model sufficient contribution publication iclr first much innovation model architecture idea bi biosan model simply split sentence blocks compute self attention using mechanisms pooling operation followed fusion level think counts careful engineering san model rather main innovation second model introduces much parameters experiments easily use times parameters commonly used encoders use amount parameters bi lstm encoders gap new model commonly used ones smaller ni appreciate answers authors added change score
paper authors studied problem semi supervised shot classification extending prototypical networks setting semi supervised learning examples distractor classes studied problem interesting paper well written extensive experiments performed demonstrate effectiveness proposed methods proposed method natural extension existing works soft means meta learning top seems authors claimed model capability first place proposed model properly classify distractor examples consider single class outliers overall would like vote weakly acceptance regarding paper
got access paper review deadline chance read hence lateness brevity nthe paper tackles important theoretical question offers results complementary existing results soudry et al however paper properly relate results assumptions context existing literature much explanation needed author reply order clear questions nthe work evaluated practical perspective theoretical nature ni agree criticism raised reviewers however also believe authors managed clear essentially criticism reply paper lacks clarity currently written nthe results interesting explanation needed main message conveyed clearly suggest paper potential become eyes future resubmission
paper proposes alternative relation network architecture whose computational complexity linear number objects present input model achieves good results babi compared memory networks relation network model understood works computing weighted average sentence representations input story attention weights output mlp whose input sentence question two sentences question average fed softmax layer answer prediction found difficult understand model related relation networks since longer scores every combination objects case babi sentences fundamental idea behind relation networks approach evaluated clevr interaction two objects perhaps critical main result original relation networks paper fact model works well babi despite simplicity interesting feels like paper framed suggest object object interactions necessary explicitly model agree based solely babi experiments encourage authors detailed experimental study tasks recommend paper acceptance current form nother questions comments use mlp produce attention weight without extrinsic computation input sentence question statement false attention computation takes input concatenation question sentence representation writing could cleaned spelling grammar last 70 stories instead last 70 sentences currently paper hard read took understand model
algorithm lot problem specific hyperparametes may difficult get right clear important analyze simpler analytically likely computationally boolean hyperparameter case hyperparameter binary realistic setting experiments use binary parameter spaces sure much buy straightforward use continuous valued polynomials interesting idea think theoretical practical feels like hammer need nail
congratulations interesting clear paper iclr focused neuroscientific studies paper clearly belongs shows representations develop recurrent networks trained spatial navigation interestingly include representations observed mammals attracted considerable attention even honored nobel prize ni found interesting emergence representations contingent regularization constraint seems similar visual domain edge detectors emerge easily trained natural images sparseness constraints olshausen field later reproduced many models incorporate sparseness constraints ni questions training paper mentions metabolic cost specified paper added nmy biggest concern figure 6a puzzled error coming boundary interaction even puzzling error go blue curve interaction u2019t least curve smooth
high quality paper tackles quadratic dependency memory sequence length attention based models presents strong empirical results across multiple evaluation tasks approach basically apply self attention two levels level small fixed number items thereby limiting memory requirement negligible impact speed captures local information called blocks using self attention applies second level self attention blocks nthe paper well organized clearly written modulo minor language mistakes easy fix proof reading contextualization method relative cnns rnns transformers good beneficial trade offs memory runtime accuracy thoroughly investigated compelling ni curious story would look one tried push beyond two levels example effective might inter sentence attention level obtaining representations long documents nminor points text eq appears twice one instance probably multiple locations s4 nli word premise promise missing word first sentence s4 reason __
paper presents extension binary networks main idea use different bit rates different layers reduce bitrate overall net achieve better performance speed memory paper addresses real problem meaningful provides interesting insights extension nthe description heterogeneous bitwidth binarization algorithm interesting simple potentially practical however also adds complication real world implementations might elegant enough approach practical usages nexperiments wise paper done solid experiments comparing existing approaches showed gain results promising noverall leaning towards rejection mostly due limited novelty
paper proposes address quadratic memory time requirement relation network rn sequentially attending via multiple layers objects gating object vectors attention weights layer proposed model obtains state art babi story based qa babi dialog task npros model achieves state art babi qa dialog think significant achievement given simplicity model paper clearly written ncons sure novel proposed model authors use notations used relation network see relevance relation network rather exactly resembles end end memory network memn2n gmemn2n please tell missing something sure contribution paper course notice small architectural differences responsible improvements believe authors conducted ablation study qualitative analysis show small tweaks meaningful nquestion exact contribution paper respect memn2n gmemn2n
pros nthe paper proposes u201cbi directional block self attention network bi blosan u201d sequence encoding inherits advantages multi head vaswani et al 2017 disan shen et al 2017 network claimed memory efficient paper written clearly easy follow source code released duplicability main originality using block hierarchical structures proposed models split entire sequence blocks apply intra block san block modeling local context apply inter block san output blocks capture long range dependency proposed model tested nine benchmarks achieve good efficiency memory trade ncons methodology paper incremental compared previous models many baselines listed paper competitive snli state art results included paper paper argues advantages proposed models cnn assuming latter captures local dependency however supported discussion comparison hierarchical cnn block splitting detailed appendix rather arbitrary terms potentially divides coherent language segments apart unnatural compared alternatives using linguistic segments blocks main originality paper block style however paper u2019t analyze block brings improvement remove intra block self attention keep token level self attention whether performance significantly worse
propose data augmentation bc learning relevant much robust frequency jitter simple data augmentation nin equation please check measure mixture simply use db criteria nthe comments applying cnn local features novel approach increase sound recognition could completed iclr 2017 work towards injected priors using chirplet transform nthe authors might discuss extend model image recognition least modalities suggested nsection shall placed later clarified ndiscussion mixing two sounds leads could completed associative properties think
paper proposes combining classification specific neural networks auto encoders done straightforward manner designating nodes output layer classification reconstruction training objective changed minimize sum classification loss measured cross entropy instance reconstruction error measured ell error done training auto encoders nthe authors minimize loss function greedy layer wise training done several prior works authors perform experiments learned representations output layer corresponding classification corresponding reconstruction example authors plot nearest neighbors classification features reconstruction features observe two different authors also observe interpolating two reconstruction feature vectors convex combinations seems interpolate well two corresponding images nwhile experimental results interesting striking especially viewed context tremendous amount work auto encoders training classification features along reconstruction features seem give significantly new insights
nthis paper proposes use deep reinforcement learning solve multiagent coordination task particular paper introduces benchmark domain model fleet coordination problems might encountered taxi companies nthe paper really introduce new methods paper seen application paper think paper could merits would really push boundary feasible think really case paper task still seems quite simplistic empirical evaluation convincing limited analysis weak baselines really see real grounds acceptance nfinally also many weaknesses paper quite poorly written places poor formatting citations incorrect half bibtex entry inlined highly inadequate treatment related work instance many related papers taxi fleet management work pradeep varakantham coordination multi robot systems spatially distributed tasks gerkey much work since scaling multiagent reinforcement learning multiagent mdps guestrin et al 2002 kok vlassis 2006 etc dealing partial observability work decentralized pomdps peshkin et al 2000 bernstein amato etc multiagent deep rl active last years see papers foerster sukhbataar omidshafiei noverall see paper improvements could make nice workshop contribution paper published top tier venue
work authors propose procedure tuning parameters hmc algorithm guess understood correctly ni think paper good strong point work points difficulties choosing properly parameters hmc method step number iterations leapfrog integrator instance literature specially machine learning fever u2019 u2019 hmc opinion partially unjustified nif understood method adaptive hmc algorithm parameters updated online training done advance please remark clarify point nhowever additional comments eqs quite complicated think running toy example help interested reader suggest compare proposed method efficient methods use gradient information cases multimodal posteriors use gradient information counter productive sampling purposes multiple try metropolis mtm schemes nl martino read flexibility design multiple try metropolis schemes computational statistics volume 28 issue pages 2797 2823 2013 nadaptive techniques nh haario saksman tamminen adaptive metropolis algorithm bernoulli 223 u2013242 april 2001 nand component wise strategies gibbs sampling nw gilks wild adaptive rejection sampling gibbs sampling appl statist vol 41 pp 337 u2013348 199 u2028 nat least add brief paragraph introduction citing discussing possible alternatives
paper proposes training autoencoder middle layer representation consists class label input hidden vector representation called style memory would presumably capture non class information idea learning representations decompose class specific class agnostic parts generally style content interesting long standing problem results paper mostly qualitative mnist show convincingly network managed learn interesting class specific class agnostic representations clear whether examples shown figures 11 representative network general behavior tsne visualization figure seems indicate style memory representation capture class information well raw pixels indicate whether representation sensible nthe use fully connected networks images may affect quality learned representations may necessary use convolutional networks get interesting results may also interesting consider class specific representations general class label example see learning nonlinear embedding preserving class neighbourhood structure salakhutdinov hinton 2007 learns hidden vector representations class specific class agnostic parts paper cited
pros n1 new dna structure gan utilized manipulate disentangle attributes n2 non attribute part explicitly modeled framework n3 based experiment results proposed method outperformed previous methods td gan icgan ncons n1 assumes individual piece represents independent factor variation hold time authors also admit two factors dependent method might fail n2 lreconstruct min difference a1 considered a2 seems a2 also similar since one bit a2 a1 different n3 one attribute manipulated time possible change one attribute time method
paper proposed generalized hmc modifying leapfrog integrator using neural networks make sampler converge mix quickly mixing one challenge problems mcmc sampler particularly many modes distribution derivations look correct experiments proposed algorithm compared methods nice mc hmc showed proposed method could mix modes posterior although method could mix well applied particular experiments lacks theoretical justifications method could mix well
paper proposes tensor train decomposition ring structure function approximation data compression techniques used well known tensor community outside machine learning main contribution paper introduce techniques ml community presents experimental results support nthe paper rather preliminary examination example claimed proposed decomposition provides enhanced representation ability justified rigorously either via comprehensive experimentation via theoretical justification furthermore paper lacks novelty aspect uses mostly well known techniques
paper proposes learn custom translation rotation invariant kernel fourier representation maximize margin svm instead using monte carlo approximation traditional random features literature main point paper learn fourier features min max sense perspective leads interesting theoretical results new interpretation synthetic simple real world experiments demonstrate effectiveness algorithm compared random features given fix number bases ni like idea trying formulate feature learning problem two player min max game connection boosting related work seems authors missed relevant pieces work learning fourier features gradient descent would interesting compare algorithms well zichao yang marcin moczulski misha denil nando de freitas alex smola le song ziyu wang deep fried convnets iccv 2015 zichao yang alexander smola le song andrew gordon wilson la carte u2014 learning fast kernels aistats 2015
majority paper focused observation making policies condition time step important finite horizon problems much smaller component episodes terminated early learning say restart promote exploration values bootstrapped reflect additional rewards received true infinite horizon setting n1 true well known typically described finite horizon mdp planning learning optimal policy well known nonstationary depend number remaining time steps number papers focusing planning learning though cited current draft ni u2019t immediately know work suggests bootstrapping episode terminated early artificially training seems reasonable straightforward thing
paper presents novel representation graphs multi channel image like structures structures extrapolated n1 mapping graph nodes embedding using algorithm like node2vec n2 compressing embedding space using pca n3 extracting 2d slices compressed space computing 2d histograms per slice nhe resulting multi channel image like structures feed vanilla 2d cnn nthe papers well written clear proposes interesting idea representing graphs multi channel image like structures furthermore authors perform experiments real graph datasets social science domain comparison soa method graph kernels deep learning architectures proposed algorithm datasets two theme statistical significant
authors present algorithm training ensembles policy networks regularly mixes different policies ensemble together distilling mixture two policies single policy network adding ensemble selecting strongest networks remain certain definitions strong network experiments compare favorably ppo a2c baselines variety mujoco tasks although would appreciate wall time comparison well training crossover network presumably time consuming nit seems much paper authors could dispense genetic terminology altogether mean compliment valuable ideas field evolutionary computing glad see authors use sensible gradient based learning gpo even makes depart many field would consider evolutionary computing another point terminology important emphasize method training crossover network direct supervised learning expert trajectories technically imitation learning behavioral cloning would perhaps even call distillation network rather crossover network many robotics tasks behavioral cloning known overfitting expert trajectories may problem setting expert trajectories generated unlimited quantities
paper extends framework neural networks finite dimension case infinite dimension setting called deep function machines theory seems interesting might potential applications
paper introduces smooth surrogate loss function top svm purpose plugging svm deep neural networks idea replace order statistics smooth lot zero partial derivatives exponential averages smooth good approximation order statistics good selection temperature parameter paper well organized clearly written idea deserves publication non hand might better direct solutions reduce combinatorial complexity temperature parameter small enough original top svm surrogate loss smooth loss computed precisely sorting vector first take good care boundary around s_
paper authors present computational framework active vision problem motivating study biologically authors explain control policy learned reduce entropy posterior belief present application mnist digit classification substantiate proposal ni convinced novelty contribution work active vision sensing problem well studied information theory bayes risk formulations already considered previous works see najemnik geisler 2005 butko movellan 2010 ahmad yu 2013 nthe paper also rife spelling mistakes grammatical errors needs thorough revision examples foveate inspection data abstract may allow motivation tu put clear motivation contrary animals retina footnote minimize current uncertainty perception driven control center keep fovea based implementation degrade te recognition outlook perspective citations non standard format section kalman 1960 noverall think paper considers important problem contribution state art minimal editing highly lacking n1 najemnik geisler optimal eye movement strategies visual search nature 434 7031 387 u201391 2005 n2 butko movellan infomax control eye movements ieee transactions autonomous mental development 91 u2013107 2010 n3 ahmad yu active sensing bayes optimal sequential decision making uncertainty artificial intelligence 2013
paper incomplete nowhere near finished withdrawn nthe theoretical results presented bitmap figure referred text explained results datasets explained either pretty bad waste time
paper introduces technique program synthesis involving restricted grammar problems beam searched using attentional encoder decoder network work knowledge first use dsl closer full language nthe paper clear easy follow one way could improved compared another system results showing guided search potent combination whose contribution would made stronger compared existing work
paper presents practical methodology use neural network recommending products users based past purchase history model contains three components predictor model essentially rnn style model capture near term user interests time decay function serves way decay input based purchase happened auto encoder component makes sure user past purchase history get fully utilized consideration time decay paper showed combination three performs best terms precision pcc also good scalability also showed good online test performance indicates approach tested real world ntwo small concerns n1 section fully sure proposed predictor model able win lstm lstm tends mitigate vanishing gradient problem likely would exist predictor model insights might useful n2 title paper weird suggest rephrase unreasonable something positive
paper applies recently developed ideas literature robust optimization particular distributionally robust optimization wasserstein metric showed framework smooth loss functions much robustness requested resulting optimization problem difficulty level original one adversarial attack concerned think idea intuitive reasonable result nice although holds light robustness imposed practice seems case say large deviation adversary exists adversarial training important topic deep learning feel work may lead promising principled ways adversarial training
hard interpret work authors mention original work gutmann colleague nce required details paper provides proof non parametric case optimum nce objective function data distribution normalisation constant either learned held fixed value like exactly purpose paper nthere number minor issues well language modelling compute normalisation term nce training testing explicitly stated authors referring chen 2016 whole point using nce equation comes
paper proposes principled methodology induce distributional robustness trained neural nets purpose mitigating impact adversarial examples idea train model perform well respect unknown population distribution perform well worst case distribution ball around population distribution particular authors adopt wasserstein distance define ambiguity sets allows use strong duality results literature distributionally robust optimization express empirical minimax problem regularized erm different cost theoretical results paper supported experiments noverall well written paper creatively combines number interesting ideas address important problem
paper authors present adaptation space time non negative matrix factorization sbt nmf rigorously account pre stimulus baseline activity authors go compare baseline corrected bc method several established methods dimensionality reduction spike train data noverall results bit mixed bc method often performs similarly outperformed non bc sbt nmf authors provide possible mechanism explain results analyzing classification performance function baseline firing rate authors posit method useful sensory responses order magnitude baseline activity however fully address non bc sbt nmf strongly outperform bc method certain tasks step light fig 3b finally method introduces principled way remove mean baseline activity sensory driven response may also discount effect baseline firing rate fast temporal fluctuations response destexhe et al nature reviews neuroscience 2003 gutnisky da et al cerebral cortex 27 2017
paper well written clearly explained paper experimental paper content experimentation less content problem definition formulation experimental section strong evaluated across different datasets various scenarios however feel contribution paper toward topic incremental significant enough accepted venue considers slight modification loss function adding trace norm regularization
paper authors study relationship training gans primal dual subgradient methods convex optimization technique applied top existing gans address issues mode collapse authors also derive gan variant similar wgan called approximate wgan experiments synthetic datasets demonstrate proposed formulation avoid mode collapse strong contribution nin table difference inception scores dcgan approach seems significant ignore authors explain possibly nthere typo page u2013 varaints variants
paper proposes multi view semi supervised method unlabelled data single input picture partitioned new inputs permitting overlap new objective obtain predictions close possible prediction model learned mere labeled data nto precise seen last formula section important factor function kl distance used author said could set noisy parameter first part zero leave parameter non zero second term otherwise model learn anything nmy understanding key factor called views first sight method resembles conventional ensemble learning much smoothing distribution around input consistency related loss another word set unlabeled data use unlabeled data times scale assuming duplicate unlabeled data keeping training consistency objective method would new method obtain similar performance understanding correct authors discuss key novelty compared previous work stated second paragraph section one obvious merit unlabeled data utilized efficiently times better
3d data processing important topic nowadays since lot applications robotics ar vr etc ncurrent approaches 2d image processing based deep neural networks provide accurate results wide variety different architectures image modelling generation classification retrieval nthe lack dl architectures 3d data due complexity representation 3d data especially using 3d point clouds nconsidered paper one first approaches learn gan type generative models nusing pointnet architecture latent space gan authors obtained rather accurate generative model nthe paper well written results experiments convincing authors provided code github realizing architectures nthus think paper published
paper considers distribution distribution regression mlps authors use energy function based approach test problems showing similar performance distribution distribution alternatives requiring fewer parameters nthis seems nice treatment distribution distribution regression neural networks approach methodological similar using expected likelihood kernels similar performance achieved fewer parameters would enlightening consider accuracy vs runtime instead accuracy vs parameters u2019s really care sense problem considered several times slightly different model classes really ought pretty strong empirical investigation discussion says u201cfor future work possible study investigate classes problems drn solve u201d feels like present work investigation classes problems drn solve practical utility questionable u2019s clear much value adding yet another distribution distribution regression approach time neural networks without pretty strong motivation seems lacking well experiments introduction would also improve paper outline clear points methodological novelty
authors present evolution idea fast weights training double recurrent neural network one slow trained usual one fast gets updated every time step based slow network authors generalize idea nice way present results experiment positive side paper clearly written fast weights new details presented method original negative side experimental results presented experiment data set task made authors results good improvements large measured weak baselines implemented authors convincing result one would require evaluation number tasks including long studied ones like language modeling comparison stronger related models neural turing machine transformer attention need without comparison stronger baselines results task constructed authors recommend rejection
paper proposed new approach feature upsampling called pixel deconvolution aims resolve checkboard artifact conventional deconvolution sequentially applying series decomposed convolutions proposed method explicitly enforces model consider relation pixels thus effectively improve deconvolution network increased computational cost extent noverall paper clearly written easy understand main motivation methods however checkboard artifact well known problem deconvolution network addressed several approaches simpler proposed pixel deconvolution example well known simple bilinear interpolation optionally followed convolutions effectively removes checkboard artifact extent bilinear additive upsampling proposed wonja et al 2017 also demonstrated effectiveness alternative deconvolution comparisons approaches would make paper stronger besides comparisons discussions based extensive analysis various deconvolution architectures presented wonja et al 2017 would also interesting nwonja et al devil decoder bmvc 2017
motivated via talor approximation residual network local minima paper proposed warp operator replace block consecutive number residual layers number parameters original residual network new operator property computation parallelized demonstrated paper improves training time multi gpu parallelization maintaining similar performance cifar 10 cifar 100 none thing currently clear rotational symmetry paper mentioned rotated filters continue talk rotation sense orthogonal matrix applying weight matrix convolution layer rotation filters 2d images images depth seem quite different rotating general dim vectors abstract euclidean space would helpful make description explicit clear
paper motivated fact gan training beneficial constrain lipschitz continuity discriminator authors observe product spectral norm gradients per layer serves good approximation overall lipschitz continuity entire discriminating network propose gradient based methods optimize spectrally normalized objective ni think methodology presented paper neat experimental results encouraging however comments presentation paper n1 using power method approximate matrix largest singular value old idea think authors cite classical references addition yoshida miyato example nmatrix analysis book bhatia nmatrix computation book golub van loan nsome recent work theory noisy power method might also helpful cited example nhttps arxiv org abs 1311 2495 n2 think matrix spectral norm really differentiable hence gradients authors calculate paper really subgradients please clarify n3 noted even product gradient norm resulting normalizer still upper bound actual lipschitz constant discriminator authors give empirical evidence showing approximation much better previous approximations l2 norms gradient rows appear much easier optimize
well written nicely structured paper tackles problem generating inferring code given incomplete description sketch task achieved novel contribution existing machine learning approaches automated programming achieved training large corpus android apps combination proposed technique leveraging real data substantial strength work compared many approaches come previously nthis paper many strengths n1 writing clear paper well motivated n2 proposed algorithm described excellent detail essential reproducibility n3 stated previously approach validated large number real android projects n4 fact language generated non trivial java like substantial plus n5 good discussion limitations noverall paper valuable addition empirical software engineering community nice break traditional approaches learning abstract syntax trees
authors introduce concept angle bias angle weight vector input vector resultant pre activation wx biased non zero non zero theorm article angle bias results almost constant activation independent input sample resulting weight updates error reduction authors chose add additional optimization constraint lcw achieve zero mean pre activation mentioned article methods like batch normalization bn tend push unit std nclearly lack scaling factor incase lcw like bn doesnot perform well used relu using sigmoid activation bouded seems compensate lack scaling input bn explicitly makes activation zero mean lcw seems achieve constraint weight features though shown computationally less expensive lcw seems work specific cases unlike bn
authors discuss regularized objective function minimized standard sgd context neural nets provide variational inference perspective using fokker planck equation note objective different desired loss function sgd noise matrix low rank evidenced experiments noverall paper written quite well authors good job explaining thesis however unable identify real novelty theory fokker planck equation widely used analysis stochastic noise mcmc samplers recent years paper mostly rephrases results also fact sgd theory works isotropic noise well known divergence true loss function case low rank noise obvious thus found section reformulation known results including theorem proof nsame goes section symmetric anti symmetric split common technique used stochastic mcmc literature last years find new insight manipulations fokker planck equation paper nthus think although paper written well theory mostly recycled empirical results section known thus acceptance threshold due lack novelty
paper presents experimental study behavior units neural networks particular authors aim show units behave binary classifiers training testing ni found paper unnecessarily longer suggested pages focus paper confusing introduction discusses works cnn model interpretability rest paper focused showing unit behaves consistently binary classifier without analyzing anything relation interpretability think formal formulation specific examples relevance partial derivative loss respect activation unit help understand better main idea paper also quantitative figures would useful get big picture example figures authors show behavior specific units examples would nice see graph showing quantitatively behavior units layer would also useful see comparison different cnns see observation holds less depending performance network
main idea paper automate construction adversarial reading comprehension problems spirit jia liang emnlp 2017 work distractor sentence manually added passage superficially logically support incorrect answer shown distractor sentences largely fool existing reading comprehension systems although fool human readers nthis paper replaces manual addition distractor sentence single word replacement narrator trained adversarially select replacement fool question answering system idea seems interesting difficult evaluate adversarial word replacement fact destroy factual information needed answer question control performance question answering system presence adversarial narrator unclear significance empirical results paper difficult interpret comparisons previous work given perhaps given na better model would addition distractor sentence preserves information original passage language model could probably used generate compelling distractor want corrupted passage correct answer uncorrupted passage difficult guarantee trained narrator could learn actually change correct answer
paper low precision training convnets proposed dynamic fixed point scheme shares exponent part tensor developed procedures nn computing format proposed method shown achieve matching performance fp32 counter parts number training iterations several state art convnets architectures imagenet 1k according paper first time kind performance demonstrated limited precision training npotential improvements please define terms like fprop wtgrad first occurance reference please include wallclock time actual overall memory consumption comparisons proposed methods methods well baseline default fp32 training
paper presents sensitivity penalized loss loss classifier additional term squart gradient classifier perturbations inputs minimax maximin driven algorithm find attacks defenses lemma claims minimax maximin solutions provide best worst case defense attack models respectively without proof although statement supported experimentally prior work seem adequately cited compared really knowledgeable adversarial attacks subdomain experiments small limited datasets mnist cifar 10 confidence intervals different initializations instance would nice addition table exact alternating optimization could considered one evaluation impact sensitivy loss vs minimax maximin algorithm paper hard follow times probably dealing point would help regard lemma experimental analysis unclear figures alternative optimization minimax converged fully sets hyperparameters optimal paper presents game formulation learning based attacks defense context adversarial examples neural networks empirical findings support claims nnitpicks nthe gradient descent gradient descent gradient descent algorithm nseeming seemingly narbitrary flexible arbitrarily flexible ncan name gradient descent maximizes gradient ascent nthe mini max maximin solution defined defined nis follow follower
paper falls far short standard expected iclr submission nthe paper little content large sections blank page throughout algorithm iterative temporal differencing introduced figure formal description experiments performed mnist subfigures labeled paper uses acronyms sentences like u201cin figure vbp vbp fba itd using fba vbp u2026 u201d painful read
paper proposes use start end rank measure long term dependency rnns shows deep rnn signficantly better shallow one metric nthe theory part seems technical enough interesting though checked details main concern paper sure whether rac studied paper realistic enough practice certain gating rnn useful know whether one train reasonable rnn multiplicative gates paper much stronger experiments along line
paper examines problem optimizing deep networks hard threshold units significant topic implications quantization computational efficiency well exploring space learning algorithms deep networks none contributions especially novel analysis clear well organized authors nice job connecting analysis work
paper introduces modifications word2vec glove loss functions incorporate affect lexica facilitate learning affect sensitive word embeddings resulting word embeddings evaluated number standard tasks including word similarity outlier prediction sentiment detection also new task formality frustration politeness detection na considerable amount prior work investigated reformulating unsupervised word embedding objectives incorporate external resources improving representation learning methodologies kiela et al 2015 bollegala et al 2016 similar proposed work main originality seems captured algorithm computes strength two words unlike prior work real valued instead binary quantity modification particularly novel believe paper primarily judged based upon effectiveness method rather specifics underlying techniques light performance relative baselines particularly important results reported tables see compelling evidence vad consistently lead significant performance increases relative baseline methods therefore recommend paper publication
paper compares recently proposed method validation properties nof piece wise linear neural networks claims propose novel method nthe unfortunately proposed branch bound method explain nhow implement bound part compute lower bound used nseveral times application incl nruediger ehlers planet https github com progirep planet nchih hong cheng georg nuhrenberg harald ruess maximum resilience artificial neural networks automated technology verification analysis nalessio lomuscio lalit maganti approach reachability analysis feed forward relu neural networks arxiv 1706 07351 nspecifically authors say experiments use result nminimising variable corresponding output network subject nto constraints linear approximation introduced ehlers 2017a nwhich sounds bit like using linear programming relaxations nthe approaches using branch bound cited use case nthe paper original contribution case nthe authors may contribution make made npaper explain lower bound computation one nbased lps ngenerally find jarring mis fit motivation deep learning nfor driving presumably involving millions billions parameters nthe actual reach methods proposed hundreds parameters nthis reach inherent integer programming per se modern solvers nroutinely solve instances tens millions non zeros constraint nmatrix require strong relaxation authors may hence consider nimproving lp relaxation noting big constraint notorious nfor producing weak relaxations
paper studies active learning convolutional neural networks authors formulate active learning problem core set selection present novel strategy nexperiments performed three datasets validate effectiveness proposed method comparing baselines ntheoretical analysis presented show performance selected subset using geometry data points nauthors suggested perform experiments datasets make results convincing nthe initialization cnn model clearly introduced however may affect performance significantly
paper active learning meets challenging multitask domain reinforcement learning diverse atari 2600 games state art deep reinforcement learning algorithm a3c used together three active learning strategies master multitask problem sets increasing size far beyond previously reported works nalthough choice problem domain particular atari reinforcement learning empirical observations especially difficulty learning many different policies together go far beyond problem instantiations paper naive multitask learning deep neural networks fails many practical cases covered paper one concern perhaps choice distinct atari games multitask learn may almost adversarial since naive multitask learning struggles case practice observed interference appear even less visually diverse inputs nalthough performance still reduced compared single task learning cases paper delivers important reference point future work towards achieving generalist agents master diverse tasks represent complementary behaviours compactly scale ni wonder efficient approach would dm lab tasks much similar visual inputs optimal behaviours still distinct
paper interprets deep residual network dynamic system proposes novel training algorithm train constructive way three image classification datasets proposed algorithm speeds training process without sacrificing accuracy paper interesting easy follow ni several comments n1 tit would interesting see comparison stochastic depth also able speed training process gives better generalization performance moreover possible combine proposed method stochastic depth obtain improved efficiency n2 tthe mollifying networks related proposed method also starts shorter networks ends deeper models would interesting see comparison discussion gulcehre mollifying networks 2016 n3 tcould show curves figure another plot training short resnet depth starting model deep resnet depth final model without using approach
paper proposes weak synchronization approach synchronous sgd goal improving even slow parameter servers improvement earlier proposals revisiting synchronous sgd allow slow workers empirical results resnet50 cifar show promising results simulations slow workers servers proposed approach nissues paper since paper focused empirical results results resnet50 cifar limiting empirical results based simulations real workloads choice simulation constants delayed delay time seems somewhat arbitrary well simulated results comparisons seem unfair since validation error different useful also provide time certain accuracy get validation error 1609 reached important cases noverall paper proposes interesting improvement area synchronous training however unable validate impact proposal
paper addresses problem one class classification authors suggest techniques learn classify samples negative class based tweaking gan learning process explore large areas input space objective class nthe suggested techniques nice show promising results feel lot still done justify even one instance authors manipulate objective using new parameter alpha_new divide heuristically range values experimental section results shown single value alpha_new authors also suggest early stopping far understand single value number iterations tested nthe writing paper also unclear several repetitions many typos first introduce architexture future work remain self ni believe lot potential approach es presented paper view much stronger experimental section together clearer presentation discussion could overcome lack theoretical discussion
paper analyzes expressivity convolutional arithmetic circuits convacs neighboring neurons single layer overlapping receptive fields compare expressivity overlapping networks non overlapping networks paper employs grid tensors computed output convacs grid tensors matricized ranks resultant matrices compared paper obtains lower bound rank resultant grid tensors uses show exponentially large number non overlapping convacs required approximate grid tensor overlapping convacs assuming result carries convnets find result interesting overlapped convolutional layers almost universally used little theoretical justification paper shows overlapped convacs exponentially powerful non overlapping counterparts
paper proposed extension fast weights ba et al include additional gating units changing fast weights learning rate adaptively authors empirically demonstrated gated fast weights outperforms baseline methods associative retrieval task ncomment found paper hard follow authors could improve clarity paper greatly listing contribution clearly readers digest authors emphasize first half method section existing works go separate background section overall contribution paper seems modification ba et al eq authors evaluated method synthetic associative retrieval task without additional experiments datasets hard reader draw meaningful conclusion proposed method general
paper clearly written good coverage previous relevant literature nthe contribution slightly incremental several different parameterization orthogonal almost orthogonal weight matrices rnn introduced ntherefore paper must show new method performs better way compared previous methods show proposed method competitive several datasets clear winner one task mse timit npros n1 new relatively simple method learning orthogonal weight matrices rnn n2 clearly written n3 quite good results several relevant tasks ncons n1 technical novelty somewhat limited n2 experiments evaluate run time memory use computational complexity stability therefore difficult make comparisons perhaps restricted capacity urnn 10 times faster proposed method
summary nthe major contribution paper generalization lambda returns called confidence based autodidactic returns car wherein rl agent learns weighting step returns end end manner cars used a3c algorithm weights based confidence value function step return even though idea new authors propose simple robust approach using value function estimation network a3c nduring experiments autodidactic returns perform better half time compared lambda returns ncomments nthe step returns td error written correctly nin figure obvious confidence values estimated nfigure unreadable na lighter version algorithm appendix moved text since novelty paper
paper proposes approach train generators within gan framework setting one access degraded imperfect measurements real samples rather samples broadly approach generator produce full real data pass simulated model measurement process train discriminator distinguish simulated measurements generated samples true measurements real samples mechanism proposed method able train gans generate high quality samples imperfect measurements nthe paper largely well written well motivated overall setup interesting find authors practical use cases convincing one access imperfect data first place empirical results convincing theoretical proofs make strong assumptions particular fact true distribution must uniquely constrained marginal along measurement however theoretical analysis gans neural networks general view proofs means gaining intuition rather strong guarantees end found analysis paper informative ni would make suggestions possible experimental analysis would nice see robust approach systematic mismatches true modeled measurement functions instance slight differences blur kernels noise variance etc especially kind settings paper considers imagine may sometimes also hard accurately model measurement function device may necessary use computationally cheaper approximation training think study mismatches affect training procedure would instructive perhaps quantitative evaluation given best approximately measure sample quality
paper develop theory study impact stochastic gradient noise sgd especially deep neural network models shown gradient noise isotropic normal sgd converges distribution tilted original objective function however gradient noise non isotropic normal shown common many models especially deep neural network models behavior sgd intriguing converge tilted distribution original objective function sometimes interestingly converge limit cycles around critical points original objective function paper also provides hints using sgd get good generalization ability gradient descend ni think finding paper interesting technical details correct still following comments nfirst assumption seems bit abstract easy see assumption means would better example given verified satisfy assumption nanother comment related overall content paper thought paper point sgd equilibrium behavior gradient noise non isotropic normal remains show far away stationary distribution original distribution defined objective function
authors develop novel scheme backpropagating adjacency matrix neural network graph using scheme able provide little bit evidence scheme allows higher test accuracy learning new graph structure couple different example problems npros authors provide empirical evidence benefits using technique authors fairly upfront overall seems technique much null results still results would interesting better understand learning better graph networks help much ncons grammar paper pretty bad could use couple passes editor less entirely empirical paper choices experiments somewhat befuddling considerably details implementation training time test time even experiment domains would paper tremendous amount good mentioned pro also seems technique simply buy much practitioner true learning better graph representations really help much would good know publishable actually establishing requires considerably experiments nultimately suggest rejection unless authors considerably beef manuscript experiments details improve grammar considerably
paper proposes integrating information semantic resource quantifies affect different words text based word embedding algorithm nthe affect lexical seems interesting resource although sure means call state art definitely support endeavour make language models reflective complex semantic pragmatic phenomena affect sentiment nthe justification might want word embeddings manner proposed seems little unconvincing statement delighted disappointed similar contexts evident least participle adjectives affect language seems contextual phenomenon tiny subset words intrinsic context free affect affect seems come use words phrasal extra linguistic contexts context dependent model affect computed phrases sentences would seem appropriate consider words like expensive wicked elimination nthe model proposes several applications sentiment prediction predicting email tone word similarity affect based embeddings yield small improvements however different cases taking different flavours affect information produces best score clear conclude sort information useful nit surprising algorithm uses wordnet running text compute word similarity scores improves one uses running text also surprising adding information affect improves ability predict sentiment tone emails nto understand importance proposed algorithm rather addition additional data would like see comparison various different post processing techniques using wordnet affect lexicon bollelaga et al including much simpler baselines instance averaging wordnet path based distance metrics distance word embedding space word similarity ways applying affect data email tone prediction
paper suggests taking glove word vectors adjust use non euclidean similarity function idea tested small data sets 80 50 examples respectively proposed techniques combination previously published steps new algorithm fails reach state art tiny data sets nit clear authors trying prove whether successfully proven trying prove point glove bad algorithm steps general latter experimental results far weaker would find convincing try multiple different word embeddings happens start random vectors happens try bigger data set complex problem
paper examines distributed deep rl system experiences rather gradients shared parallel workers centralized learner experiences accumulated central replay memory prioritized replay used update policy based diverse experience accumulated workers using system authors able harness much compute learn high quality policies little time results convincingly show ape far outperforms competing algorithms recently published rainbow nit u2019s hard take issue paper overwhelmingly convincing experimental results however couple additional experiments would quite nice u2022 tin order understand best way training distributed rl agent would nice see side side comparison systems distributed gradient sharing gorila versus experience sharing ape u2022 tit would interesting get sense ape performs function number frames seen rather wall clock time example table ape 200m frames better rainbow 200m frames npros u2022 twell written clear u2022 tvery impressive results u2022 tit u2019s remarkable ape preforms well given simplicity algorithm ncons u2022 thard replicate experiments without deep computational pockets deepmind
paper proposes compositional nearest neighbors approach image synthesis including results several conditional image generation datasets npros simple approach based nearest neighbors likely easier train compared gans scales high resolution images ncons requires potentially costly search procedure generate images seems require relevant objects textures present training set order succeed given conditional image generation task
paper proposes deep learning framework predict somatic mutations extremely low frequencies occurs detecting tumor cell free dna key innovation convolutional architecture represents invariance around target base method validated simulations well cfdna nhown provide increased precision competing methods nwhile method interest recent mutation callers compared example snooper uses randomforest https bmcgenomics biomedcentral com articles 10 1186 s12864 016 3281 hence would interest another machine learning framework also compare strelka whic nh interestingly included make final calls mutations comparison nfurther would also liked see use standard benchmark datasets mutation calling https www nature com articles ncomms10001 nit appears proposed method kittyhawk steep decrease ppv enrichment low tumor fraction presumably parameter greatest interest authors explore behavior greater detail
authors consider method trace back 1998 may longer history learning learning rate first order algorithm time underlying model optimized using stochastic multiplicative update basic observation sgd theta_ theta_t alpha nabla theta_t partial partial alpha theta_ nabla theta_t nabla theta_ negative inner product two successive stochastic gradients equal expectation derivative tth update learning rate alpha ni seen sgd authors claim basic idea novel believe application algorithms authors explicitly consider nesterov momentum adam novel use multiplicative normalized update equation particularly normalization nthe experiments well presented appear convincingly show benefit figure explores robustness algorithms choice alpha_0 beta particularly nicely done addresses natural criticism approach replaces one hyperparameter two nthe authors highlight theoretical convergence guarantees important future work item lack aside theorem shows asymptotic convergence learning rates become sufficiently small weakness think critical one appears promising approach bringing back attention machine learning community valuable
author proposed use deep bidirectional recurrent neural network estimate auction price license plates based sequence letters digits method uses learnable character embedding transform data end end approach analysis squared error price regression shows clear advantage method previous models used hand crafted features nhere concerns n1 price shows high skewness fig may make sense use relative difference instead absolute difference predicted actual auction price evaluating training model making error 100 plate priced 1000 huge difference meaning plate priced 10 000 n2 time series data seems temporal trend makes retraining beneficial suggested authors section evaluation setting dividing data three random sets training validation test seem right appropriate choice however divided sets corresponding non overlapping time intervals avoid model use temporal information making prediction
hate say current version paper ready poorly written authors present observations weaknesses existing vector space models list step approach refining existing word vectors glove work test refined vectors 80 toefl questions 50 esl questions addition incoherent presentation proposed method lacks proper justification given small size datasets also unclear generalizable approach npros experimental study retrofitting existing word vectors esl toefl lexical similarity datasets ncons u000b paper poorly written proposed methods well justified results tiny datasets
mars suggested combine multiple adversaries different roles nexperiments show suited create censoring representations increased anonymisation data context wearables nexperiments satisfying show good performance compared methods nit could made clearer significance tested given frequent usage term nthe idea slightly novel framework otherwise state art nthe paper well written use proof reading nreferencing okay
paper introduces neural network architecture continual learning model inspired current knowledge long term memory consolidation mechanisms humans consequence uses tone temporary memory storage inspired hippocampus long term memory ta notion memory replay implemented generative models vae order simultaneously train network different tasks avoid catastrophic forgetting previously learnt tasks noverall although result surprising approach well justified extensively tested provides insights challenges benefits replay based memory consolidation ncomments n1 tthe results somewhat unsurprising able learn generative models tasks use train tasks time beat algorithms use replay approach n2 tit unclear whether approach provides benefit particular application task information available training separate task specific architectures using classical multitask learning approaches would suffer catastrophic forgetting perform better assume n3 tso main benefit approach seems point towards direction possibly happens real brains interesting see authors address practical issues training based replay show two differences real brains know episodic memory consolidation system modeled paper closer unsupervised learning consequence information task id dictionary balancing samples would available cortex long term memory already learns wakefulness proposed algorithm procedure restricted replay based learning sleep n4 tdue differences view work avoids addressing directly critical difficult issues catastrophic forgetting relates finding optimal plasticity rules network unsupervised setting n5 tthe writing could concise authors could make effort stay closer recommended number pages
authors propose reducing number parameters learned deep network setting sparse connection weights classification layers numerical experiments show sparse networks similar performance fully connected ones introduce concept u201cscatter u201d correlates network performance although found results useful potentially promising find much insight paper nit clear scatter way defined paper would useful performance proxy anywhere first classification layer signals different windows intermixed even define windows nminor nsecond line section u201clesser u201d less fewer
apologies short review got called late marking review educated guess since time detailed review nthe paper proposes algorithm tune momentum learning rate sgd algorithm theory general non quadratic functions experimental validation extensive making worthy contribution opinion personally tried algorithm paper came vouch empirical results presented
paper provides survey attribute aware collaborative filtering particular classifies existing methods four different categories according representation interactions users items attributes furthermore authors also provide probabilistic interpretation models addition preliminary experiments comparing among different categories also provided nthere existed several works also provide surveys attribute aware collaborative filtering hence contribution paper limited although authors claim two differences work existing ones particular advantages disadvantages different categories systematically compared hence readers get insightful comments suggestions survey nin general survey papers suitable publication conferences
paper studies global convergence policy gradient methods linear control problems topic paper seems minimal connection icrl might appropriate paper reviewed control optimization conference technical analysis evaluated carefully convinced main results novel convergence policy gradient rely convexity loss function known community control dynamic programming convergence policy gradient related convergence actor critic essentially form policy iteration sure good idea examine convergence purely optimization perspective main results paper seem technical sound however results seem bit limited apply neural network function approximator apply general control problem rather quadratic cost function quite restricted might missed something strongly suggest results submitted suitable venue
paper presents approach improving variational autoencoders structured data provide output syntactically valid semantically reasonable idea presented seems merit however found presentation lacking many sentences poorly written making paper hard read especially familiar presented methods experimental section could organized better like two types experiment presented parallel finally paper stops abruptly without final discussion conclusion
paper considers problem self normalizing models kind nof approaches nce noise contrastive estimation npromising important provide efficient large vocabulary nlanguage models nby interpreting nce terms matrix factorization allows nauthors better explain learning criterion nspecifically self normalizing mechanism nhowever first theoritical contribution make link nbetween matrix decomposition sampling based objective nalready shown negative sampling paper melamud et al nemnlp 2017 therefore nothing new difference slight nmoreover paper cited experimental part ncontribution far emphasized authors nthe second part makes link self normalization nnot really surprising already explained way npapers pihlaja gutmann hyvarinen published 2010 12 see nsome references inproceedings melamud dagan goldberger 2017 emnlp2017 author melamud oren dagan ido goldberger jacob title simple language model based pmi matrix approximations booktitle proceedings 2017 conference empirical methods natural language processing month september year 2017 address copenhagen denmark publisher association computational linguistics pages 1861 1866 abstract study introduce new approach learning language models ttraining estimate word context pointwise mutual information pmi tthen deriving desired conditional probabilities pmi test time tspecifically show minor modifications word2vec algorithm tget principled language models closely related well established tnoise contrastive estimation nce based language models compelling aspect tof approach models trained simple negative tsampling objective function commonly used word2vec learn word tembeddings url https www aclweb org anthology d17 1198 inproceedings pmlr v9 gutmann10a title noise contrastive estimation new estimation principle unnormalized statistical models author michael gutmann aapo hyv u00e4rinen booktitle proceedings thirteenth international conference artificial intelligence statistics pages 297 304 year 2010 editor yee whye teh mike titterington volume series proceedings machine learning research address chia laguna resort sardinia italy month 13 15 may publisher pmlr pdf http proceedings mlr press v9 gutmann10a gutmann10a pdf url http proceedings mlr press v9 gutmann10a html abstract present new estimation principle parameterized statistical models idea perform nonlinear logistic regression discriminate observed data artificially generated noise using model log density function regression nonlinearity show leads consistent convergent estimator parameters analyze asymptotic variance particular method shown directly work unnormalized models models density function integrate one normalization constant estimated like parameter tractable ica model compare method estimation methods used learn unnormalized models including score matching contrastive divergence maximum likelihood normalization constant estimated importance sampling simulations show noise contrastive estimation offers best trade computational statistical efficiency method applied modeling natural images show method successfully estimate large scale two layer model markov random field inbook pihlaja10unnorm tauthor pihlaja gutmann hyv rinen tpages 442 449 tpublisher auai press ttitle family computationally efficient simple estimators unnormalized statistical models tyear 2010
paper deals u201cfixing gans computational level u201d similar sprit gans wgans fix specific restricted relies logistic regression model discriminator dual formulation logistic regression jaakkola haussler ncomments n1 experiments performed restricting alternatives also use linear classifier discriminator mentioned results expected lower produced methods multi layer classifier discriminator shen et al wasserstein distance guided representation learning domain adaptation ganin et al domain adversarial training neural networks n2 considering unsupervised domain adaption problem set hyper parameters lambda kernel width u201creverse validation u201d method described ganin et al domain adversarial training neural networks jmlr 2016 might helpful nminor comments upper bound distance alpha_i instead alpha top please label axes figures
paper presents application measure dependence input power spectrum frequency response filter spectral density ratio shajarisales et al 2015 cascades two filters successive layers deep convolutional networks authors apply newly defined measure dcgans plain vaes relus show dependency successive layers may lead bad performance nthe paper proposed possibly interesting approach found quite hard follow especially section thought quite unstructured also section could improved simplified would also good add related work u2019m expert assume must similar idea cnns nfrom limited point view seems like sound novel potentially useful application interesting idea writing improved think paper may even impact nsmaller details spacing issues extra punctuation pg u201c hence u201d typo pg u201ctraining vae lead values satisfactory obtained gan u201d
paper gives sufficient necessary conditions global optimality loss function deep linear neural networks paper extension kawaguchi 16 also provides sufficient conditions non linear cases ni think main technical concerns paper technique applies linear model sound techniques much beyond kawaguchi 16 happy see papers linear models would expect conceptual technical ingredients far see technique fail non linear models reason kawaguchi technique also think interesting question might turning landscape results algorithmic result algorithm guarantee converge global minimum trivial deep linear networks lot flat saddle points therefore unclear whether one avoid saddle points
paper proposes new method train dnns quantized weights including quantization constraint proximal quasi newton algorithm simultaneously learns scaling quantized values possibly different positive negative weights nthe paper clearly written proposal well placed context previous methods purpose experiments clearly presented solidly designed nin fact paper somewhat simple extension method proposed hou yao kwok 2017 novelty resides consequently great degree novelty terms proposed method results slightly better previous methods nfinally terms analysis algorithm authors simply invoke theorem hou yao kwok 2017 claims convergence proposed algorithm however shown paper sequence loss function values converges imply sequence weight estimates also converges presence non convex constraint b_j n_l may relevant practical results accurate simply stated algorithm converges without careful analysis
paper presents interesting idea word embeddings combines base vectors generate new word embeddings also adopts interesting multicodebook approach encoding binary embeddings nthe paper presents proposed approach nlp problems shown able significant reduce size increase compression ratio still achieved good accuracy nthe experiments convincing solid overall weakly inclined accept paper
paper presents interesting spectral algorithm multiscale hmm derivation analysis seems correct however well known spectral algorithm robust model mis specification clear whether proposed algorithm useful practice method compare em algorithms neural network based approaches
work evaluates complex valued neural networks complex weights complex activation functions nthe paper acknowledges complex networks new findings previous authors complex networks perform less well real valued alternatives nthe paper reports comparison real valued complex valued neural networks controlling storage capacity interesting discussion controlling capacity terms computational inference nthe paper concludes overall complex valued neural networks perform well expected understand conclusion previous work found complex valued neural networks inferior consistent results reported see support paper claim abstract special architectures make complex networks work better well suited particular data sets nthe empirical results presented table numbers format graphical comparisons would easier understand tables zero make sense classification tasks
paper gives empirical estimation intrinsic dimensionality convolutional neural network vgg19 due simonyan zisserman estimate id authors apply singular value decomposition matrix activation vectors layers network intrinsic dimension determined less standard way rank two consecutive singular values ratio exceeding threshold convolutional layers vgg19 observe sum ids feature map roughly equal id matrix formed concatenating vectors feature maps also observe id drops successive layer nthe authors findings intuitively obvious certainly surprising although thorough careful empirical investigation phenomenon would welcome addition research literature paper yet reach standard first foremost result single neural network constitute enough evidence justify authors conclusions second latter half paper concerned details experimental results without offering insights implications deep learning third paper well presented organized introduction scant notational formulism clear rigorous consistent paper overall lacks polish many grammatical errors noverall feel line research worthwhile stage work yet ready publication
pros nthis great paper enjoyed reading authors lay general method addressing various transfer learning problems transferring across domains tasks unsupervised fashion paper clearly written easy understand even though method combines previous general learning frameworks proposed algorithm learnable clustering objective lco novel fits well framework experimental evaluation performed several benchmark datasets proposed approach outperforms state art specific tasks cases ncons suggestions authors discuss detail limitations approach clear high discrepancy source target domains similarity prediction network fail deal cases better detect deploying method pair wise similarity prediction network become dense deal extreme cases
good paper objective perform robust learning minimize risk distribution p_0 also worst case distribution ball around p_0 nsince min max problem intractable general actually studied relaxation problem possible give non convex dual formulation problem duality parameter large enough functions become convex given initial losses smooth nwhat follows certifiable bounds risk robust learning stochastic optimization ball distributions experiments show performs expected gives good intuition reasons occurs separation lines pushed away samples margin seems increased procedure
work authors suggest use control variate schemes estimating gradient values within reinforcement learning framework authors also introduce specific control variate technique based called stein u2019s identity paper interesting well written ni question consideration useful improving appealing paper believe different monte carlo quasi monte carlo strategies applied order estimate integral expected value eq also suggested work alternatives literature please please discuss cite papers required suggest divide section two subsections first one introducing stein u2019s identity related comments need second one starting theorem title u201cstein control variate u201d please also discuss relationships connections possible applications technique algorithms used bayesian optimization active learning sequential learning instance nm gutmann corander u201cbayesian optimization likelihood free inference simulator based statistical mod els u201d journal machine learning research vol 16 pp 4256 u2013 4302 2015 ng da silva ferreira gamerman u201coptimal design geostatistics preferential sampling u201d bayesian analysis vol 10 pp 711 u2013735 2015 nl martino vicent camps valls automatic emulator optimized look table generation radiative transfer models ieee international geoscience remote sensing symposium igarss 2017 please also discuss dependence algorithm respect starting baseline function phi_0
many language issues rendering text hard understand abstract several convolution graphs architectures definitions let data observation verb plural etc computational section training size 9924 testing 6695 nso part negative impression may pure mis understanding nthe authors say nstill authors clearly utilise basic concepts utilize eigenvector nbasis graph laplacian filtering fourier domain ways nthat seem sensible interpretation whatsoever even allowing nfor mis understanding due grammar clear insight nno theorems empirical evaluation ill defined problem ntime series forecasting relate graphs graph nin time series among multiple time series authors nimplement graph related approaches problem featuring ntime series impression hence possible outcome nrejection
paper consider method weight normalization layers neural network weight matrix maintained normalized helps accuracy however simplest way normalize fully connected layer quadratic adding squares weights taking square root paper proposes fastnorm way implicitly maintain normalized weight matrix using much less computation essentially normalization vector maintained updated separately pros natural method weight normalization efficeintly cons natural simple solution fairly obvious limited experiments
well written appropriately structured well within remit conference nnot much technical novelty found original contributions adequately identified interesting nmy main concern complaint technical application based study unfortunately typical focuses provides detail technical modeling issues ignores medical applicability model results exemplified fact data set hardly described 14 abnormalities pathologies rationale behind choice possible interrelations dependencies never described medical viewpoint medical expert would clue results models could applied practice medical insight could achieve nthe bottom line seems model approach works better guys model approach one left impression experiments could made data problems fields application would changed much
paper develops interesting approach solving multi class classification softmax loss nthe key idea reformulate problem convex minimization double sum structure via simple conjugation trick sgd applied reformulation step samples subset training samples labels appear double sum main contributions paper max idea numerical stability reasons proposing implicit sgd idea nunlike first review see term exact title supposed mean believe explained paper agree second reviewer approach interesting however also agree criticism double sum formulations exist literature comments experiments repeat stress though statement newton paper justified newton method converge globally linear rate cubic regularisation needed global convergence local rate quadratic ni believe paper could warrant acceptance criticism raised reviewer addressed ni apologise short late review got access paper original review deadline
paper presents new idea use pact quantize networks showed improved compression comparable accuracy original network idea interesting novel pact applied compressing networks past results paper also promising showed convincing compression results nthe experiments paper also solid done extensive experiments state art datasets networks results look promising noverall paper descent one limited novelty weak reject
paper clear well written nit incremental modification prior work resnext performs better several experiments selected author comparisons included relative resnext nthis paper gating gates lstms mixture experts etc rather masking perhaps kind block sparsity gates paper depend upon input fixed masking matrices see eq nthe main contribution appears optimisation procedure binary masking tensor procedure justified step minimise loss seems unlikely due sampling authors show procedure always converge would good contrast attempts learn discrete random variables example concrete distribution continuous relaxation continuous random variables maddison et al iclr 2017
parameterized clipping activation pact idea clear extend clipping activation learning clipping parameter pact combined quantizing activations nthe proposed technique sounds performance improvement expected validated experiments nbut sure novelty strong enough iclr paper
paper proposes new neural network based method recommendation nthe main finding paper relatively simple method works recommendation compared methods based neural networks recently proposed nthis contribution bad empirical paper certainly much groundbreaking methodologically though certainly nice know simple scalable method works nthere much detail data industrial paper would certainly helpful know well proposed method performs standard recommender systems benchmark datasets compared baselines order get sense whether improvement actually due better model versus due unique attributes particular industrial dataset consideration little concerned may method happens work well types data authors considering may work elsewhere nother nice see evaluation real production data nice authors provided enough info method less reproducible slight concern maybe paper would better industry track conference given focused empirical evaluation rather really making much methodological contribution could somewhat alleviated evaluating standard reproducible benchmarks
authors describe method encoding text discrete representation latent space measure propose outperforms alternative gumbel softmax method language modeling nmt nthe proposed method seems effective proposed dsae metric nice though u2019s surprising previous papers used metrics similar normalized reduction log ppl datasets considered experiments also large another plus however overall paper difficult read parse especially since low level details weaved together higher level points throughout often motivated nthe major critique would qualitative nature results sections u201cdecipering latent code u201d lesser extent u201cmixed sample beam decoding u201d two sections simply anecdotal although nice stepped reasoning single example considered section quantitative aggregate results needed least straightforward using human evaluation subset examples diverse decoding
work investigates convergence guarantees gradient type policies reinforcement learning continuous control nproblems deterministic randomized case whiling coping non convexity objective found paper suffers many shortcomings must addressed n1 writing organization quite cumbersome improved n2 authors state abstract elsewhere showing model free policy gradient methods globally converge optimal solution misleading true authors show convergence objective iterates sequence rephrased elsewhere n3 important literature convergence descent type methods semialgebraic objectives available discussed
key argument authors present relu bn fact using relu bn skews values resulting non normalized activations although bn paper suggests using bn non linearity many articles using bn non linearity gives normalized activations https github com ducha aiki caffenet benchmark blob master batchnorm md also better overall performance approach using bn non linearity termed standardization layer https arxiv org pdf 1301 4083 pdf encourage authors validate claims simple approach using bn non linearity
think understand gist paper interesting action tilde drawn distribution author also explains detail relation pgq soft learning recent paper expected policy gradient ciosek whiteson seems sound interesting nweakness n1 major weakness throughout paper see algorithm formulation smoothie algorithm major algorithmic contribution paper think major contribution paper algorithmic side instead theoretical representation style highly discouraging brings un necessary readability difficulties n2 sec little bit abbreviated major focus paper guess important novel educational guess guess whole algorithm smoothie suggest moving appendix make major focus narrowed
original value iteration network paper assumed trained near expert trajectories used information learn convolutional transition model could used solve new problem instances effectively without training nthis paper extends work training reinforcement signals rather near expert trajectories making transition model state depdendent scaling larger problem domains propagating reward values navigational goals special way nthe paper fairly clear extensions reasonable however think focus 2d grid based navigation sufficient interest impact true original vin paper worked grid navigation domain also domain fairly different structure believe used gridworld convenient initial test case inherent value making improvements help solve grid worlds better motivating may possible motivate demonstrate methods paper domains however work dynamic environments interesting step would interesting see models learned dynamic environments differed static environments
rather difficult evaluate manuscript large part manuscript reviews various papers active vision domain subsequently proposes directly modeled using friston u2019s free energy principle essentially u201canalogy u201d authors state extends page would argue quite stretch free energy principle essentially blind idea rewards preferable states tasks essentially evaluated terms surprise reduction much different large part cited classic active vision literature authors furthermore introduce simplification setting nothing changes scene saccadic exploration rather unusual active vision problems nthe authors provide detail actual implementation model section depth details required iclr missing comparisons gaze selection models saliency models given nfurthermore manuscript seems suggest simulation results somehow related human vision stated u201cthe model provides apparently realistic saccades cover full range image tend point regions contain class characteristic pixels u201d nbut actual comparisons evaluations provided
paper proposes improvement speed training inference structured prediction energy networks spens replacing inner optimization loop network trained predict outputs nspens energy based structured prediction method final prediction obtained optimizing min_y e_theta f_phi finding label set least energy computed energy function using set computed features f_phi comes neural network key innovation spens representing energy function arbitrary neural network takes features candidate labels outputs value energy inference time optimized gradient descent steps spens trained using maximum margin loss functions final optimization problem max loss argmin_y nthe key idea paper replace minimization energy function min_y neural network trained predict resulting output minimization resulting formulation min max problem training time striking similarity gan min max problem predicting network learns predict labels low energy according computing network high loss energy network learns assign high energy predicted labels higher loss true labels predicting network acts generator predicting network acts discriminator nthe paper explores multiple loss functions techniques train models seem rather finnicky experimental results particularly strong comes improving quality spens essentially test time complexity simple feedforward models accuracy comparable full inference requiring energy based models improved understanding spens potential work justify accepting paper
think title misleading concise results paper linear networks recommend adding linear title changing title u2026 deep linear networks theorems observation nice theorem discussion nature saddle point strict theorem imply global optima reached random initialization regardless theorem deal issues discussion computational implications theorem necessary u2019m bit puzzled theorems useful since results seem computational implications training neural nets insights gain problem knowing result discussion would helpful
paper seems claims n1 certain convnet architectures particularly alexnet vgg many parameters n2 sensible solution leave trunk convnet unchanged randomly sparsify top weight matrices ni two problems claims n1 modern convnet architectures inception resnext squeezenet bottleneck densenets shufflenets large fully connected layers n2 authors reject technique deep compression impractical suspect actually much easier use practice priori know correct level sparsity every level network np3 normalized mean batch norm np3 using l2 weight penalty fully connected baseline may unnecessarily overfitting training data np3 table choice cl junction densities come grid search find optimal level sparsity level np7 trouble following left right front back notation np8 figure decide data points include plots
paper discusses phenomenon fast convergence rate training resnet cyclical learning rates particular setting tries provide explanation phenomenon procedure test happens however find paper high significance proposed method solid publication iclr nthe paper based cyclical learning rates proposed smith 2015 2017 understand offered beyond original papers super convergence occurs special settings hyper parameters resnet therefore concerned general interest deep learning models also authors give conclusive analysis condition may happen nthe explanation cause super convergence perspective transversing loss function topology section rather illustrative best without convincing support arguments feel content paper section observational results lack solid analysis discussion behind observations
paper proposed framework connect solving gan finding saddle point minimax problem nas result primal dual subgradient methods directly introduced calculate saddle point nadditionally idea fill relatviely lacking theoretical results gan wgan also provide new perspective modify gan type models nbut saddle point model reformulation section quite standard limited theoretical analysis theorem nas follows resulting algorithm also standard primal dual method saddle point problem nmost important think advantage considering gan type model saddle point model first order methods designed solve numerical experiments part seems bit weak minst cifar 10 dataset large enough test extensibility large scale cases
main strength paper think theoretical result theorem result quite nice wish authors actually concluded following minor improvement proof actually strengthens result nthe authors ended discussion thm page sec saying sufficiently close one goes back 10 easy see converges one three things happen assuming beta fixed loss selected n1 goes infinity n2 alpha goes n3 goes nthe authors discussed alpha close virtue submodular optimization lower bounds close fact proof shows situation much better nif really concerned making converge willing tolerate increasing computational complexity associated solving submodular problems larger schedule increase time guarantees alpha goes goes zero nthere also remark tends modular lambda small useful nfrom algorithm seems clear authors recognized two useful aspects objective scheduled lambda decrease exponentially increase linearly nit would really nice complete analysis thm1 formal analysis convergence speed lambda scheduled fashion analysis would help practitioners make better choices hyper parameters gamma delta
paper good contribution domain adaptation provided new way looking problem using cluster assumption experimental evaluation thorough shows vada dirt performs really well ni found math bit problematic example l_d involves max operator although understand authors mean think correct way write discuss min max objective probably involve explanation gradient reversal etc speaking grl mentioned replaced grl traditional gan objective actually pretty important discuss detail change symmetric nature domain adversarial training asymmetric nature traditional gan training important authors nthe literature review could also include shrivastava et al bousmalis et al cvpr 2017 latter also mnist mnist experiments
high quality clear paper looking biologically plausible learning algorithms deep neural networks contributions experiments testing dtp algorithm difficult datasets proposing minor modification dtp algorithm output layer testing dtp algorithm locally connected architectures novel contributions one seems incremental context previous work similar algorithms nokland direct feedback alignment provides learning deep neural networks 2016 baldi et al learning machine symmetries deep learning channel 2017
paper authors discuss several gan evaluation metrics nspecifically authors pointed desirable properties gans evaluation metrics satisfy nfor properties raised authors experimentally evaluated whether existing metrics satisfy properties nsection summarizes results concluded kernel mmd nn classifier feature space far recommended metrics used ni think paper tackles interesting important problem metrics preferred evaluating gans nin particular authors showed inception score one popular metric actually preferred several reasons nthe result comparing data distributions distribution generator would preferred choice attained kernel mmd nn classifier seems reasonable nthis would surprising result ultimate goal gan mimicking data distribution nhowever result supported exhaustive experiments making result highly convincing noverall think paper worthy acceptance several gan methods proposed good evaluation metrics needed improvements research field
paper gives elaboration gated attention reader gar adding gates based answer elimination multiple choice reading comprehension found formal presentation model reasonably clear empirical evaluation reasonably compelling nin opinion main weakness paper focus race dataset dataset attracted much attention work reading comprehension moved squad dataset active leader board realize squad explicitly multiple choice challenge answer elimination architecture however seems answer elimination might applied choice initial position possible answer span case competing active leader board would much compelling
paper revisits subject seen revisited empirically since 90s relative performance td monte carlo style methods different values rollout length furthermore paper performs controlled experiments using vizdoom environment investigate effect number environment characteristics reward sparsity perceptual complexity interesting surprising result finite horizon monte carlo performs competitively tasks exception problems terminal states play big role well pong simple gridworld type representations outperforms td approaches many interesting settings really interesting experiment performed suggests case due finite horizon mc easier time learning perceptual representations also show side result reward decomposition dosvitskiy koltun oral presentation iclr 2017 necessary learning good policy vizdoom noverall find paper important furthering understanding fundamental rl algorithms however main concern regarding confounding factor may influenced results q_mc uses multi headed model trained different horizon lengths whereas models seem single prediction head may helped q_mc better perceptual capabilities na couple questions find mention eligibility traces async rl framework used would nice discussion whether choice may affected results
work presents cnn training setup uses half precision implementation get 2x speedup training work clearly presented evaluations seem convincing presented implementations competitive terms accuracy compared fp32 representation expert area contribution seems relevant enough published
paper introduces generative approach 3d point clouds specifically two generative adversarial approaches introduced raw point cloud gan latent space gan gan gan referred paper addition gmm sampling gan decoder approach generation also among experimented variations nthe results look convincing generation experiments paper class specific figure multi class generators figure quantitative results also support visuals none question arises whether point cloud approaches generation valuable compared voxel grid based approaches especially octree based approaches show convincing high resolution shape generation results whereas details seem washed point cloud results presented paper ni would like see comparison experiments voxel based approaches next update paper article tatarchenko2017octree title octree generating networks efficient convolutional architectures high resolution 3d outputs author tatarchenko maxim dosovitskiy alexey brox thomas journal arxiv preprint arxiv 1703 09438 year 2017 nin light authors octree updates score updated expect updates reflected final version paper well
authors paper propose extensions dynamic coattention networks models presented last year iclr first modify architecture answer selection model adding extra coattention layer improve capture dependencies question answer descriptions main modification train dcn model using cross entropy loss f1 score using rl supervision order reward system making partial matching predictions empirical evaluations conducted squad dataset indicates architecture achieves improvement least f1 exact match accuracy comparable systems ablation study clearly shows contribution deep coattention mechanism mixed objective training model performance nthe paper well written ideas presented clearly experiments section provide interesting insights impact rl system training capability model handle long questions answers seems paper significant contribution field question answering systems
paper reviews existing literature attribute based collaborative filtering author categories existing works int four categories nwhile categorization reasonable proposed new work beyond existing approaches new insight discussed survey style paper appropriate iclr
paper develops technique understand nodes neural network important nfor prediction approach develop consists using indian buffet process nto model binary activation matrix number rows equal number examples nthe binary variables estimated taking relaxed version nasymptotic map objective problem one question use nindian buffet process asymptotics feature allocation determine nthe number hidden units selected noverall results warrant complexity method results neat ni tell approach better others nlastly intuitively explain additivity assumption distribution
paper proposes simple problem demonstrate short horizon bias learning rate meta optimization idealized case quadratic function analytical solution offers good way understand step look ahead benefit meta algorithm second part paper seems bit disconnected quadratic function analysis would helpful understand gap gradient based meta optimization best effort given analytical solution unfortunately guideline solution offered paper nin summary idealized model gives good demonstration problem think might interest audiences iclr
paper proposes model structured alignments sentences means comparing two sentences matching latent structures overall paper seems straightforward application model first proposed kim et al 2017 latent tree attention nin section formula looks wrong c_ ijk indicator variables scores span think c_ ijk delta_ ijk summations instead nin section expression alpha_ ij seems assume delta_ ijk dlta_ ij regardless production rule scores transitions seems rather limiting comment nin answer selection nli experiments proposed model beat sota marginally better unstructured decomposable attention rather disappointing nthe plots fig marginals cky charts enlightening marginals help solving nli task nminor comments sec language inherently tree structured debatable page laf 2008 bad formatted reference
authors present complex valued analogues real valued convolution relu batch normalization functions related work section brings uses complex valued computation discrete fourier transforms holographic reduced representations however application seem connect uses simply reimplement existing real valued networks complex valued ntheir contributions n1 formulate complex valued convolution n2 formulate two complex valued alternatives relu compare n3 formulate complex batch normalization whitening operation complex domain n4 formulate complex analogue glorot weight normalization scheme nsince complex valued computation done real valued arithmetic switching complex arithmetic needs compelling use case instance existing algorithm may formulated terms complex values reformulating terms real valued computation may awkward however cases authors address training batch norm relu networks standard datasets already formulated terms real valued arithmetic switching networks complex values seem bring benefit either simplicity classification performance
work exploits causality principle quantify weights successive layers adapt interesting results obtained enforcing independence successive layers generators may lead better performance modularity architectures generally result interesting presentation easy follow however proposed approach experiments convincible enough example hard obtain conclusion independence lead better performance experimental results maybe justifications needed
paper presents nearest neighbor based continuous control policy two algorithms presented nn runs open loop trajectories beginning state nn runs state condition policy retrieves nearest state action tuples state nthe overall algorithm simple implement reasonably well simple control tasks quickly gets overwhelmed higher dimensional stochastic environments similar learning steer winding tracks using semi parametric control policies effectively indirect form tile coding could seen fixed voronoi cell sure idea tried 90s familiar enough literature find quick google search brings reinforcement learning active recognition behaviors chapter nearest neighbor lookup policies https people eecs berkeley edu trevor papers 1997 045 node3 html nalthough believe work done current round rl research using nearest neighbor policies believe paper delves far pushing new ideas even simple adaptive distance metric could provided interesting results nevermind learned metric latent space allow rapid retrainig policy new domains reason think place conference paper iclr would suggest submission workshop might use triggering discussion work area
papers proposes recurrent neural network based model learn temporal evolution probability density function monte carlo method suggested approximating high dimensional integration required multi step ahead prediction nthe approach tested two artificially generated datasets two real world datasets compared standard approaches autoregressive model kalman filter regression lstm nthe paper quite dense quite difficult follow also due complex notation used authors nthe comparison methods week authors compare approach two simple alternatives namely first order autoregressive mode kalman filter sophisticated employed
paper introduces svd parameterization uses mostly controlling spectral norm rnn nmy concerns paper include na paper says method works convolutional neural networks find anything convolution nb theoretical analysis might misleading clearly section title critical points global minimum critical point global minimum theorem phrased nall critical points population risk non singular global minima nc paper run experiments language applications rnn widely used nd might wrong point seems gpu utilization method would poor kind impossible scale large datasets
summary nthe paper deal problem rl proposes non parametric approach maps trajectories optimal policy avoids learning parameterized policies fundamental idea store passed trajectories policy executed nearest neighbor search find closest trajectory executes ncomments nwhat happens agent finds self state close state similar trajectory action required could completely different nnot certain claim standard rl policy learning algorithms make difficult assess difficulty problem nhow execute trajectory actions rl definition stochastic would make unlikely trajectory reproduced exactly
paper presents continuous surrogate ell_0 norm focuses applications regularized empirical regularized minimization proposed continuous relaxation scheme allows gradient based stochastic optimization binary discrete variables reparameterization trick extends original binary concrete distribution allowing parameter taking values exact zeros ones additional stretching thresholding operations compound construction sparsity proposed approach easily incorporate group sparsity sharing supports among grouped variables combined types regularizations magnitude non zero components efficacy proposed method sparsification speedup demonstrated two experiments comparisons baseline methods npros paper clearly written self contained pleasure read based evidence provided procedure seems useful continuous relaxation scheme consider handling optimization spike slab regularization ncons would interesting see induced penalty behaves terms shrinkage comparing ell_0 ell_p choices unclear properties proposed hard concrete distribution closed form density convexity etc authors offer rigorous analysis influence base concrete distribution provide guidance choose stretching parameters practice paper would significant
paper extends recurrent weight average rwa ostmeyer cowell 2017 order overcome limitation original method maintaining advantage motivation paper approach taken authors sensible adding discounting applied introduce forget mechanism rwa manipulating attention squash functions nthe proposed method using elman nets base rnn think method applied grus lstms parameters might redundant however assuming kind attention mechanism helpful learning long term dependencies computed efficiently would nice see outcomes combination nis explanation lstms perform badly compared grus rwa rda noverall proposed method seems useful rwa
paper discusses application word prediction software keyboards goal customize predictions user account member specific information adhering strict compute constraints privacy requirements nthe authors propose simple method mixing global model user specific data collecting user specific models averaging form next global model nthe proposal practical however convinced novel enough publication iclr none major question authors assume global model depict general english however necessary population users adhere general english hence averaged model next time step might significantly different general english clear mechanism guarantees fit catastrophic forgetting
paper provides analysis empirical risk landscape general deep neural networks dnns assumptions comparable existing results oversimplifed shallow neural networks main results analyzed correspondence non degenerate stationary points empirical risk population counterparts uniform convergence empirical risk population risk generalization bound based stability theory first developed linear dnns generalized nonlinear dnns sigmoid activations nhere two detailed comments n1 deep linear networks squared loss kawaguchi 2016 shown global optima non degerenate stationary points thus obtained non degerenate stationary deep linear network equivalent linear regression model xw risk bound depends dimensions matrix n2 comparison bartlett maass u2019s bm work bit unfair result holds polynomial activations paper handles linear activations thus authors need refine bm result comparison
paper proposed improved version dynamic coattention networks used question answering tasks specifically aspects improve dcn one use mixed objective combines cross entropy self critical policy learning one imporve dcn deep residual coattention encoder proposed model achieved stoa performance stanford question asnwering dataset several ablation experiments show effectiveness two improvements although dcn improvement dcn think improvement incremental none question since model compicated authors release source code repeat experimental results
nthis paper presented interesting ideas reduce redundancy convolution kernels close existing algorithms tthe sw sc kernel figure extension existing shaped kernel figure tthe cw sc kernel figure similar interleaved group convolutions cw sc kernel regarded redundant version interleaved group convolutions ni would like see discussions relation methods strong arguments convincing reviewers accept paper interleaved group convolutions ting zhang guo jun qi bin xiao jingdong wang iccv 2017 http openaccess thecvf com content_iccv_2017 papers zhang_interleaved_group_convolutions_iccv_2017_paper pdf
paper presents simple useful ideas improving sentence embedding drawing context authors build skip thought model sentence predicted conditioned previous sentence posit one obtain information sentence governing sentences document title document sentences based html sentences table contents etc way understand previous sentence like skipthought provides local discourse context sentence whereas governing sentences provide semantic global context nhere pros paper n1 useful contribution terms using broader context embedding sentence n2 novel simple trick generating oov words mapping local variables generating variables n3 outperforms skipthought evals ncons n1 coreference eval details provided data annotated coreference task crucial understanding reliability evaluation new domain coreference also authors make dataset available replicability also authors used embedding eval standard coreference datasets like ontonotes please clarify n2 clear model learns generate specific oov variables authors clarify decoder learns generate words nclarifications n1 section performance skip thought oov trick paper n2 exact heuristic text styles section stated replicability
positive side paper well written problem interesting non negative side limited innovation techniques proposed indeed small variations existing methods
summary nthis paper proposed extension dynamic coattention network dcn deeper residual layers self attention also introduced mixed objective self critical policy learning encourage predictions high word overlap gold answer span resulting dcn model achieved significant improvement dcn nstrengths nthe model mixed objective well motivated clearly explained nnear state art performance squad dataset according squad leaderboard nother questions comments nthe ablation shows improvement em mixed objective interesting mixed objective targets f1 also brings improvement em
contribution authors propose improvement tensor decomposition method decoding spike train relying non negative matrix factorization authors tackle influence baseline activity decomposition main consequence retrieved components necessarily non negative proposed decomposition rely signed activation coefficients experimental validation shows high frequency baseline hz baseline corrected algorithm yields better classification results non corrected version common factorization techniques nthe objective function defined frobenius norm important influence obtained solutions could seen figure proposed method seems provide discriminant factorization nmf one expense sparsity spatial temporal components impeding biological interpretability possible solution add regularization term objective function ensure sparsity factorization
quality nthe authors introduce deep network predictive coding unclear approach improves original predictive coding formulation rao ballard also use hierarchy transformations results seem indicate layers basically performing insight provided kinds filters learned nclarity nin present form hard assess benefits current formulation compared already existing formulations paper checked typos noriginality nthere exist alternative deep predictive coding models https arxiv org abs 1605 08104 work discussed compared nsignificance nit hard see present paper improves classical alternative deep predictive coding results npros nrelevant attempt develop new predictive coding architectures ncons nunclear gained compared existing work
paper explores gan training linear measurement model one assumes underlying state vector directly observed access measurements linear measurement model plus noise paper explores detail several practically useful versions linear measurement model blurring linear projection masking etc establishes identifiability conditions theorems underlying models nthe ambientgan approach advocated paper amounts learning end end differentiable generator discriminator networks operate measurement space experimental results paper show works much better reasonable baselines trying invert measurement model individual training sample followed standard gan training nthe theoretical analysis satisfactory however would great theoretical results paper able associate difficulty inversion process difficulty ambientgan training example condition number linear measurement model high one would expect recovering target real distribution difficult condition theorem step direction showing required number samples correct recovery increases probability missing data would great theorems also came similar quantitative bounds
paper discusses several gradient based attribution methods popular fast computation saliency maps interpreting deep neural networks paper provides several advances epsilon lrp deeplift formulated way calculated using back propagation training gives unified way understanding implementing methods paper points situations methods equivalent paper analyses methods sensitivity identifying single joint regions sensitivity paper proposes new objective function measure joint sensitivity noverall believe paper useful contribution literature solidifies understanding existing methods provides new insight quantitate ways analysing methods especially latter appreciated
authors discuss direct gamma sampling method interpolated samples gans show improvements usual normal sampling celeba mnist cifar svhn datasets nthe method involves nice albeit minor trick chi squared distribution sum z_ dependence dimensionality removed however convinced distribution prime first place eqn samples gaussian approximately orthogonal high dimensions inner product least thus although z_ z_ chi squared gamma think prime exactly gamma general nthe experiments show interpolated samples qualitatively better thorough empirical analysis different dimensionalities would welcome figures add anything story since plot gamma pdfs shows difference constant kl normal case linear noverall think trick needs motivated better experiments improved really show import independence kl thus think paper acceptance threshold
authors propose improve robustness trained neural networks adversarial examples randomly zeroing weights activations empirically authors demonstrate two different task domains one trade accuracy little robustness qualitatively speaking non one hand approach simple implement minimal impact computationally pre trained networks hand find lacking terms theoretical support fact added stochasticity induces certain amount robustness example compare random perturbation say zero mean weights adds stochasticity well work authors give insight regard noverall still recommend acceptance weakly since empirical results may valuable general practitioner paper could strengthened addressing issues well including empirical results nothing else
work objective analyze robustness neural network sort attack nthis measured naturally linking robustness network local lipschitz properties network function approach quite standard learning theory aware original point view within deep learning community nthis estimated obtaining values norm gradient also naturally linked lipschitz properties function backpropagation natural idea
overall idea paper simple interesting via performing variational inference kind online manner one address continual learning deep discriminative generative networks considerations model uncertainty nthe paper written well literature review sufficient comment mainly importance large scale computer vision applications neural networks experiments shallow
paper author propose cnn based solution somatic mutation calling ultra low allele frequencies nthe tackled problem hard task computational biology proposed solution kittyhawk although designed standard ingredients several layers cnn inspired vgg structure seems effective shown datasets nthe paper well written misprints introduction biological background accurate although bit technical broader audience bibliography reasonably complete maybe manuscript part definition accuracy measures may skipped moreover authors suggest proceed along line research improvements ni would suggest expand experimental section real examples strengthen claim noverall rate manuscript top 50 accepted papers
paper proposes deep neural network compression method maintaining accuracy deep models using hyper parameter however compression methods pruning quantization also concern example basic assumption pruning discard subtle parameters little impact feature maps thus accuracy original network preserved therefore novelty proposed method somewhat weak n2 lot new algorithms compressing deep neural networks r1 r2 r3 however paper simple investigation related works r1 cnnpack packing convolutional neural networks frequency domain r2 lcnn lookup based convolutional neural network r3 xnor net imagenet classification using binary convolutional neural networks n3 experiments paper conducted several small datasets mnist cifar 10 necessary employ proposed method benchmark datasets verify effectiveness imagenet
paper proposes replacing layer standard residual convnet set convolutional modules run parallel input model sparse sum outputs modules previous set paper shows marginal improvements image classification datasets cifar imagenet resnext architecture build npros connectivity constrained sparse modules somewhat interesting connectivity learned algorithms similar previously proposed learn binary weights furthermore learning extends large scale image datasets indeed boost classification performance approach shows promise automatically reducing number parameters network ncons overall approach seems incremental improvement previous work resnext datasets used interesting cifar small imagenet essentially solved standpoint computer vision community increasing performance datasets longer meaningful objective modifications add complexity nthe paper well written conceptually simple however feel paper demonstrates neither enough novelty enough performance gain advocate acceptance
paper studies problem one shot shot learning using graph neural network gnn architecture proposed simplified several authors data points form nodes graph edge weights learned using ideas similar message passing algorithms similar kearnes et al gilmer et al method generalizes several existing approaches shot learning including siamese networks prototypical networks matching networks authors also conduct experiments omniglot mini imagenet data sets improving state art nthere typos presentation paper could improved polished would also encourage authors compare work unrelated approaches attentive recurrent comparators shyam et al learning remember rare events approach kaiser et al achieve comparable performance omniglot would also interested seeing whether approach authors used improve real world translation tasks gnmt
paper proposes shallow model approximating stacks resnet layers based mathematical approximations resnet equations experimental insights uses technique train resnet like models half time cifar 10 cifar 100 experiments particularly impressive liked originality paper
paper intends show complex real valued neural network different lead different results similar tasks complex valued network appropriate difficult problems datasets nthe work seems written rush leading big number typos quickly filled experiment tables full zeros valid conclusion real complex valued neural network directly compared using number parameters theoretical aspect least intuition depth detailed understand one better nconcerning novelty paper spirit https arxiv org abs 1705 09792 weaker experiments theoretical justifications valid conclusion
paper focuses imitation learning intentions sampled nfrom multi modal distribution papers encode mode hidden nvariable stochastic neural network suggest stepping around posterior ninference hidden variable generally required ndo efficient maximum likelihood biased importance nsampling estimator lastly incorporate attention large visual inputs nthe unimodal claim distribution without randomness weak distribution ncould replaced normalizing flow use latent variable nin setting makes intuitive sense think multimodality motivates nmoreover really felt like biased importance sampling approach ncompared formal inference scheme see adds value sampling nfrom prior unclear value modern approximate inference nscheme like black box variational inference algorithm stochastic gradient mcmc nhow important using pretrained weights deterministic rnn nfinally also curious much added value get naccess extra rollouts
paper proves weak convergence regularised ot problem kantorovich monge optimal transport problems ni like weak convergence results weak convergence appears overstatement claim approach nearly optimally transports one distribution cf conclusion penalty pay choosing small epsilon seems visible figure also near optimality would refer parameters chosen best possible way see paper however weak convergence results good na better result hinting optimal would guarantee solution regularised ot within epsilon optimal one within epsilon one smaller epsilon possibilities exist one things experimenters would really care price pay regularisation compared unknown unregularized optimum ni also like choice two regularisers wonder whether authors tried make general considering regularisations l2 one approximation entropic one ntypoes n1 kanthorovich kantorovich intro n2 cal eq
reviewer found proposed approach quite compelling empirical validation requires significant improvements n1 include comparison query bagging boosting two best box active learning strategies n2 empirical validation arbitrarily split 14 datasets training testing ones many questions still unanswered would split work well ie cross validate 14 domains happens train 10 13 domains results significantly different nother comments p3 images figure labeled figure p3 typo theis nabe mamitsuksa icml 1998 query learning strategies using boosting bagging
paper proposed procedure assessing performance gans considering key observation using procedure test improve current version gans demonstrated interesting stuff nit easy follow main idea paper paper told difference stories section section based understanding claims new formalization goal gan training using test evaluate success gan algorithms empirically suggested author reform structure ignore unrelated content make clear claims contributions introduction part nregarding experimental part make strong support claims figure showed almost similar plots varieties meanwhile results performed specific model configurations like resnet settings difficult justify whether generalize cases figures notations curvey making people hard compare ntherefore think current version ready published author make stronger consider next venue
paper application paper detecting face disguised however poorly written contribute much terms novelty approach application domain interesting however simply classification problem nthe paper written clearly mistakes equation however contribute much terms novelty new ideas nto make paper better empirical results needed addition would useful investigate particular problem different binary classification problem using cnns nnotes nequation typo
paper proposes new character encoding scheme use character convolutional language models poor quality paper unclear results metric even reported table little significance though may highlight opportunity revisit encoding scheme characters
paper describes problem continual learning non iid nature real life data point catastrophic forgetting phenomena deep learning work defends point view bayesian inference right approach attack problem address difficulties past implementations nthe paper well written problem described neatly conjunction past work proposed algorithm supported experiments work useful addition community nmy main concern focus validity proposed model harder tasks atari experiments kirkpatrick et al 2017 split cifar experiments zenke et al 2017 even though experiments carried paper important fall short justifying major step direction solution continual learning problem
interesting paper provides theoretical support low dimensional vector embeddings computed using lstms simple techniques using tools compressed sensing paper also provides numerical results support theoretical findings paper well presented organized theorem embedding dimension depending may scale poorly respect
authors propose penalization term enforces decorrelation dimensions representation nthey show included additional term cost functions train generic models nthe idea simple seems work presented examples nhowever talk gradient descent using extra term like see derivatives nproposed term depending parameters model depends model hand ngiven expression proposed regulatization nit seems lead non convex optimization problems hard solve comment nmoreover results quantitatively compared non linear generalizations pca ica designed similar goals cited related work section others proved consistent non linear generalizations pca principal polynomial analysis dimensionality reduction via regression follow family introduced book jolliffe principal component analysis nminor points fig conveys much information
authors describe procedure building production recommender system scratch begining formulating recommendation problem label data formation model construction learning use several different evaluation techniques show successful model offline metrics test results etc nmost originality comes integrating time decay purchases learning framework rest presented work less standard npaper may useful practitioners looking implement something like production
paper describes empirical evaluation common metrics evaluate gans inception score mode score kernel mmd wasserstein distance loo accuracy nthe paper well written clear organized easy follow ngiven underlying application image generation authors move pixel representation images using feature representation given pre trained resnet key results comparisons analyzed discriminability mode collapsing dropping robustness transformations efficiency overfitting nalthough work results useful practitioners lacks two aspects first considers single task gans popular second could benefit deeper maybe theoretical analysis questions conclusions could clarified additional experiments sec u2018while reason rms also fails detect overfitting may lack generalization datasets classes contained imagenet dataset u2019
interesting paper exploring gan dynamics using ideas online learning particular pioneering sparring follow regularized leader analysis freund schapire using listed lemma restricting discriminator single layer maximum player plays concave parameter space stabilizes full sequence losses lemma proved allowing proof dynamics convergence nash equilibrium analysis suggests practical heuristic algorithm incorporating two features emerge theory l2 regularization keeping history past models simple queue latter shown quite competitively practice nthis paper merits acceptance theoretical merits alone ftrl analysis convex concave games robust tool theory see also recent sequel syrgkanis et al 2016 fast convergence regularized learning games natural employ gain insight much brittle gan case practical aspects also interesting incorporation added randomness mixed generation strategy area theoretical justifications motivate practical performance gains ideas could clearly developed future work
paper presents proof self normalization nce result low rank matrix approximation low rank approximation normalized conditional probabilities matrix however seems equation authors assume noise distribution unigram model words however one allowed use noise distribution nce convergence quicker distributions close true distribution argument hold general noise distributions assumption borrow easily goldberg levy 2014 proof nin experiments find nce result self normalization inversely correlated perplexity bit surprising paper interesting lacks strong empirical results could stronger could exploit findings improve language modeling strong baseline
problem numerical instability applying sgd soft max minimization motivation would helpful author could made formal statement nsince main contributions two algorithms stable sgd clear one formally say stable formal problem statement necessary discussion around eq helpful intuitive difficult get formal problem use later examine proposed algorithms nthe proposed algorithms variants sgd clear converge faster existing strategies nsome parts text badly written see example following line see paragraph sec since converge sgd ninversely proportional magnitude gradients lacoste julien et al 2012 expect nformulation converge faster nwhich could shed light matter nthe title also misleading using word exact understand correct proposed sgd method solves optimization problem additive error nin summary algorithms novel variants sgd associated claims numerical stability speed convergence vis vis existing methods missing choice word exact also clear
paper presents model encode decode trees distributed representations nthis first attempt encoders decoders however comparative evalution methods nin fact demonstrated possible encode decode trees distributed structures without learning parameters see decoding distributed tree structures distributed tree kernels nthe paper present comparison kinds models
paper proposes idea faster rnn inference via skip rnn state updates ni like idea paper particular design enables calculating number steps skip advance experiments convincing enough first tasks tested simple synthetic tasks plus small scaled task like see idea works larger scale problems computation speed matters also besides number updates reported table think wall clock time inference also reported demonstrate paper trying claim nminor ncite estimating propagating gradients stochastic neurons conditional computation yoshua bengio nicholas leonard aaron courville straight estimator
main concern relevance paper iclr nthis paper much related representation learning user interface nthe paper well organized technical novelty method unclear nfor example existing method proposed method seems mixed section nyou clearly divide existing study work nthe experimental setting also unclear nkss seems need user study nbut catch details user study number users
paper abstracts two recently proposed rnn variants family rnns called linear surrogate rnns satisfy blelloch criteria parallelizable sequential computation authors propose efficient parallel algorithm class rnns produces speedups existing implements quasi rnn sru lstm apart efficiency results paper also contributes comparison model convergence long term dependency task due hochreiter schmidhuber 1997 novel linearized version lstm outperforms traditional lstm long term dependency task raises questions whether rnns lstms truly need nonlinear structure nthe paper written well explanation opposed obfuscation goal linear surrogate rnns important concept useful understand rnn variants today potentially future novel architectures nthe paper provides argument experimental evidence rotation used typically rnns interesting insight worthy discussion claim needs backing large scale experiments real datasets nwhile experiments toy tasks clearly useful paper could significantly improved adding experiments real tasks language modelling
authors extensive tuning parameters several recurrent neural architectures results interesting however corpus authors choose quite small variance estimate quite high suspect whether conclusions could drawn nit would convincing experiments billion word corpus larger datasets least corpus 50 million tokens use significant resources much difficult also really valuable much close real world usage language models less tuning needed larger datasets nfinally better experiments machine translation speech recognition see improvement bleu wer could get
paper proposes deep learning architecture joint learning feature representation target task mapping function sample weighting function specifically method tries discover feature representations invariance different domains minimizing weighted empirical risk distributional shift designs noverall paper well written organized good description related work research background theoretic proofs nthe main contribution idea learning sample weighting function highly important domain shift however stated paper since causal effect intervention conditioned one main interests expected add related analysis experiment section
paper considers special deep learning model shows expectation one unique local minimizer result gradient descent algorithm converges unique solution works address conjecture proposed tian 2017 nwhile clearly written main concern whether model significant enough assumptions v1 v2 reduces difficulty analysis makes model considerably simpler practical setting
paper analyzes expressiveness loss surface deep cnn think paper clearly written interesting insights
summary paper studies series reinforcement learning rl techniques combination recurrent neural networks rnns model synthesise molecules experiments seem extensive using many recently proposed rl methods show sophisticated rl methods less effective simple hill climbing technique ppo perhaps exception noriginality significance nthe conclusion experiments could valuable broader sequence generation synthesis field showing many current rl techniques fail dramatically nthe paper provide theoretical contribution nevertheless good application paper combining comparing different techniques nclarity paper generally well written however expert molecule design might caught trivial errors experimental set
nthe authors present study aims inferring emotional tags provided thumblr users starting images texts captions text processing authors use standard lstm taking input glove vectors words sentence visual information authors use pretrained cnn fine tuning fully connected layer used fuse multimodal information experimental results reported self generated data set nthe contribution rl perspective limited sense authors simply applied standard models predict bunch labels case emotion labels interesting psychological analysis authors present section still think contribution part sentiment psychologically inspired analysis thumbrl data set ni think author statement study leads plausible psychological model emotion well founded also mention learn recognize latent emotional state whereas true psychological studies rely self filled questionnaires comparing questionnaire produced expert psychologist tags provided users social network ambitious parts authors make explicit approximation stressed every part paper
authors proposed compress word embeddings approximate matrix factorization solve problem gumbel soft trick proposed method achieved compression rate 98 sentiment analysis task compression rate 94 machine translation tasks without performance loss nthis paper well written easy follow motivation clear idea simple effective nit would better provide deeper analysis subsection current analysis simple may interesting explain meanings individual components component related certain topic meaningful perform add substract leaned code nit may also interesting provide suitable theoretical analysis relationships svd embedding matrix
paper misses point vaes gans general used idea using vaes encode decode images general input recover generating process created images unlimited source samples use techniques compressing still unclear quality today low attack authors proposing make sense take see significant changes make sense nbut let u2019s assume point used authors propose one person encodes image send latent variable friend foe intercepts way tampers receiver recovers wrong image without knowing sender believes sample tampered sender codes private key would make attack useless think make first attack useless nthe two attacks require foe inserted middle training vae even less doable encoder decoder train remotely train machine cluster controlled manner person would use system train give away decoder keep encoder sending information
main insight paper lstms viewed producing sort sketch tensor representations grams allows authors design matrix maps bag gram embeddings lstm embeddings show result matrix satisfies restricted isometry condition combining results allows argue classification performance based lstm embeddings comparable based bag gram embeddings ni check proof details based knowledge compressed sensing theory results seem plausible think paper nice contribution theoretical analysis lstm word embeddings
authors propose decoupled backpropagation method called continuous propagation interpretation backpropagation continuous differential system layer wise decoupling easily applied distributed training model authors provide convergence proof proposed algorithm also provides empirical experiment results nalthough found proposed method interesting enough investigate thoroughly shame see overall quality paper weak writing requires significant improvement addition overall unclarity exposition sometimes use unexplained abbreviation cpgd cp experiments also weak important information experiment settings missing model parallelized mini batch gradient descent mbgd unfamiliar concept compared sgd needs better defined
paper shows several recently proposed interpretation techniques neural network performing similar processing yield similar results authors show techniques seen product input activations modified gradient local derivative activation function neuron replaced fixed function na second part paper looks whether explanations global local authors propose metric called sensitivity purpose make observations optimality interpretation techniques respect metric linear case behavior explanation properties tested multiple dnn models tested real world datasets results outline resemblance compared methods nin appendix last step proof eq unclear far see variable g_i lrp u2019t defined use eq achieve last could better explained also seems issues ordering indices alternatively describe lower higher layers higher lower layers
paper demonstrates need usage flexible priors latent space alongside current priors used generator network priors indirectly induced data example discussed via empirical diagonal covariance assumption multivariate gaussian experimental results show benefits approach nthe paper provides good read ncomments n1 pag scores differ using full covariance structure diagonal covariances still restrictive n2 results depicted latent space 20 dimensions informative see model holds high dimensional settings data sparse n3 could consider giving discriminator real data etc fig completeness graphical summary
main idea paper replace feedforward summation ny nwhere vectors matrix nby integral int nwhere functions kernel deep neural network integral feedforward called deep function machine nthe motivation along lines functional pca vector obtained discretization function one encounters curse dimensionality one obtains finer finer discretization idea functional pca view function appropriate hilbert space expands appropriate basis way finer discretization increase dimension approximation rather improves resolution nthis paper takes idea applies deep neural networks unfortunately beyond rather obvious approximation results paper get major mileage idea approach amounts change basis therefore resolution invariance surprising experiments results method compared nns trained data directly nns trained dimension reduced version data eg first fixed number pca components unfortunately done suspect case results would similar
paper describes interesting work combination reasons think like workshop track paper nthere much technically new paper least much really understandable text variant ctc explain clearly done motivation nthere also quite misspellings nsince system presented without comparisons alternatives individual components really shed light significance various modeling decisions made limits value nif rejected could perhaps submitted icassp interspeech paper
authors proposed novel rnn model input state update recurrent cells skipped adaptively time steps proposed models learned imposing soft constraint computational budget encourage skipping redundant input time steps experiments paper demonstrated skip rnns outperformed regular lstms grus thee addition pixel mnist video action recognition tasks nstrength experimental results simple skip rnns shown good improvement previous results nweakness although paper shows skip rnn worked well found appropriate baseline lacking comparable baselines believe regular lstm gru whose inputs randomly dropped training experiments main paper toy tasks small lstms thought main selling point method computational gain would make sense show large rnns thousands hidden units going additional experiments appendix find three results shown main paper seem cherry picked good include nlp tasks
paper describes drelu shift version relu drelu shifts relu sigma sigma author runs cifar 10 100 experiments drelu ncomments n1 using expectation explain drelu works well sufficient convincing although drelu u2019s expectation smaller expectation relu u2019t explain drelu better leaky relu elu etc n2 cifar 10 100 saturated dataset convincing drelu perform complex task imagenet object detection etc n3 experiments elu lrelu worse relu suspicious personally tried elu lrelu rrelu inception v3 batch norm better relu noverall u2019t think paper meet iclr u2019s novelty standard although authors present good numbers convincing
paper presents multi task architecture perform multiple tasks across multiple different domains authors design architecture works image captioning image classification machine translation parsing nthe proposed model maintain performance single task models cases show slight improvements main take away paper nthere factually incorrect statement depthwise separable convolutions introduced chollet 2016 section paper also notes depthwise convolutions traced back least 2012
paper describes technique incorporate dialog acts neural conversational agents interesting work existing techniques neural conversational agents essentially mimic data large corpora message response pairs therefore use notion dialog act important type dialog act switching topic often done ensure conversation continue paper describes classifier predicts dialog act next utterance next utterance generated based dialog act paper also describes increase relevance responses length conversations self reinforcement learning also interesting empirical evaluation demonstrates effectiveness approach paper also well written suggestion improvement good work published
paper studies empirical risk deep neural networks results provided section linear networks section nonlinear networks nresults deep linear neural networks puzzling whatever number layers deep linear nn simply matrix multiplication minimizing mse simply linear regression results section results linear regression understand number layers come play nalso never explicitly mentioned paper guess authors make assumption samples x_i y_i drawn given distribution case sure results population risk minimization found linear regression compare results section
good paper makes interesting algorithmic contribution sense joint clustering dimension reduction unsupervised anomaly detection n2 demonstrates clear performance improvement via comprehensive comparison state art methods n3 number gaussian mixtures hyper parameter training process trainable parameter n4 also interesting get insights anecdotal evidence joint learning helps beyond decoupled learning framework kind data points normal anomalous moving apart due joint learning
paper devises sparse kernel rnns urgently needed current gpu deep learning libraries cudnn exploit sparsity presented number works proposed sparsify prune rnns able run devices limited compute power smartphones unfortunately due low level gpu specific nature work would think work better critiqued gpu centric conference another concern experiments provided demonstrate speedups achieved exploiting sparsity contrasted presenting loss accuracy caused introducing sparsity main portion paper may case reducing density speedup fold observation may value accuracy becomes abysmal npros addresses urgent timely issue devising sparse kernels rnns gpus experiments show kernel effectively exploit sparsity utilizing gpu resources well ncons work may better reviewed gpu centric conference experiments main paper show speedups show loss accuracy due sparsity
paper proposes variant hierarchical hidden markov models hmms chains operate different time scales associate spectral estimation procedure computationally efficient nthe model applied artificially generated data high frequency equity data showing promising results nthe proposed model method reasonably original novel nthe paper well written method reasonably well explained would add explanation spectral estimation appendix rather citing rodu et al 2013 nadditional experimental results would make stronger paper nit would great authors could include code implements model
manuscript makes case particular parameterization conditional gans specifically add conditioning information network motivates method examining form log density ratio continuous discrete cases nthis paper empirical work quite strong bringing bare nearly established tools currently evaluating implicit image models ms ssim fid inception scores nwhat bothers mostly hyperparameters stated thank seem optimized candidate method rather baseline particular beta1 adam momentum coefficient seems like bold choice based experience would easier sell hyperparameter search details included separate hyperparameter search conducted candidate control allowing baseline put best foot forward nthe sentence containing assume network model shared puzzled minutes think meant parameterize log density ratio directly including terms belong data distribution explicit access could clearer
paper proposes improving performance large rnns combing techniques model pruning persistent kernels authors propose model pruning optimizations aware persistent implementation nit clear paper relevant iclr audience due emphasize low level optimization little insight learning representations exposition paper also well suited people without systems background although admit mostly using proxy average machine learning researcher instance authors could explain lamport timestamps 1974 citation nmodulo problems relevance expected audience paper well written presents useful improvements performance large rnns work potential impact industrial applications rnns work clearly novel contributions clear well justified using experiments ablations
paper introduces lr net uses reparametrization trick inspired similar component vae although idea reparametrization new applying purpose training binary ternary network sample pre activations instead weights novel experiments see proposed method effective nit seems could things show experiments part example since using multinomial distribution weights makes sense see entropy training epochs also since reparametrization based lyapunov central limit theorem assumes statistical independence visualization least correlation pre activation layer would informative showing histogram nalso literature low precision networks people concerning training time test time computation demand since sampling pre activations instead weights guess approach also able reduce training time complexity order thus calculation train test time computation could highlight advantage approach boldly
authors disclosed identity violated terms double blind reviews npage previous work aly dugan 2017 nalso page full typos hard read
paper presents interesting framework babi qa essentially argument given long paragraph existing approaches end end learning becomes inefficient linear number sentences proposed alternative encode knowledge sentence symbolically grams thus easy index argument makes sense clear one simply index original text additional encode decode mechanism seems introduce unnecessary noise framework include several components techniques latest recent work look pretty sophisticated however dataset generated simulation small set vocabulary value proposed framework practice remains largely unproven npros interesting framework babi qa encoding sentence grams ncons overall justification somewhat unclear approach could engineered special lengthy version babi lacks evaluation using real world data
unsupervised approach proposed build bilingual dictionaries without parallel corpora aligning monolingual word embeddings spaces via adversarial learning nthe paper well written makes rather pleasant read save need toning claims novelty voiced comment ravi knight 2011 simply general nice paper enjoy reading spite text sales pitching times nthere gaps awareness related work sub field bilingual lexicon induction work vulic moens 2016 nthe evaluation part intrinsic would nice see approach applied downstream beyond simplistic task english esperanto translation plenty outlets applying multilingual word embeddings would nice see least instead plethora intrinsic evaluations limited general interest nin view conclude still nice paper vote clear accept hope see minor flaws filtered revision
good application paper quite interesting workshop related deep learning applications physical sciences engineering n2 lacks sufficient machine learning related novelty required relevant main conference n3 design solving inverse problem using deep learning quite novel see nstoecklein et al deep learning flow sculpting insights efficient learning using scientific simulation data scientific reports article number 46368 2017 n4 however paper introduces two different types networks parametrization physical behavior mapping interesting useful surrogate models cfd simulations n5 interesting see impacts physics based knowledge choice network architecture hyper parameters training considerations n6 claiming generalization capability deep networks enough need show much model interpolate extrapolate effects regulariazations regard
authors propose generative method produce images along hierarchy specificity relevant attributes specified left undefined creating abstract generation task npros results demonstrating method ability generate results abstract novel unseen attribute descriptions generally convincing quantitative qualitative results provided paper fairly clear ncons unclear judge diversity qualitatively fig fig could convincing bushy eyebrows difficult attribute judge abstract generation attribute specified clear good results
paper extends loss aware weight binarization scheme ternarization arbitrary bit quantization demonstrate promising performance experiments nreview npros nthis paper formulates weight quantization deep networks optimization problem perspective loss solves problem proximal newton algorithm extend scheme allow use different scaling parameters bit quantization experiments demonstrate proposed scheme outperforms state art methods nthe experiments complete writing good ncons nalthough work seems convincing little bit straight forward derived original binarization scheme hou et al 2017 tenarization bit since analogous extension ideas lin et al 2016b li liu 2016b algorithm section seen additive complementary
authors propose use multiple adversaries random subspaces features adversarial feature learning produce censoring representations show idea effective reducing private information leakage idea alone might signifcant enough contribution idea training multiple adversaries random subspaces similar idea random forests help variance reduction indeed judging large variance accuracy predicting table 1a single adversaries suspect one main advantage current mars method comes variance reduction author also mentioned using high capacity networks adversaries work well practice introduction could also due high model variance high capacity networks definition private information set clear statement experiments section assume subject identity makes train test split described rather odd since overlap subjects train test split need clarifications experimental details judging figure table methods tested effective hiding private information learned representation even though proposed method works better prediction accuracies still high
topic interesting however description paper lacking clarity paper written procedural fashion first third proper mathematical description good diagrams would immensely helped another big issue lack proper validation section even know metric use objectively compare approach versus baseline plenty fields suffering similar problem yet subjective evaluations listening tests speech synthesis given see one example objectively know model produces examples like time one example good none
paper proposes method jointly learns label embedding form class similarity classification model motivation paper makes sense model properly justified learned little reading paper nthere terms proposed objective function also several parameters associated example label temperature z_2 u2019 u2019 parameter alpha second last term etc nfor experiments set parameters used claimed u201cthe method robust experiment simply works without fine tuning u201d agree robust fine tuning free model ideal justified experiment showing experiment different parameters help us understand role component plays perhaps important improving baseline method point especially given goal work beat state art
paper investigates effect time dependencies specific type rnn nthe idea important paper seems sound however sure main result theorem explains effect depth sufficiently main comment nabout deep network case theorem affects bound ranks current statement result seems independent geq think paper quantify effect increase sub comment nnumerical experiments calculating separation rank necessary provide evidence main result simple example make paper convincing
summary significance authors prove expressing simple multivariate monomials variables networks depth require exp many neurons whereas networks depth represent monomials using neurons nthe paper provides simple clear explanation important problem theoretically explaining power deep networks quantifying improvement provided depth ves nexplaining power depth nns fundamental understanding deep learning paper easy follow proofs clearly written theorems provide exponential gaps simple polynomial functions ves n1 main concern paper novelty contribution techniques results paper general lin et al proofs basically difficult see contribution paper terms contributing fundamentally new ideas n2 second concern results apply non linear activation functions sufficiently many non zero derivatives requirements results lin et al n3 finally prop reducing uniform approximations taylor approximations inequality u03b4x u03b4 follow definition taylor approximation ndespite criticisms contend significance problem clean understandable results paper make decent paper iclr
paper extends previously proposed monotonic alignment based attention mechanism considering local soft alignment across features chunk certain window npros paper clearly written proposed method applied several sequence sequence benchmarks paper show effectiveness proposed method comparable full attention better previous hard monotonic assignments ncons terms originality methodology method rather incremental prior study raffel et al shows significant gains terms considering monotonic alignment hori et al advances joint ctc attention based end end speech recognition deep cnn encoder rnn lm interspeech 17 also tries solve issue combining ctc attention based methods paper also discuss method section ncomments eq 16 denominator t_j
paper proposes fine tune last layer keeping others fixed initial end end training viewing last layer learning light kernel theory well actually linear model nsummary evaluation nthere much novelty idea optimizing carefully last layer post training stage treating last layer kernel machine post processing step dates back least decade real contribution would experiments however experimental setup questionable look like care given control overfitting regular training method nmore details nprevious work idea least decade old huang lecun 2006 see review work deep learning using linear support vector machines recently nexperiments nyou also weight norm penalty end end regular training case make sure appropriately separately tuned necessarily value post training otherwise improvements may simply due better regularization one case vs experimental curves suggest interpretation correct
find paper suitable iclr results less direct applications existing optimization techniques provide fundamental new understandings learning representation
paper authors propose method compressing network means weight ternarization network weights ternatization formulated form loss aware quantization originally proposed hou et al 2017 nto reviewer u2019s understanding proposed method regarded extension previous work lab twn main contribution work nwhile proposed method achieved promising results compared competing methods still necessary compare computational complexity one main concerns network compression nit would appreciated discussion results table tells performance quantized networks better full precision network
paper derived upper bound adversarial perturbation neural networks one hidden layer upper bound derived via theorem middle value replace middle value maximum eq replace maximum gradient value locally global maximal value eq leads non convex quadratic program authors convex relaxation similar maxcut upper bound function sdp solved polynomial time nthe main idea using upper bound opposed lower bound reasonable however find limitations weakness proposed method n1 method likely extendable complicated practical networks beyond ones discussed paper ie one hidden layer n2 sdp tractable would still require expensive computation solve exactly n3 relaxation seems bit loose particular step authors replace gradient value global upper bound seems pretty loose
paper proposes automatically recognize domain names malicious benign deep networks convnets rnns trained directly classify character sequence npros nthe paper addresses important application deep networks comparing performance variety different types model architectures nthe tested networks seem perform reasonably well task ncons nthere little novelty proposed method models paper primarily focused comparing existing models new task nthe descriptions different architectures compared overly verbose simple standard convnet rnn architectures code specifying models also excessive main text moved appendix even left code release nthe comparisons various architectures enlightening u2019t done controlled way large number differences pair models u2019s hard tell performance differences come u2019s also difficult compare learning curves among different models fig separate plots differently scaled axes nthe proposed problem explicitly adversarial setting adversarial examples well known issue deep networks models issue addressed analyzed paper fact intro claims advantage using hand engineered features malicious domain detection seemingly ignoring literature adversarial examples deep nets example case attacker could start legitimate domain name use black box adversarial attacks white box attacks given access model weights derive similar domain name models proposed would classify benign nwhile paper addresses important problem current form novelty analysis limited paper presentation issues
paper relatively clear follow implement nthe main concern looks like class project rather scientific paper class project could get ml class nin particular authors take already existing dataset design trivial convolutional neural network report results absolutely nothing interest iclr except fact know trivial network capable obtaining 90 accuracy dataset
authors show two types singularities impede learning deep neural networks elimination singularities unit effectively shut loss input output weights overly strong negative bias overlap singularities two units similar input output weights demonstrate skip connections reduce prevalence singularities thus speed learning nthe analysis thorough authors explore alternative methods reducing singularities explore skip connection properties strongly reduce singularities make observations consistent overarching claims ni major criticisms none suggestion future work would provide procedure users tailor skip connection matrices maximize learning speed efficacy authors could use procedure make highly trainable networks show test training data resultant network leads high performance
paper summarizes compares current explanation techniques deep neural networks rely redistribution relevance contribution values output input space nthe main contributions introduction unified framework expresses common attribution techniques gradient input integrated gradient eps lrp deeplift similar way modified gradient functions definition new evaluation measure sensitivity generalizes earlier defined properties completeness summation delta nthe unified framework helpful since points equivalences methods makes implementation eps lrp deeplift substantially easy modern frameworks however correctly stated authors unification relation lrp gradient input already mentioned prior work nsensitivity measure tries tackle difficulty estimating importance features seen either separately combination measure shows interesting trends towards linear behaviour simpler methods persuade measure well relevance attribution method mimics decision making process really point substantial differences different methods furthermore authors could comment relation sensitivity region perturbation techniques samek et al ieee tnnls 2017 sensitivtiy seems extension region perturbation idea nit would interesting see relation unified gradient based explanation methods approaches saliency maps alpha beta lrp deep taylor deconvolution networks grad cam guided backprop fit unification framework good author mention works still would great see discussion advantages disadvantages methods may nice theoretically properties see discussion gradient vs decompositiion techniques montavon et al digital signal processing 2017 incorporated unified framework
idea using cross task transfer performance task clustering new please refer paper u201cdiscovering structure multiple learning tasks tc algorithm u201d published icml 1996 one issue use cross task transfer performance measure task relations ignores negative correlations tasks useful learning multiple tasks example binary classification tasks small s_ ij indicates changing sign classification function two tasks useful use cross task transfer performance task clustering approach capture positive correlations tasks ignore negative task relations also important sharing among tasks multi task learning nproblem identical robust pca theorem common matrix completion literature u2019t see much novelty appendix seems obvious prove validity assumption made problem based previous works u201cmulti task sparse structure learning gaussian copula models u201d u201clearning sparse task relations multi task learning u201d number tasks large task relation exhibits sparse structure u2019t know whether low rank structure exist cross task transfer performance nthe two parts paper new combination two parts seems bit incremental bring much novelty
paper motivated building robots learn open ended way really interesting actually investigates performance existing image classifiers object detectors could find technical contribution something sufficiently mature interesting presenting iclr nsome issues submission supposed double blind authors reveal identity start section implementation details place section called implementation point concrete idea proposed seems early talking tensorflow keras
paper discusses universal perturbations perturbations mislead trained classifier added input data points main results two fold decision boundary flat linear classifiers classifiers tend vulnerable universal perturbations decision boundaries correlated decision boundary curved vulnerability universal perturbations directly resulted existence shared direction along decision boundary positively curved authors also conducted experiments show deep nets produces decision boundary satisfies curved model nthe main issue applicable insight analysis n1 universal perturbation important topic opposed adversarial perturbation n2 result implies make decision boundary flat curved different directions achieve might mis understanding reading prescriptive procedure universal perturbation seems attained results presented
paper dives deeper understand reward augmented maximum likelihood training overall feel paper hard understand would benefit clarity section states decoding softmax distribution similar bayes decision rule please elaborate ndid compare minimum bayes risk decoding chooses output lowest expected risk amongst set candidates nsection says ranzato et al bahdanau et al require sampling model distribution however methods analyzed paper also require sampling cf appendix mention sample size 10 please explain difference
submitted manuscript describes exercise performance comparison neural language models standardization hyperparameter tuning model selection strategies costs type study important give perspective non standardized performance scores reported across separate publications indeed results interesting favour relatively simpler structures ni favourable impression paper would hope another reviewer familiar specific application domain
nthis paper illustrates method compute produce word embeddings fly rare words using pragmatic combination existing ideas backing separate decoder rare words la luong manning https arxiv org pdf 1604 00788 pdf cited though idea might older using character level models la ling et al using dictionary embeddings la hill et al nnone ideas new u2019t seen combined way practical idea well explained thorough set experiments across three different tasks paper surprising seems like effective technique people want build effective systems whatever data u2019ve got
paper intends interpret well trained multi class classification deep neural network discovering core units one multiple hidden layers prediction making however discovered core units specific particular class retained maintain deep neural network u2019s ability separate particular class ones thus non core units particular class could core units separating another class remaining ones consequently aggregation class specific core units could include hidden units layer therefore hard understand u2019s motivation identify core units one vs remaining manner moment identified class specific core units useful neither reducing size network accelerating computation
authors proposed use implicit weight normalization approach replace explicit weight normalization used training neural networks authors claimed obtain efficiency improvement better numerical stability nthis short paper contains five pages idea proposed implicit weight normalization apply normalization scaling input rather rows matrices terms overall time complexity improvement seems quite limited considering normalization bottleneck operations training addition clear proposed approach benefits mini batch training network terms numerical stability though experimental results reported theoretical analysis experiments quite limited
paper proposes use image representation techniques means learning representations graphs via adjacency matrices adjacency matrix subgraph first ordered produce canonical ordering fed image representation method fed classifier nthis little unprincipled taste particular paper uses caffe reference model top adjacency matrix rather learning method specifically graphs perhaps due lack available graph training data seem make lot sense nmaybe missed overlooked detail spot exactly classification task think goal identify graphs subgraph belongs sure relevant graph classification task nthe method prove caffe reference model maintains information used classification really suggest generalizable method could confidently use variety tasks surprising works ultimately reveal big scientific finding could used
paper introduces neural translation model automatically discovers phrases idea interesting tries marry phrase based statistical machine translation neural methods principled way however clarity paper could improved nthe local reordering layer ability swap inputs however ensure actually swap inputs rather ignoring inputs duplicating others nare segments translated independently carry hidden state decoder rnn segments figure brnn swan layer shown another rnn swan layer brnn emit final outputs segments determined
paper extends previous results differentially private sgd user level differentially private recurrent language models experimentally shows proposed differentially private lstm achieves comparable utility compared non private model nthe idea training differentially private neural network interesting important machine learning differential privacy community work makes pretty significant contribution topic adapts techniques previous work address difficulties training language model providing user level privacy experiment shows good privacy utility nthe presentation paper improved bit example might better preliminary section section2 introducing original differentially private sgd algorithm clipping original fedavg fedsgd moments accountant well privacy amplification otherwise pretty difficult readers familiar concepts fully understand paper introduction also help readers understand difficulty adapting original algorithms appreciate contributions work
summary nthe paper presents three different methods training low precision student network teacher network using knowledge distillation nscheme consists training high precision teacher jointly low precision student scheme traditional knowledge distillation method scheme uses knowledge distillation fine tuning low precision student pretrained high precision mode nreview nthe paper well written experiments clear three different schemes provide good analytical insights nusing scheme student model low precision could achieve accuracy close teacher compressing model ncomments ntensorflow citation missing nconclusion short directions future research would useful
paper presents extensive framework complex valued neural networks related literature suggests variety motivations complex valued neural networks biological evidence richer representation capacity easier optimization faster learning noise robust memory retrieval mechanisms nthe contribution current work lie presenting significantly superior results compared traditional real valued neural networks rather developing extensive framework applying conducting research complex valued neural networks indeed standard work nowadays real valued neural networks depends variety already well established techniques weight initialization regularization activation function convolutions etc work complex equivalent many basics tools developed number complex activation functions complex batch normalization complex convolution discussion complex differentiability strategies complex weight initialization complex equivalent residual neural network nempirical results show new complex flavored neural networks achieve generally comparable performance real valued counterparts variety different tasks major contribution work advancing state art many benchmark tasks constructing solid framework enable stable solid application research well motivated models
authors propose use gamma prior distribution nthe latent representation space gans motivation behind nin gans interpolating sampled points common process generating examples use normal prior results samples fall low probability mass regions use proposed gamma distribution simple alternative overcomes problem nin general proposed work interesting idea neat nthe paper well presented want underline importance nthe authors good job presenting problem motivation solution coherent fashion easy follow nthe work interesting provide useful alternatives distribution latent space
paper summary nthis paper proposes technique generalize deconvolution operations used standard cnn architectures traditional deconvolution operation uses independent filter weights compute output features adjacent pixels work proposes sequential prediction adjacent pixel features via intermediate feature maps resulting spatially smooth outputs deconvolution layer new layer referred u2018pixel deconvolution layer u2019 demonstrated two tasks semantic segmentation face generation npaper strengths despite simple technique proposed pixel deconvolution layer novel interesting experimental results two different tasks demonstrating general use proposed deconvolution layer nmajor weaknesses main weakness paper lies weak experiments although authors say several possibilities exist dependencies intermediate feature maps systematic ablation studies type connectivities work best proposed layer authors experimented two randomly chosen connectivities enough understand type connectivities work best important forms main contribution paper also several quantitative results seem incomplete deeplab resnet performance low quick look pascalvoc results indicate deeplab resnet iou 79 dataset reported numbers paper around 73 iou mention iou base deeplab resnet model standard deeplab crf technique quantitative results image generation nminor weaknesses although paper easy understand several parts paper poorly written several sentences repeated multiple times across paper statements need corrections refinements u201cmean iou accuracy evaluation measure u201d better tone statements changing u201csolving u201d u201ctackling u201d illustration checkerboard artifacts standard deconvolution technique clear example results presented figure indicate segmentation mistakes network rather checkerboard artifacts nclarifications authors choose u2018resize u2019 images training semantic segmentation networks instead generally used u2018cropping u2019 create batches see u2018red u2019 figure see later feature map u2018pinkish u2019 color probably due color vision case better use different color scheme distinguish nsuggestions strongly advice authors ablation studies connectivities make good paper also would great authors revise writing thoroughly make enjoyable read nreview summary nthe proposed technique despite simple novel interesting weak incomplete experiments make yet ready publication
contribution deal nuisance factors afflicting biological cell images domain adaptation approach embedding vectors generated cell images show spurious correlation authors define wasserstein distance network find suitable affine transformation reduces nuisance factor evaluation real dataset yields correct results approach quite general could applied different problems nthe contribution approach could better highlighted early stopping criteria tend favor suboptimal solution indeed relying cramer distance possible improvement nas side note nn moa central evaluation proposed approach possible improvement try means embedding instead euclidean one
paper proposed new parametrization scheme weight matrices neural network based householder reflectors solve gradient vanishing exploding problems training proposed method improved two previous papers n1 stronger expressive power mahammedi et al 2017 n2 faster gradient update vorontsov et al 2017 nthe proposed parametrization scheme natrual numerical linear algebra point view authors good job section explaining corresponding expressive power experimental results also look promising nit would nice authors analyze spectral properties saddle points linear rnn nonlinear better difficult believe authors show strict saddle properties corollary stochastic gradient descent finds global minimum noverall strong paper recommend accept
key contributions paper proposes reduce vocabulary size large sequence sequence mapping tasks translation first mapping standard form correct morphological form achieve clever use character lstm encoder decoder sandwiches bidirectional lstm captures context demonstrate clear substantial performance gains opensubtitle task demonstrate clear substantial performance gains dialog question answer task ntheir analysis section shows one clear advantage model context long sequences nas aside authors correct numbering figures figure provide better captions tables results shown easily understood glance nthe drawback paper advance representation learning per se though nice application current models
paper describes approach decode non autoregressively neural machine translation tasks solved via seq2seq models advantage possibility parallel decoding result significant speed factor 16 experiments described disadvantage complicated standard beam search auto regressive teacher models needed training results reach yet bleu scores standard beam search noverall interesting paper would good see speed accuracy curve plots decoding speed different sized models versus achieved blue score one standard benchmarks like wmt14 en fr en de understand better pros cons proposed approach able compare models speed bleu scores table gives hint clear whether much smaller models standard beam search possibly good fast nat losing bleu points wmt14 significant ro en results good particular language pair used much others would interesting stay single well used language pair benchmark analyze wmt14 en de de en improving finally would good address total computation comparison well seems total decoding time smaller total computation nat npd actually higher depending choice
paper focuses accelerating rnn applying method blelloch 1990 application straightforward thus technical novelty paper limited results impressive none concern proposed technique applied types rnns may limit applications practice could authors comment potential limitation
paper proposed class control variate methods based stein identity stein identity widely used classical statistics recently statistical machine learning literature nevertheless applying stein identity estimating policy gradient novel approach reinforcement learning community approach right way constructing control variates estimating policy gradient authors also good job connecting existing works gave concrete examples gaussian policies experimental results also look promising nit would nice include theoretical analyses like conditions proposed method achieve smaller sample complexity existing works noverall strong paper recommend accept
paper demonstrate freezing penultimate layers end regular training improves generalization however results convince reviewer switch using post training nlearning features use classifier softmax svm new actually widely used 10 years ago however freezing layers continue train last layer minor novelty results paper show generalization gain terms better test time performance however seems like gain could due lambda term added post training added baseline eq eq ntherefore unclear whether gain generalization due additional lambda term post training training na way improve paper convincing would obtain state art results post training possible otherwise nother notes nremark true dropout would change feature function say dropout applied would good support statement experiments nfor table please use decimal points instead commas
paper studies problems solved using dynamic programming approach proposes neural network architecture called divide conquer networks dcn solve problems network two components one component learns split problem learns combine solutions sub problems using setup authors able beat sequence sequence baselines problems amenable approach particular authors test approach computing convex hulls computing minimum cost means clustering euclidean traveling salesman problem tsp problem three cases proposed solution outperforms baselines larger problem instances
paper made efforts smoothing top losses proposed lapin et al 2015 family smooth surrogate loss es proposed help top error may minimized directly properties smooth surrogate losses studied computational algorithms svm losses function also proposed npros n1 paper well presented easy follow n2 contribution made paper sound mathematical analysis seems correct n3 experimental results look convincing ncons nsome statements paper clear example authors mentioned sparse non sparse loss functions statement view could misleading without explanation non sparse loss mentioned abstract
paper discusses agent architecture uses shared representation train multiple tasks different sprite level visual statistics key idea agent learns shared representations tasks different visual statistics lot important references touching similar ideas missing unsupervised pixel level domain adaptation generative adversarial networks using simulation domain adaptation improve efficiency deep robotic grasping schema networks zero shot transfer generative causal model intuitive physics paper lot orthogonal details instance sec reviews history games ai besides key point provide literary context single runs results shown plots statistically valid results last section authors mention intent future work atari env given general idea discussed literature several times seems imperative least scale experiments paper ready publication
interesting read ni feel mismatch intuition model could based structure architecture versus model transition function shared model could learn construct tree trained end end system sufficiently constrained learn specific behaviour point think search tree perspective interesting u2019t deeper model shared weights max operation seems loss used force embeddings produced transition model match embeddings would get take particular action particular state right specific attempt visualize understand embeddings inside tree regarding rewards auxiliary loss attempting force intermediary prediction valid rewards would model use free latent variables encode rewards think pitfall many deep network papers fall laying particular structure directly inferred model discovers follows particular solution latent prescribed semantics would argue rarely case system learned end end structure impose behaviour model authors paper prove trained model anything similar expanding tree showing final performance game indeed model anything similar search intermediary representations correspond semantically nignoring verbose comment another view baseline disadvantaged treeqn less parameters less deep huge impact learnability expressivity deep network
paper authors propose new approach learning underlying structure visually distinct games nthe proposed approach combines convolutional layers processing input images asynchronous advantage actor critic deep reinforcement learning task adversarial approach force embedding representation independent visual representation games nthe network architecture suitably described seems reasonable learn simultaneously similar games visually distinct however authors explain architecture used domain adaptation nindeed games learnt proposed algorithm authors precise modules retrained learn new game critical issue experiments show gain terms performance learn shared embedding manifold see da drl versus baseline figure nif gain learn shared embedding manifold plausible gain evaluated baseline learns separately games algorithm learns incrementally games nmoreover experimental setting games similar simply nmy opinion paper ready publication interesting issues referred future works
well written paper compelling topic train automated teacher use intuitive strategies would also apply humans nthe introduction fairly strong reviewer wishes authors would come intuitive example illustrates strategy train random exs train pick exs makes sense example would dramatically improve paper readability nthe paper appears original related work section quite extensive na second significant improvement would add depth running example section authors could illustrate br strategy makes sense algorithm
main idea paper improve policy policy gradient estimates using control variates based multi step rollouts reduce variance control variates using reparameterization trick laid primarily equations seems like nice idea although must admit trouble following maths equation include results showing method better sample efficiency trpo method also uses hood update value function parameters nmy main issue paper empirical section bit weak instance one run seems shown methods mention hyper parameter selection measure used generating table seems pretty arbitrary thresholds chosen addition one thing would liked get paper better understanding much component helps could done via empirical work instance explore effect planning horizon implicitly compare svg authors point method horizon show effect reparameterization trick estimator variance compare bias variance trpo estimates vs proposed method
paper examines use skip connections including residual layers deep networks way alleviating two perceived difficulties training neuron contain information two neurons layer compute function cases lead singularities hessian matrix work includes number experiments showing effect skip connections hessian training nthis significant timely topic may best one judge originality work appreciated authors presented clear concise arguments experiments back claims
paper proposes improve time complexity factorization machine unfortunately paper claim fm time complexity quadratic feature size wrong specifically dot product computed linear feature size sum x_i beta_i sum x_i beta_i sum_i x_i beta_i beta_i nthe projection feature group one embedded space proposed paper viewed another form representing model group equals one number feature groups equal one correspond field aware factorization machine ffm
authors propose method reducing computational burden performing inference deep neural networks method based previously developed approach called incomplete dot products works pruning inputs dot products via introduction pre specified coefficients authors paper extend method introducing task wise learning procedure sequentially optimizes loss function decreasing percentage included features dot product nunfortunately paper hard follow someone actively work field making hard judge contribution significant description problem adequate comes describing tesla procedure alternative training procedure relevant passages opinion vague allow researchers implement procedure npositive points application seems relevant task wise procedure seems like improvement original idp proposal application two well known benchmarking datasets nnegative points method described sufficient detail allow reproducibility algorithms sketches clear advantage approach opposed alternative ways compressing network via group lasso regularization training emulator full model task nminor point figure unclear requires better caption
paper introduces fairly elaborate model reading comprehension evaluated squad dataset model shown improve published results submission leaderboard numbers nthe main weakness paper opinion innovations seem incremental based overarching insight general principle less significant issue english often disfluent nspecific comments would remove significance daggers table standard deviations already reported null hypothesis significance measured seems unclear also concerned see test performance significantly better development performance table systems seem development test performance closer together authors evaluating many times test data
paper proposes distributed architecture deep reinforcement learning scale specifically focusing adding parallelization actor algorithm prioritized experience replay framework nice introduction literature review prioritized experience replay also suggested parallelize actor algorithm simply adding actors execute parallel experience replay obtain data learner sample learn surprisingly framework able learn way data atari outperforms baselines figure clearly shows actors better performance nwhile strength paper clearly good writing well rigorous experimentation main concern paper novelty opinion somewhat trivial extension previous work prioritized experience replay literature hence challenge work quite clear hence feel adding practical learnings setting infrastructure might add flavor paper example
paper attempts extend predictive coding model multilayer network math developed learning rule demonstrations shown reconstructions cifar 10 images nthe overall idea approach pursued good one model needs development could also use better theoretical motivation sorts representations expect emerge higher layers demonstrate toy example extend real data nthat model reconstruct images per se particularly interesting would like see somehow learned useful meaningful representation data example learned weights look like would tell something learned
paper extends softmax consistency adding relative entropy term entropy regularization applying trust region policy optimization instead gradient descent expert area hard judge significance extension nthe paper largely follows work nachum et al 2017 differences claimed novelty work relative entropy trust region method training however relative entropy term added seems like marginal modification authors claimed satisfies multi step path consistency derivation missing ni bit confused way trust region method used paper initially problem written constrained optimization problem 12 converted penalty form softmax consistency finally lagrange parameter estimated trust region method addition get lagrange parameter epsilon nthe pseudo code algorithm missing would much clearer detailed description algorithmic procedure given nhow performance trust pcl compared pcl
paper investigates effect adversarial training based experiments using cifar10 authors show adversarial training effective protecting shared adversarial perturbation particular universal perturbation contrast less effective protect singular perturbations show singular perturbation less robust image transformation meaning image transformation perturbations longer effective finally show singular perturbations easily detected ni like message conveyed paper however statements mostly backed experiments think makes sense ask statistically significant present results moreover cifar 10 experiments conclusive enough
authors propose using piecewise linear activation functions contraints make continous report training tuning piecewise versions multiple activation functions relu elu lrelu converge shifted elu termed shelu article authors claim achive better performance using shelu learning individual bias shift neuron ngiven prelu learnable alpha elu applied pre activation wx neuron one achieve shift reported shelu required authors present clear explanation shift result improved performance
paper well written clear main idea exploit schema semisupervised learning based acol gar unsupervised learning task idea introduce notion pseudo labelling npseudo labelling obtained transformations original input data nthe key point definition transformations nonly whether design transformation captures latent representation input data pseudo labelling might improve performance unsupervised learning task nsince known advance might good set transformations clear behaviour model large portion transformations encoding latent representation clusters
paper analyzes loss function properties cnns one wide layer layer number neurons greater train sample size additional technique conditions paper shows layer extract linearly independent features critical points local minimums like presentation writing paper however find uneasy fully evaluate merit paper mainly wide layer assumption seems somewhat artificial makes corresponding results somewhat expected mathematical intuition severe overfitting induced wide layer essentially lifts loss surface extremely flat training zero small error becomes easy surprising would interesting make results quantitive quantify tradeoff local minimums nonzero training error
paper titled faster discovery neural architectures searching paths large model authors proposed efficient algorithm used efficient less resources time faster architecture design neural networks motivation new algorithm sharing parameters across child models searching archtecture new algorithm empirically evaluated two datasets cifar 10 penn treeback new algorithm 10 times faster requires 100 resources performance gets worse slightly noverall paper well written although methodology within paper appears incremental previous nas method efficiency got improved quite significantly
