Total sentences to be embedded: 258
{'Recommendation': 6.653, 'Confidence': 3.938, 'Exhaustive': 7.032, 'Aspectual Score': 8.928, 'Intensity': 4.61, 'Actual REC': '4', 'Actual CONF': '5', 'comments': "Summary : This paper presents a way to combine existing factorized second order representations with a codebook style hard assignment . The number of parameters required to produce this encoded representation is shown to be very low . Like other factorized representations , the number of computations as well as the size of any intermediate representations is low . The overall embedding is trained for retrieval using a triplet loss . Results are shown on Stanford online , CUB and Cars-196 datasets . Comments : Review of relevant works seems adequate . The results seem reproducible . The only contribution of this paper is combining the factorized second order representations of ( Kim et . al . 2017 ) with a codebook style assignment ( sec . 3.2 ) . Seems marginal . The scheme described in Sec . 3.2 needs clarification . The assignment is applied to x as h ( x ) \\kron x in ( 7 ) . Then the entire N^2 D^2 dimensional second order descriptor h ( x ) \\kron x \\kron h ( x ) \\kron x is projected on a N^2 D^2 dim w_i . The latter is factorized into p_i , q_i \\in \\mathbb { R } ^ { Nd } , which are further factorized into codebook specific projections u_ { i , j } , v_ { i , j } \\in \\mathbb { R } ^ { d } . Is this different from classical assignment , where x is hard assigned to one of the N codewords as h ( x ) , then projected using \\mathbb { R } ^d dimensional p_i , q_i specific to that codeword ? In section 4.1 and Table 2 , is the HPBP with codebook the same as the proposed CHPBP ? The wording in `` Then we re-implement ... naively to a codebook strategy '' seems confusing . The method denoted `` Margin '' in Table 4 seems to be better than the proposed approach on CUB . How does it compare in terms of efficiency , memory/computation ? Is it possible to see any classification results ? Most of the relevant second order embeddings have been evaluated in that setting . ===============After rebuttal =============================== After reading all reviews , considering author rebuttal and AC inputs , I believe my initial rating is a bit generous . I would like to downgrade it to 4 . It has been pointed out that many recent works that are of a similar flavor , published in CVPR 2018 and ECCV 2018 , have slightly better results on the same dataset . Further , the only novelty of this work is the proposed factorization and not the encoding scheme . This alone is not sufficient to merit acceptance ."}
